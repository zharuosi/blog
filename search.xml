<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[理解螺旋桨]]></title>
    <url>%2Fblog%2F2018%2F04%2F06%2Fpropeller-concepts%2F</url>
    <content type="text"><![CDATA[Introduction 螺旋桨是一种常见的推进装置，一般由桨毂和桨叶组成。在空气中旋转的螺旋桨可以为飞机提供动力，在水里旋转的螺旋桨可以推动船舶前进。本文介绍螺旋桨设计中的基本概念和流体力学原理。 Reference Geometry Hub: The solid center disk that mates with the propeller shaft and to which the blades are attached. A smaller hub can lead to a larger thrust, however there is a tradeoff between size and strength. Blade: Twisted fins or foils that protrude from the propeller hub. The shape and the speed at which they are driven dictates the torque a given propeller can deliver. Higher diameter equates to higher efficiency for low speed vehicles (&lt;35kn). To obtain higher torque, the rpm (revolution per minute) should be reduced and the diameter should be increased. In high speed vessels, larger diameters lead to high drag. ZZZ is used to represent the number of blades. Here are basic nomenclatures of to describe a blade section: Leading edge: the point at the front of the airfoil that has maximum curvature. Trailing edge: the point of maximum curvature at the rear of the airfoil. Meanline (camber): half distance along a section between the upper and lower surfaces. Quite often the meanline distribution is tabulated forms such as a NACA a=0.8 meanline, where a=0.8 means the meanline can create constant lift of 80% of the chord, then the lift drops linearly to zero at 100%. Chord ©: the nose-tail line, connecting the leading edge and trailing edge. Camber height (f): the distance between nose-tail line and meanline normal to chord. Thickness (t): the section thickness along a line normal to the meanline based on American convention. Thickness measured normal to the chord line is based on British convention. Angle of attack (AOA): the angle between a reference line on a body (often the chord line of an airfoil) and the vector representing the relative motion between the body and the fluid through which it is moving. The chord line of the root or the zero lift axis is often chosen as the reference line. Face: the pressure face, high-pressure side, faces backwards and pushes the water. Back: the suction face, low-pressure side, faces upstream and towards the front of the vessel. Leading edge: the side cuts through the fluid. Trailing edge: the edge of the downstreams. Pitch: the axial distance advanced during one complete rotation of screw is called nominal pitch. The distance the ship is propelled forward in one propeller rotation is actually less than the pitch. The trace of the tip points on the blade is a helix. Pitch ratio is the ratio of pitch to diameter. Slip: The difference between the nominal pitch and the actual distance in one retation (s=pnt−VAts = pnt - V_Ats=pnt−V​A​​t). Propeller section: A circular arc section cut through the blade at some radius. We can expand it to 2-D foil section. Midchord line: the line produced from the midpoint of section nose tail line of each section along a blade. Rake: Axial distance from the midchord point at the hub section and the section of interest. skew angle: the angle between a radial line going through the hub section midchord point and a radial line through the midchord point of the section of interest AND projected. Performance Speed of advance: the propeller advances through the water at a speed of advance VAV_AV​A​​, which delivers a thrust TTT. When the speed of advance is zero, the efficiency is also zero, but the propeller still delivers thrust and absorbs power. Thrust power: PT=TVAP_T = T V_A P​T​​=TV​A​​ Effective power: PE=RVP_E = R V P​E​​=RV Wake: In general the water around the stern has acquired a forward motion in the same direction as the ship. This forward-moving water is called wake. The difference between the ship speed V and the speed of advance is the wake speed. The wake fraction is defined as: w=V−VAVw = \frac{V-V_A}{V} w=​V​​V−V​A​​​​ Wake is due to three principal causes: The frictional drag of the hull causes a following current towards the stern. The streamline flow past the hull causes an increased pressure around the stern. In this region the relative velocity of the water past the hull will be less than the ship’s speed. The ship forms a wave pattern on the surface of the water, and the water particles in the crests have a forward velocity due to the orbital motion, while in the troughs the orbital velocity is sternward. This wake will be positive or negative according to whether there is a crest or a trough of the wave in the vicinity of the propeller. Here are the non-dimensional characterization of propeller performance. Advance coefficient: J=VAnDJ = \frac{V_A}{nD} J=​nD​​V​A​​​​ Thrust coefficient: KT=T/(ρn2D4)K_T = T/(\rho n^2D^4) K​T​​=T/(ρn​2​​D​4​​) Torque coefficient: KQ=Q/(ρn2D5)K_Q = Q/(\rho n^2D^5) K​Q​​=Q/(ρn​2​​D​5​​) Propeller efficiency: η0=TVA2πnQ\eta_0 = \frac{T V_A}{2\pi nQ} η​0​​=​2πnQ​​TV​A​​​​ Propulsive efficiency: ηt=RtVs2πnQ\eta_t = \frac{R_t V_s}{2\pi nQ} η​t​​=​2πnQ​​R​t​​V​s​​​​]]></content>
      <categories>
        <category>工技篇之曹衣出水</category>
      </categories>
      <tags>
        <tag>Principle of Naval Architecture</tag>
        <tag>Marine Hydrodynamics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[雪风号的故事]]></title>
    <url>%2Fblog%2F2018%2F03%2F25%2FDD-Yukikaze%2F</url>
    <content type="text"><![CDATA[引言 雪风是日本在二战期间建造的一艘驱逐舰。她的一生充满传奇，作为战败国的一艘普通的驱逐舰，历经太平洋上大大小小超过十六次的海战而没有沉没，几无受损，被称作“奇跡の駆逐艦”、“呉の雪風 佐世保の時雨”、“不沈の航迹”等。关于雪风的文献很多：《激動の昭和 : 世界奇跡の駆逐艦 : 雪風》、《强运駆逐舰栄光の生涯》、《駆逐艦雪風―誇り高き不沈艦の生涯》等。 本文要讲的雪风的故事，从1938年开始，到1971年结束。 雪风的诅咒 雪风的诞生 1938年8月2日，第8艘阳炎型驱逐舰开工建造，地点是佐世保海军工厂。 1939年3月24日，举行下水及命名仪式。 1940年1月20日，雪风竣工并服役，母港为吴港。 佐世保海军工厂是日本重要的海军造船厂，赤城加贺两艘王牌航母在这里接受改装。明治22年（1889）佐世保镇守府设立，同时设有佐世保造船部。明治36年（1903）改为佐世保海军工厂。昭和36年（1961）更名为佐世保重工业株式会社佐世保造船所。佐世保港位于长崎县，其母港位于广岛县。这两个县都是当时日本的重工业基地，不必再介绍，早已因遭原子弹轰炸而闻名。 雪风的诅咒有二，一是自己永不沉没，二是队友非沉即破。海军间流传雪风会将周围舰船的运气都吸光。也就是从那个遥远的昭和15年1月开始，雪风开始了它精彩而又祥瑞的一生。 雪风和它的朋友们 1941年12月12日，雪风初登场，作为支援参加了菲律宾的登陆作战。 1941年12月24日，雪风前往拉蒙湾，支援登陆作战。此战雪风轻微受损，接受维修舰明石的维修。 1942年1月4日，作为第五战队的旗舰重巡洋舰妙高遭到美军B-17轰炸机轰炸而严重受损，前炮塔损毁，35名水兵死亡。雪风由于指挥得当躲避了该次轰炸。 妙高：MMP... 雪风：？ 1942年3月3日，泗水北方海域，雪风击沉美军潜艇白桦号。 白桦：出名了出名了！ 雪风：First Blood! 1942年6月4日，中途岛海战爆发。雪风作为运输船队的护卫并预备从事防空战。中途岛海战详情参考此链接。日本海军四艘主力航母、248架飞机、1艘重巡洋舰沉没，3000+战死。雪风及时撤退完好无损。 赤城：露落露消我太阁， 加贺：浪花之梦梦还多。 雪风：？ 1942年8月8日，第一次所罗门海战（萨沃岛海战）爆发，也是瓜达尔卡纳尔岛战役的开端。雪风执行运输与护送任务。 1942年10月26日，平静的海色被大破。圣克鲁斯群岛战役爆发，雪风执行护卫任务，舰队旗舰翔鹤大破，舰体严重受损，退出一线战场长达一年。 翔鹤：MMP... 雪风：？ 1942年11月12日夜晚，第三次所罗门海战（瓜达尔卡纳尔岛海战）爆发，海况恶劣，能见度极低，雪风遭到友军误射。舰队旗舰比叡（金刚级战列舰）开着九座探照灯就冲进美军舰队并被围殴。雪风的九妹驱逐舰天津风大破，比叡严重受损并自沉，司令官阿部弘毅转乘雪风。14日同型舰雾岛被击沉。12月28日，天皇批准日军从瓜岛撤退。 比叡：四十九年一睡梦， 雾岛：荣花一期酒一盅。 天津风：MMP... 雪风：？ 1943年2月1日开始执行ケ号作战（瓜达尔卡纳尔岛撤退）。每次海战均有战舰沉没或严重受损，例如，驱逐舰卷云沉没，舞风沉没，卷风大破，矶风大破。雪风毫发无损。 卷云：时之有限花吹散， 舞风：此心归于春山风。 卷风：MMP... 矶风：MMP... 雪风：？ 1943年3月2日，俾斯麦海海战爆发，日本海军被澳大利亚空军和美国空军袭击并重创，运输船队包括旭盛丸、建武丸、爱洋丸、神爱丸、太明丸、帝洋丸、大井川丸、野岛全军覆没。驱逐舰白雪、荒潮、朝潮、时津风被击沉，3000至5000名人员战死，损失物资2500吨。其中，被击沉的时津风也是雪风的姐妹舰，雪风依然毫发无损，顺便捞起了时津风的船员。 各种丸：人生五十年， 时津风：如梦亦如幻。 白雪： 有生斯有死， 二潮： 壮士何所憾。 雪风：？ 1943年7月6日至9日，改造后的雪风做了第八舰队（外南洋部队）的旗舰，驱逐舰做舰队旗舰可能是前无古人后无来者。 1943年7月12日，科隆班加拉岛海战爆发。夜战中雪风作战神勇，在克死旗舰神通（以及战队司令伊崎俊二）后击破了美军3艘巡洋舰。雪风也不幸被击中，然而炮弹竟然未爆炸。此战役以美军作战失败告终，但日军同样损失惨重，所谓的最精锐战队“華の二水戦”也如昙花一现。 神通：仅与金刚寺菩萨种的青松作一别。 雪风：？ 1943年7月20日，参与向科隆班加拉岛的输送作战。满月的夜间，船队受到美军轰炸，驱逐舰夕暮、清波被击沉，重巡洋舰熊野损毁严重。有流言认为夕暮成为了雪风的替死鬼。 夕暮：生于天地之清澈， 清波：归于本愿之清澄。 熊野：MMP... 雪风：？ 1944年1月10日起，雪风和姐妹舰天津风以及轻型航母千岁执行护卫运输船队任务。1月16日天津风被美军潜艇击中断成两截，司令战死，船头包括舰桥部分沉没，船尾在海上漂了一周被拖回越南，安装新船头后得以苟活。 1944年2月，由于时津风被击沉，初风被妙高撞沉，天津风也在大修，第十六驱逐队只剩雪风一艘了。雪风此时被编入第十七驱逐队，然而并不受大家欢迎，因为有流言称“雪風が十六駆で僚艦を全部食い尽くした”。 1944年5月14日，驱逐舰“电”被美军潜艇击中，雪风前往救援，电此时已完全沉没。5月18日，雪风触礁导致螺旋桨损坏。6月9日，谷风被美军潜艇击沉。第十七驱逐队从突袭珍珠港开始至此都没有战损，然而雪风加入之后，谷风成为第一艘被击沉的队员。此后雪风倍遭排挤。 谷风：吾心如那吹散云雾见明月的秋之晚风。 雪风：？ 1944年6月19日，史上最大的航空母舰对决，马里亚纳海战（菲律宾海海战）爆发。由于力量对比悬殊，当天的空战被美军戏称为“马里亚纳射火鸡大赛”。雪风没有受损，反而使用探照灯击落3架美军飞机。五航战的骄傲正规空母翔鹤被击沉，傍晚时分大凤也追随翔鹤而去。 翔鹤：梅雨如露亦如泪， 大凤：杜鹃载吾名至云。 美军飞机：瞎了，瞎了... 雪风：令人窒息的操作... 1944年10月20日，史上最大的海战，莱特湾海战爆发，双方舰船总吨位达到了206万吨。宇宙第一战列舰大和的姐妹舰武藏被击沉。雪风曾护航过的扶桑和山城被击沉。第二舰队司令栗田健男的旗舰爱宕号重巡洋舰被击沉。重巡洋舰摩耶被击沉。妙高、高雄、长门、金刚和榛名受重创，其中金刚与浦风在返航日本途中被美军潜艇击沉。战列舰大和接替旗舰位置。10月25日，“幸运的空母”瑞鹤也走到了终点，被美军飞机击沉。除了这些大船，还有12艘驱逐舰被击沉。雪风本计划救援重巡洋舰筑摩，结果大和发错了命令让雪风归队，让野分去救援。驱逐舰野分执行完救援任务后被美军击沉。 瑞鹤：迩来忧患集一身， 武藏：铁胄身躯今始破。 野分：我比窦娥还冤... 雪风：？ 1944年11月24日，雪风护卫长门返回横须贺港，然后执行新航母信浓的护航任务。信浓是战列舰大和的三号舰，后被改为航空母舰。11月29日，信浓海试，刚出港口就被美军潜艇Archerfish发现并跟踪。护航的雪风误判其为渔船，导致信浓被美军潜艇击沉。雪风继续执行打捞工作，甲板上坐满信浓的船员安全返航。 信浓：吾身如筑摩江芦间点点灯火随之消逝。 雪风：？ 1945年4月6日，日本海军已经消耗殆尽，宇宙第一战列舰大和领衔的冲绳水上特攻作战开始，雪风和时雨为旗舰大和护航。由于资源奇缺，大和出航的燃料来自于其他船舰油罐底部的剩油。4月7日，大和被400架飞机围殴后沉没。雪风的姐妹舰滨风被击沉。雪风也被击中，幸运的是炸弹没爆炸，鱼雷穿底而过。几乎无损的雪风又顺便捞起了大和的幸存者，再顺手送沉了严重损毁已不能行动的驱逐舰矶风。第十七驱逐队也被雪风吃掉只剩雪风一艘了。 大和：春樱秋枫留不住， 滨风：人去关卡亦成空。 雪风：？ 1945年8月15日，日本宣布投降，全体联合舰队的82艘驱逐舰只剩雪风。在整个太平洋战争中，雪风只有不到10名船员死亡，2 人失踪，四任舰长均得以善终。8月18日，雪风护送潜水母舰“长鲸”回到舞鹤港，途中意外触发水雷。屈服于雪风殿下的淫威，这颗水雷在雪风完全通过后才爆炸。8月26日雪风改为第一预备舰，解除武装后，9月15日改为特别输送舰并引渡给美军。 雪风的尾声 1945年10月5日，不沉战舰雪风退役。阳炎型姐妹舰一共18艘，只有雪风活到了这一天。然而雪风的传奇还没结束。 1946年12月30日，雪风成为赔偿舰引渡至联合国。雪风的乘组员们直到最后时刻仍细心地整备。联合国方面感叹曰“战败国的军舰仍不理后事，细心地整备及保养舰只，真是令人惊叹”。 1947年7月3日雪风到达上海高昌庙码头，7月6日移交至中华民国。 1948年5月1日，雪风正式改名为“丹阳”舰，舷号12，成为中华民国海军旗舰。从此雪风不再，只有丹阳。次日，常凯申就听到了刘邓大军渡过黄河开始千里跃进大别山的消息…此后内战开始攻守逆转，国民党方面美援断绝，丹阳被拖到台湾基隆以免遭掳获。 1954年6月23日，苏联油轮“陶普斯”号在台湾海峡被丹阳舰拦截并押送返回高雄，获得大量航空汽油。1955年10月20日陶普斯被改名为会稽号运油舰，编入中华民国海军。主要任务为每个月往返基隆、高雄，给空军机场运送飞机燃油。1953年10月4日，中国和波兰合资成立的中波轮船股份公司所属油轮“布拉卡”号被丹阳舰拦截，羁押于高雄。1954年3月18日，中波轮船股份公司所属远洋货轮“哥德瓦尔特”号在台湾东南海域遭丹阳舰炮击，被俘获后羁押于基隆。 1965年12月16日，由于船体机件老化等问题，丹阳舰降旗停役，1966年11月16日正式退役，1971年12月31日完成拆解。1971年10月25日联合国大会通过2758号决议，中华人民共和国政府依据此决议取得原由中华民国政府在联合国拥有的中国席位与代表权，而中华民国政府被驱逐出联合国。 有文章称雪风拆解的当天，蒋介石意外车祸一病不起。经考据，蒋介石这场意外车祸发生在1969年9月16日下午，距离雪风拆解还早。不过自从雪风拆解以后，委员长身体健康日渐恶化，于1975年4月5号去世。 丹阳舰除役后，日本人成立了“雪风永久保存期成会”，致力于“最後之日本海軍艦艇”促成归还日本。但在最后关头由于台风的关系而浸水严重受损，导致归还失败。 1971年12月8日，中华民国政府将雪风的锚与舵轮送还日本。舵轮收藏于江田岛的旧海军兵学校，而锚则在庭园中展示。1972年，得到了雪风的日本金融危机爆发，田中角荣内阁因被指金权政治而倒台，而失去了雪风的台湾的经济开始起飞。 结束了这传奇的一生后，雪风的两只螺旋桨仍留在台湾，一只被被台湾海军官校收藏，另一只现存于台湾成功岭军史公园。舰钟则安放在台湾左营军史馆。 蒋公：我有一句mmp一定要讲... TG：雪风如此多娇，引无数英雄竞折腰。 日本：祥瑞御免，家宅平安。 雪风：我们的战士，神圣的信仰，永远都不会磨灭，她照耀着我们每一个人，阳炎型8号舰の雪风向您报道。 冷静分析 雪风由于其不沉的奇迹，深受海军高层的重视。然而其他舰船上的水兵们却不这么认为。人人都把雪风看作是扫把星，会把友军的好运全部都吸干。雪风所在的第十六驱逐队消耗殆尽，自己毫发无损。所以最痛恨雪风的应该是她的亲姐妹。从开战至末期都完好无缺的第十七驱逐队，在雪风加入后又逐一沉没，战后只剩孤独的雪风。不过奇迹也好，诅咒也罢，很多时候都被过分解读了。除了雪风的运气好之外，其乘员素质高超，作战英勇，比一般驱逐舰都要优秀，这才是使雪风不沉奇迹的保障。战争总有伤亡和牺牲，雪风并非猥琐自保坑队友，而且出勤率极高，别的舰船被击沉了，又关雪风什么事呢？后期日美国力已不可同日而语，联合舰队的毁灭也是必然，雪风要为此背锅那也是很冤了。 很多时候人们会忽略了雪风的历任舰长和船员的高素质。1941年8月联合舰队最后一次划船赛中，雪风的轮机部门夺得了第一名。而且由于伤亡较少，出勤率高，船员更替很少，所谓好的越好，死的大都是新兵和菜鸟。经历的战役越多，老兵们的经验值也越来越高，就停靠码头这一基本功，技术差的舰长半天靠不上去，雪风每次都是一次成功。所以别的军舰很难躲避的攻击，雪风能够很简单地规避。另外，日本研发的新式海军装备，都先给雪风当试验品，雪风也就成为了日本海军第一艘装备雷达和声纳的驱逐舰。雪风的传奇舰长寺内正道曾是驱逐舰“电”的舰长，在各种伤亡惨重的战役中往往能全身而退。寺内正道曾自信的说，只要我在的船就不会沉，他的信条就是“雪风上不能死人”。寺内正道准确控制航速，油耗和燃油分配能够处于最佳状态。莱特湾海战中，回航期间驱逐舰的油箱都跑干了，需要冒着巨大危险停下来靠其他巡洋舰加油，只有雪风没有发生断油的状况。冲绳海战中，寺内正道直接站在椅子上，头伸出天井目视观测美军密集的空袭。此次作战大和特攻舰队全军覆没，而寺内正道“左脚踢航海长表示左舵，右脚踢航海长表示右舵”，带领雪风在有着“鉄の暴風”称号的冲绳海战中杀出了一条生路。丹阳舰时代，寺内正道也是“雪风永久保存期成会”的成员之一，并且参与了迎回雪风船锚的仪式。 无论如何，雪风是一艘优秀的驱逐舰，也遇到了优秀的舰长和船员，可惜生在了万恶的帝国主义日本。虽然在历史的洪流里，没有带领联合舰队走向胜利，但是可以说在浩瀚的太平洋上留下了经典的传奇。 终章 很多文章包括中文维基百科均提到雪风号“于12月24日参与拉莫湾的登陆支援”，一查发现拉莫湾居然在南极，还在想雪风这是圣诞节开了时空传送到了南极？再一想不对啊，日本跑到南极登陆做什么，又没有发现第一使徒。这个“拉莫湾”一定还是在菲律宾海域。终于查到是“拉蒙湾”，即Lamon Bay，位于菲律宾吕宋岛东部。 雪风这么bug的存在，也许因为它也算是个八阿哥。因为雪风是第八艘阳炎级驱逐舰，她的名字意思是混杂雪花的强风。阳炎（陽炎）也是一种气象现象。英语里叫Heat haze or heat shimmer。汉语里叫热霾，也就是天气炎热时空气中产生的雾，热浪滚滚，有点像（但不是）海市蜃楼的感觉。其他姐妹舰名字的含义： 天津风指的是吹过高天原的风（天つ风）。 时津风指的是顺应季节刮的风。 矶风指的是吹向海滩的风。 浦风即是海风。 滨风为沙滩上的风。 谷风是白天从山谷向山顶吹拂的风。 山风是夜晚从山顶向山谷吹拂的风。 瓜达尔卡纳尔群岛位于南太平洋，热带岛屿的风光如画。发生在这里的瓜岛战役是太平洋战争中一段长达半年的拉锯战，也是日本从战略优势走向劣势的转折点，也是美军在太平洋战略反攻的开始。期间共进行了六次较大规模的海战，31000名日本士兵和7100名盟军士兵战死，38艘日本舰船和29艘盟军舰船长眠海底。《细细的红线》这部詹姆斯·琼斯1961写的小说描述了这场血腥的战斗，极为精彩，并被改编成电影，1999年获德国国际电影节金熊奖，以及第71届奥斯卡最佳影片、最佳导演奖提名。 神通号轻巡洋舰长期担任最精锐的第二水雷战队的旗舰。1943年7月的科隆班加拉岛海战中，面对绝对优势的盟军，神通开着探照灯就冲啊冲，冲入敌阵掩护旗下驱逐舰作战。神通以一敌三，承受了超过2000发炮弹，迅速被还原为零件状态。后人云：“如融熔的铁水在燃烧”。最终，烈火熊熊的神通在受到鱼雷攻击之后断为两截，后半截迅速沉没，在海面挣扎的前半截居然还在坚持炮击直到最后一刻。神通一战成名，被美国军史学家萨缪埃尔·莫里森评价为“整个战争中作战最勇猛的日本军舰”。 有趣的是，1943年3月俾斯麦海战中盟军空军使用了“跳弹攻击”。跳弹（？）指的是轰炸机低空高速投掷航空炸弹，打水漂般地击中对方舰船。这种战术最先由英国人发明，使用一种叫跳弹的炸弹成功地轰炸了德国鲁尔水坝。一般的航空炸弹为了减小阻力均被设计成流线型，试验发现流线型根本跳不起来啊，于是跳弹就被设计成了圆柱体，高速投下后可蹦蹦跳跳的漂出去几千米距离。轰炸机就可以提前逃走避免进入日军防空炮射程，挽救了无数飞行员的生命。跳弹攻击战术最有名的B-24远程轰炸机，二战中产量高达18482架。作为参考，今天美国空军所有类型的飞机总量大约是5000架，天朝空军大约1700架。 莱特湾海战是人类历史上最大的海战，也是世界上最后一次航空母舰+战列舰的对决，美日双方合计42艘主力舰加175艘驱逐舰参与了战斗，也是神风特攻队自杀式攻击的首秀。此时日军已到穷途末路，这175艘驱逐舰中，美军141艘，日本只有34艘。正规航空母舰日军只剩1艘，而美军有9艘，外加26艘轻型航母和护航航母。作战飞机美国有约1500架，日本只有200来架。 本文资料主要来自于维基百科、萌娘百科和日本战国时代名人辞世诗。在此一并致谢。]]></content>
      <categories>
        <category>考据篇之紫电青霜</category>
      </categories>
      <tags>
        <tag>Navy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记 (3)]]></title>
    <url>%2Fblog%2F2018%2F03%2F15%2Fmachine-learning-3%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. Supervised Learning In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal), like SVM, regression, decision trees, naive Bayes, etc. Logistic Regression In logistic regression, the dependent variable is categorical. Therefore, logistic regression is a classification model (with discrete labels). Logistic regression can be binomial, ordinal or multinomial. The binary logistic model is used to estimate the probability of a binary response based on one or more predictor (or independent) variables (features). The output for a dependent variable can have only two values, e.g., “0” and “1”. The conditional distribution y∣xy \mid xy∣x is a Bernoulli distribution rather than a Gaussian distribution. Fig. 1. The standard logistic function The model function of LR model is called the logistic function: g(z)=11+e−zg(z) = \frac{1}{1+e^{-z}} g(z)=​1+e​−z​​​​1​​ where zzz is defined as: z=θTx=θ0+θ1x1+...+θnxnz = \theta^T x = \theta_0 + \theta_1 x_1 + ... + \theta_n x_n z=θ​T​​x=θ​0​​+θ​1​​x​1​​+...+θ​n​​x​n​​ We can obtain the generalized linear model function parameterized by θ\thetaθ: hθ(x)=11+e−θTxh_{\theta}(x) = \frac{1}{1+e^{-\theta^T x}} h​θ​​(x)=​1+e​−θ​T​​x​​​​1​​ The probability of the dependent varible is: p(y=1∣x;θ)=hθ(x)p(y=1 \mid x;\theta) = h_{\theta}(x) p(y=1∣x;θ)=h​θ​​(x) p(y=0∣x;θ)=1−hθ(x)p(y=0 \mid x;\theta) = 1 - h_{\theta}(x) p(y=0∣x;θ)=1−h​θ​​(x) p(y∣x;θ)=(hθ(x))y(1−hθ(x))1−yp(y \mid x;\theta) = (h_{\theta}(x))^y (1 - h_{\theta}(x))^{1-y} p(y∣x;θ)=(h​θ​​(x))​y​​(1−h​θ​​(x))​1−y​​ Maximum likehood estimation(MLE) is used to find the parameter values that maximize the likelihood function with the given observations. Define the likehood: L(θ)=p(y⃗∣X;θ)=(hθ(x))y(1−hθ(x))1−yL(\theta) = p(\vec{y} \mid X;\theta) = (h_{\theta}(x))^y (1 - h_{\theta}(x))^{1-y} L(θ)=p(​y​⃗​​∣X;θ)=(h​θ​​(x))​y​​(1−h​θ​​(x))​1−y​​ =∏i=1m((hθ(x(i)))y(i)(1−hθ(x(i)))1−y(i)= \prod_{i=1}^m ((h_{\theta}(x^{(i)}))^{y^{(i)}} (1 - h_{\theta}(x^{(i)}))^{1-y^{(i)}} =​i=1​∏​m​​((h​θ​​(x​(i)​​))​y​(i)​​​​(1−h​θ​​(x​(i)​​))​1−y​(i)​​​​ In practice, it is often convenient to use the log-likelihood (natural logarithm): logL(θ)=∑i=1my(i)loghθ(x(i))+(1−y(i))log(1−hθ(x(i)))\log L(\theta) = \sum_{i=1}^m y^{(i)}\log h_{\theta}(x^{(i)}) + (1-y^{(i)})\log(1 - h_{\theta}(x^{(i)})) logL(θ)=​i=1​∑​m​​y​(i)​​logh​θ​​(x​(i)​​)+(1−y​(i)​​)log(1−h​θ​​(x​(i)​​)) A log-likelihood can be interpreted as a measure of “encoding length” - the number of bits you expect to spend to encode this information, which is called cross-entropy. The problem is to find the independent variable at which the function values are maximized: argmaxθ∑i=1my(i)loghθ(x(i))+(1−y(i))log(1−hθ(x(i))){argmax}_{\theta} \sum_{i=1}^m y^{(i)}\log h_{\theta}(x^{(i)}) + (1-y^{(i)})\log(1 - h_{\theta}(x^{(i)})) argmax​θ​​​i=1​∑​m​​y​(i)​​logh​θ​​(x​(i)​​)+(1−y​(i)​​)log(1−h​θ​​(x​(i)​​)) By derivation, we can find the θ\thetaθ. there are some optimization methods: Gradient ascent (to find a local maximum) Newton method Conjugate gradient etc. Gradient ascent Apply a gradient ascent algorithm for every jjj: θj:=θj+α∂∂θjlogL(θ)\theta_j:= \theta_j + \alpha \frac{\partial}{\partial \theta_j} \log L(\theta) θ​j​​:=θ​j​​+α​∂θ​j​​​​∂​​logL(θ) where +++ means the direction is the same as the gradient. From the definition of g(z)g(z)g(z), we have: ∂g(z)∂z=g(z)(1−g(z))\frac{\partial g(z)}{\partial z} = g(z) (1 - g(z)) ​∂z​​∂g(z)​​=g(z)(1−g(z)) ∂hθ(x(i))∂θj=hθ(x(i))(1−hθ(x(i)))xj\frac{\partial h_{\theta}(x^{(i)})}{\partial \theta_j} = h_{\theta}(x^{(i)}) (1 - h_{\theta}(x^{(i)})) x_j ​∂θ​j​​​​∂h​θ​​(x​(i)​​)​​=h​θ​​(x​(i)​​)(1−h​θ​​(x​(i)​​))x​j​​ ∂logL(θ)∂θj=∑i=1m(y(i)−hθ(x(i)))xj\frac{\partial \log L(\theta)}{\partial \theta_j} = \sum_{i=1}^m (y^{(i)} - h_{\theta}(x^{(i)}))x_j ​∂θ​j​​​​∂logL(θ)​​=​i=1​∑​m​​(y​(i)​​−h​θ​​(x​(i)​​))x​j​​ Thus, batch gradient ascent method: θj:=θj+α∑i=1m(y(i)−hθ(x(i)))xj\theta_j:= \theta_j + \alpha \sum_{i=1}^m (y^{(i)} - h_{\theta}(x^{(i)})) x_j θ​j​​:=θ​j​​+α​i=1​∑​m​​(y​(i)​​−h​θ​​(x​(i)​​))x​j​​ stochastic gradient ascent method: θj:=θj+α(y(i)−hθ(x(i)))xj\theta_j:= \theta_j + \alpha (y^{(i)} - h_{\theta}(x^{(i)})) x_j θ​j​​:=θ​j​​+α(y​(i)​​−h​θ​​(x​(i)​​))x​j​​ where iii is a randomly chosen sample. Then we can repeat choosing the sample and update the gradient and weights. Newton’s method (Newton-Raphson method) In numerical analysis, Newton’s method is a iterative method for finding successively better approximations to the solution xxx of a real-valued function in the form of f(x)=0f(x) = 0f(x)=0 based on Taylor expansion. The process is repeated as: θk+1:=θk−f(θk)f′(θk)\theta^{k+1}:=\theta^k - \frac{f(\theta^k)}{f&#x27;(\theta^k)} θ​k+1​​:=θ​k​​−​f​′​​(θ​k​​)​​f(θ​k​​)​​ Now we need to solve ∇logL(θ)=0\nabla \log L(\theta) = 0∇logL(θ)=0, the process can be written as: θk+1:=θk−H−1∇θlogL(θ)\theta^{k+1}:=\theta^k - H^{-1} \nabla_{\theta} \log L(\theta) θ​k+1​​:=θ​k​​−H​−1​​∇​θ​​logL(θ) where HHH is the Hessian matrix with n+1n+1n+1 by n+1n+1n+1, whose entries are given by: Hij=∂2logL(θ)∂θi∂θjH_{ij} = \frac{\partial^2 \log L(\theta)}{\partial \theta_i \partial \theta_j} H​ij​​=​∂θ​i​​∂θ​j​​​​∂​2​​logL(θ)​​ The problem is: (why 1/m?) argminθJ(θ)=1m∑i=1m−y(i)loghθ(x(i))−(1−y(i))log(1−hθ(x(i)))argmin_{\theta} J(\theta) = \frac{1}{m} \sum_{i=1}^m -y^{(i)}\log h_{\theta}(x^{(i)}) - (1-y^{(i)})\log(1 - h_{\theta}(x^{(i)})) argmin​θ​​J(θ)=​m​​1​​​i=1​∑​m​​−y​(i)​​logh​θ​​(x​(i)​​)−(1−y​(i)​​)log(1−h​θ​​(x​(i)​​)) The Newton’s method is: θ(t+1)=θ(t)−H−1∇J(θ(t))\theta^{(t+1)} = \theta^{(t)} - H^{-1} \nabla J(\theta^{(t)}) θ​(t+1)​​=θ​(t)​​−H​−1​​∇J(θ​(t)​​) where the gradient of the cost function is: ∇J(θ)=1m∑i=1m(hθ(x(i))−y(i))xj\nabla J(\theta) = \frac{1}{m} \sum_{i=1}^m (h_{\theta} (x^{(i)}) - y^{(i)} )x_j ∇J(θ)=​m​​1​​​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)x​j​​ and the Hessian Matrix is: H=1m∑i=1mhθ(x(i))(1−hθ(x(i)))x(i)(x(i))TH = \frac{1}{m} \sum_{i=1}^m h_{\theta}(x^{(i)})( 1 - h_{\theta}(x^{(i)}) )x^{(i)}(x^{(i)})^T H=​m​​1​​​i=1​∑​m​​h​θ​​(x​(i)​​)(1−h​θ​​(x​(i)​​))x​(i)​​(x​(i)​​)​T​​ (H is a matrix, where is jjj?) Logistic regression has a linear decision boundary (hyperplane). Linear discriminant analysis (LDA) Linear discriminant analysis (LDA), which is also known as Fisher’s linear discriminant, is a simple way for dimensionality reduction in linear classification. By LDA, we can find a linear combination of features that classifies objects in two or more groups. Compared with logistic regression, LDA assumes that the independent variables are normally distributed (Gaussian distribution). Logistic regression is more preferable in applications when the assumption is not valid. Compared with analysis of variance (ANOVA), ANOVA uses categorical independent variables and a continuous dependent variable while LDA adopts continuous independent variables and a categorical dependent variable. Compared with principle component analysis (PCA), PCA is an unsupervised learning method, which does not take into account any difference in class, only for dimensionality reduction. LDA attemps to model the difference between the classes of data using labeled data. Compared with factor analysis, factor analysis builds the feature combinations based on differences. LDA deals with similarities. LDA is a dependence technique. A dependence method is one in which a variable of set of variables is identifies as the dependent variable to be predicted or explained by other, independent variables. Dependence techniques include multiple regression analysis, discriminant analysis, and conjoint analysis. An interdependence method is one in which no single variable or group of variables is defined as being independent or dependent. The goal of interdependence methods is data reduction, or grouping things together. Cluster analysis, factor analysis, and multidimensional scaling are the most commonly used interdependence methods. Consider the nnn dimensional input vector xxx and project it down to one dimension: y=wTxy = w^Tx y=w​T​​x where www is the weight vector for linear transformation. Thus we can place a threshold on yyy and then classify yyy as C1/C2C_1/C_2C​1​​/C​2​​. The dimensionality reduction leads to a considerable loss of information, and classes may become strongly overlapping in the low dimensional space. The goal is to find a projection that maximizes the class separation by adjusting www. Therefore, we need to maxmimize the between-class scatter (use difference of mean values) and minimize the within-class scatter (use covariance matrix). Fig. 1. Comparison of the good projection and the bad projection in LDA Algorithm of LDA Two-class cases The purpose of LDA considers maximizing the “Rayleigh quotient”: J(w)=wTSBwwTSWwJ(w) = \frac{w^T S_B w}{w^T S_W w} J(w)=​w​T​​S​W​​w​​w​T​​S​B​​w​​ where SBS_BS​B​​ is the between class scatter matrix and SWS_WS​W​​ is the within class scatter matrix. the means of every class is: μ1=1N1∑x∈c1x\mu_1 = \frac{1}{N_1} \sum_{x\in c_1} \mathbf{x} μ​1​​=​N​1​​​​1​​​x∈c​1​​​∑​​x μ2=1N2∑x∈c2x\mu_2 = \frac{1}{N_2} \sum_{x\in c_2} \mathbf{x} μ​2​​=​N​2​​​​1​​​x∈c​2​​​∑​​x μ=1N1+N2∑x∈allx\mu = \frac{1}{N_1+N_2} \sum_{x\in all} \mathbf{x} μ=​N​1​​+N​2​​​​1​​​x∈all​∑​​x where xk(k=1,..,m)x_k (k=1,..,m)x​k​​(k=1,..,m) is a vector in Xm×nX_{m\times n}X​m×n​​. The scatter matrix for every class is defined as S1S_1S​1​​ and S2S_2S​2​​: S1=∑x∈c1(x−μ1)(x−μi)TS_1 = \sum_{x\in c_1} (x - \mu_1) (x - \mu_i)^T S​1​​=​x∈c​1​​​∑​​(x−μ​1​​)(x−μ​i​​)​T​​ S2=∑x∈c2(x−μ2)(x−μ2)TS_2 = \sum_{x\in c_2} (x - \mu_2) (x - \mu_2)^T S​2​​=​x∈c​2​​​∑​​(x−μ​2​​)(x−μ​2​​)​T​​ SW=S1+S2S_W = S_1 + S_2 S​W​​=S​1​​+S​2​​ SB=N1(μ1−μ)(μ1−μ)T+N2(μ2−μ)(μ2−μ)T=(μ2−μ1)(μ2−μ1)TS_B = N_1(\mu_1 - \mu) (\mu_1 - \mu)^T + N_2(\mu_2 - \mu) (\mu_2 - \mu)^T = (\mu_2 - \mu_1) (\mu_2 - \mu_1)^T S​B​​=N​1​​(μ​1​​−μ)(μ​1​​−μ)​T​​+N​2​​(μ​2​​−μ)(μ​2​​−μ)​T​​=(μ​2​​−μ​1​​)(μ​2​​−μ​1​​)​T​​ The solution to maximize the J(w)J(w)J(w) can be obtained by derivation and Lagrange multiplier. Let ∣∣wTSww∣∣=1||w^T S_w w||=1∣∣w​T​​S​w​​w∣∣=1 to find one solution for www. ∇wc(w)=∇w(wTSBw−λ(wTSww−1))=0\nabla_w c(w) = \nabla_w (w^T S_B w - \lambda (w^T S_w w - 1)) = 0 ∇​w​​c(w)=∇​w​​(w​T​​S​B​​w−λ(w​T​​S​w​​w−1))=0 The Fisher linear discrimination can be obtained as: Sw−1SBw=λwS_w^{-1} S_B w = \lambda w S​w​−1​​S​B​​w=λw Thus www is the eigenvector of the matrix Sw−1SBS_w^{-1} S_BS​w​−1​​S​B​​. Solution is based on solving a generalized eigenvalue problem. In two-class case, www is calculated as: w=Sw−1(μ2−μ1)w = S_w^{-1} (\mu_2 - \mu_1) w=S​w​−1​​(μ​2​​−μ​1​​) Multi-class cases The purpose is to maximize the “Rayleigh quotient”: J(w)=∣ATSBA∣∣ATSWA∣J(w) = \frac{\mid A^T S_B A \mid}{\mid A^T S_W A \mid} J(w)=​∣A​T​​S​W​​A∣​​∣A​T​​S​B​​A∣​​ where AAA is the projection matrix. μj=1Nj∑x∈cjx\mu_j = \frac{1}{N_j} \sum_{x\in c_j} \mathbf{x} μ​j​​=​N​j​​​​1​​​x∈c​j​​​∑​​x μ=1∑j=1CNj∑x∈allx\mu = \frac{1}{\sum_{j=1}^C N_j} \sum_{x\in all} \mathbf{x} μ=​∑​j=1​C​​N​j​​​​1​​​x∈all​∑​​x where cjc_jc​j​​ means the group of class jjj. CCC is the number of classes. NjN_jN​j​​ is the sample number in the class cjc_jc​j​​. The scatter matrix is generalized as: Sj=∑x∈cj(x−μj)(x−μj)TS_j = \sum_{x\in c_j} (x - \mu_j) (x - \mu_j)^T S​j​​=​x∈c​j​​​∑​​(x−μ​j​​)(x−μ​j​​)​T​​ SW=∑j=1CSj=∑j=1C∑xi∈c=j(xi−μc=j)(xi−μc=j)TS_W = \sum_{j=1}^C S_j = \sum_{j=1}^C \sum_{x_i\in c=j} (x_i - \mu_{c=j}) (x_i - \mu_{c=j})^T S​W​​=​j=1​∑​C​​S​j​​=​j=1​∑​C​​​x​i​​∈c=j​∑​​(x​i​​−μ​c=j​​)(x​i​​−μ​c=j​​)​T​​ SB=∑j=1CNj(μj−μ)(μj−μ)TS_B = \sum_{j=1}^C N_j(\mu_j - \mu) (\mu_j - \mu)^T S​B​​=​j=1​∑​C​​N​j​​(μ​j​​−μ)(μ​j​​−μ)​T​​ where NjN_jN​j​​ is used as the weight. Determinants are the product of all eigenvalues of the matrix. We also have: Sw−1SBAk=λAkS_w^{-1} S_B A_k = \lambda A_k S​w​−1​​S​B​​A​k​​=λA​k​​ To solve AAA, we need to use the eigenvalues of Sw−1SBS_w^{-1} S_BS​w​−1​​S​B​​ and obtain the matrix AAA by KKK eigenvectors. KKK is the number of base vectors, namely the dimension of projection matrix AAA as A=[A1∣A2∣...AK]A=[A_1\mid A_2 \mid ... A_K]A=[A​1​​∣A​2​​∣...A​K​​]. The maximum of KKK is C−1C-1C−1. The classification will be better for the eigenvectors with respect to the larger eigenvalues. Multi-class classification can be transferred to binary classification. The techniques can be categorized into One vs Rest and One vs One. One vs Rest OVR (or one-vs.-all, OvA, one-against-all, OAA) involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. The base classifiers are required to produce a real-valued confidence score for its decision, rather than just a class label, e.g., a new label vector zzz where zi=1z_i = 1z​i​​=1 if yi=ky_i = ky​i​​=k and zi=0z_i = 0z​i​​=0 otherwise. We can use C−1C - 1C−1 binary classifiers. Each binary classifier solves the problem of separating samples in a particular class from samples not in that class. This popular strategy suffers from several problems. Firstly, the scale of the confidence values may differ between the binary classifiers. Second, even if the class distribution is balanced in the training set, the binary classification learners see unbalanced distributions because typically the set of negatives they see is much larger than the set of positives (?). Benefits: Small storing cost and short test time. Shortcomings: Long training time and imbalanced Samples. One vs One OVO involves using C(C−1)/2C(C-1)/2C(C−1)/2 binary classifiers for every possible pairs of classes and learning to distinguish these two classes. At prediction time, a voting scheme is applied: all C(C−1)/2C(C-1)/2C(C−1)/2 classifiers are applied to an sample. The class that got the highest number of vote amongst the discriminant functions gets predicted by the combined classifier. Sometimes it suffers from ambiguities in that some regions of its input space may receive the same number of votes. Benefits: Short training time. Shortcomings: Too many classifiers, large storing cost and long test time.]]></content>
      <categories>
        <category>笔记篇之夙兴夜寐</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
        <tag>Classification</tag>
        <tag>Logistic Regression</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记 (2)]]></title>
    <url>%2Fblog%2F2018%2F03%2F06%2Fmachine-learning-2%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. The Learning Problem Input variables / features : x(i)x^{(i)}x​(i)​​ Target variable : y(i)y^{(i)}y​(i)​​ A training example: (x(i)x^{(i)}x​(i)​​,y(i)y^{(i)}y​(i)​​) A training set: {(x(i)x^{(i)}x​(i)​​,y(i)y^{(i)}y​(i)​​); i=1,2,...,mi=1,2,...,mi=1,2,...,m} The space of input values: XXX The space of output values: YYY The goal is addressed as: Given a training set, learn a function h:X−&gt;Yh: X-&gt;Yh:X−&gt;Y so that h(x)h(x)h(x) is a good predictor for the corresponding value of yyy. If there are nnn features, like x=[x1,x2,...,xn]Tx=[x_1,x_2,...,x_n]^Tx=[x​1​​,x​2​​,...,x​n​​]​T​​, the training set will be Xm×nX_{m\times n}X​m×n​​ and Ym×1Y_{m\times 1}Y​m×1​​. Note that mmm is the number of samples and nnn is the number of features of each sample. Linear Regression Linear regression is a very simple approach for supervised learning. We use linear regression to predict a quantitative response YYY from the predictor variable XXX. The relationship between XXX and YYY is assumed to be linear. A linear function h(x)h(x)h(x) can be used to map from XXX to YYY: h(x)=∑i=0nθixi=θTϕ(x)h(x) = \sum_{i=0}^n \theta_i x_i = \theta ^T \phi(x) h(x)=​i=0​∑​n​​θ​i​​x​i​​=θ​T​​ϕ(x) The intercept term is: x0=1x_0 = 1x​0​​=1. Note that here iii is not the number of samples. multiple linear regression Multiple linear regression is a generalization of linear regression by considering more than one independent variable (XXX: n&gt;1n&gt;1n&gt;1). multivariate linear regression (General linear model) The multivariate linear regression is a generalization of multiple linear regression model to the case of more than one dependent variable (YYY will be a matrix). Linear basis function models In polynomial curve fitting, the feature extraction is represented as the polynomial basis functions: ϕj(x)=xj\phi_j(x) = x^jϕ​j​​(x)=x​j​​, jjj is the polynomial order (1≤j≤M1\leq j\leq M1≤j≤M). A small change in xxx affects all basis functions. Thus using a polynomial basis function is global. Consider the sigmoidal basis functions: ϕj(x)=σ(x−μjs)\phi_j(x) = \sigma (\frac{x - \mu_j}{s})ϕ​j​​(x)=σ(​s​​x−μ​j​​​​), where σ(x)=ex1+ex\sigma(x) = \frac{e^x}{1+e^x}σ(x)=​1+e​x​​​​e​x​​​​. A small change in xxx only affects nearby basis functions (near μj\mu_jμ​j​​). The Least Mean Square (LMS) method The cost function is defined as: J(θ)=12∑i=1m(hθ(x(i))−y(i))2J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 J(θ)=​2​​1​​​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)​2​​ The problem is how to get a minimum J(θ)J(\theta)J(θ). Gradient descent Apply a gradient descent algorithm for every jjj: θj:=θj−α∂∂θjJ(θ)\theta_j:= \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) θ​j​​:=θ​j​​−α​∂θ​j​​​​∂​​J(θ) where jjj is the number of basis function (1≤j≤M1\leq j\leq M1≤j≤M). Note that descent means the direction is the opposite of the gradient (−∇J- \nabla J−∇J). For a single sample, the LMS update rule (or Widro-Hoff learning rule) can be obtained as: θj:=θj−α(hθ(x(i))−y(i))xj(i)\theta_j:= \theta_j - \alpha (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} θ​j​​:=θ​j​​−α(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ Batch gradient descent For many samples, we use the batch gradient descent as: θj:=θj−α∑i=1m(hθ(x(i))−y(i))xj(i)\theta_j :=\theta_j - \alpha \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)} θ​j​​:=θ​j​​−α​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ in the iteration: Repeat until convergence { ... (for every j) } The iteration can be written as: θjk+1=θjk−α∑i=1m(hθ(x(i))−y(i))xj(i)\theta_j^{k+1}=\theta_j^k - \alpha \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)} θ​j​k+1​​=θ​j​k​​−α​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ where iii is the number of samples (1≤i≤m1\leq i\leq m1≤i≤m), kkk is the iteration step. Note that xj(i)x_j^{(i)}x​j​(i)​​ is the feature extraction, x(i)x^{(i)}x​(i)​​ is the sample. until ∣θjk+1−θjk∣&lt;ϵ|\theta_j^{k+1} - \theta_j^{k}|&lt;\epsilon∣θ​j​k+1​​−θ​j​k​​∣&lt;ϵ, then θj\theta_jθ​j​​ reaches convergence. For every iteration, all samples will be traversed. If a new sample is added, the iteration has to start from the first sample again. Stochastic gradient descent When the training set is large, instead, we use Stochastic gradient descent as: θjk+1=θjk−α(hθ(x(i))−y(i))xj(i)\theta_j^{k+1} = \theta_j^k - \alpha (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} θ​j​k+1​​=θ​j​k​​−α(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ in Loop { for i = 1,m { ... (for every j) } } until ∣θjk+1−θjk∣&lt;ϵ|\theta_j^{k+1} - \theta_j^{k}|&lt;\epsilon∣θ​j​k+1​​−θ​j​k​​∣&lt;ϵ, then θj\theta_jθ​j​​ reaches convergence. For each θ\thetaθ, only one sample is used for iteration until the convergence. If a new sample is added, the iteration will continue only using the new sample. Finally, the new θ\thetaθ and hθh_{\theta}h​θ​​ are obtained. Feature scaling is a method used to standardize the range of independent variables or features of data (data normalization). Feature scaling can be used to improve the iteration rate in gradient descent, e.g., rescaling and standardiztion. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. Thus each feature contributes approximately proportionately to the final values. The normal equations To avoid interations, we can use the normal equations. Consider Xm×nX_{m\times n}X​m×n​​ and Ym×1Y_{m\times 1}Y​m×1​​ in matrix, the derivative of J(θ)J(\theta)J(θ) equals zero: ∇θJ(θ)=XTXθ−XTy⃗=0\nabla_\theta J(\theta) = X^T X\theta - X^T \vec{y} = 0 ∇​θ​​J(θ)=X​T​​Xθ−X​T​​​y​⃗​​=0 Finally, θ=(XTX)−1XTy⃗\theta = (X^T X)^{-1}X^T\vec{y} θ=(X​T​​X)​−1​​X​T​​​y​⃗​​ In most situations of practical interest, the number of data points mmm is larger than the dimensionality nnn of the input space, and the matrix XXX is of full column rank. A matrix is full column rank when each of the columns of the matrix are linearly independent. Then XTXX^TXX​T​​X is necessarily invertible and therefore positive definite. The minimum J(θ)J(\theta)J(θ) can be obtained at the critical point when ∇θJ(θ)=0\nabla_\theta J(\theta)=0∇​θ​​J(θ)=0. If m≤nm \leq nm≤n or XXX is not of full column rank, XTXX^TXX​T​​X is not invertible. Regularization is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting. E.g. In Quadratic ridge regression, the cost function is rewritten as: J(θ)=12∑i=1m(hθ(x(i))−y(i))2+λ2∑j=1n∣θj∣2J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2} \sum_{j=1}^n |\theta_j|^2 J(θ)=​2​​1​​​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)​2​​+​2​​λ​​​j=1​∑​n​​∣θ​j​​∣​2​​ According to ∇θJ(θ)=0\nabla_\theta J(\theta)=0∇​θ​​J(θ)=0, θ^\hat{\theta}​θ​^​​ can be calculated as: θ^=(XTX+λI)−1XTy⃗\hat{\theta} = (X^TX+\lambda I)^{-1}X^T\vec{y} ​θ​^​​=(X​T​​X+λI)​−1​​X​T​​​y​⃗​​ Direct methods: Solve the normal equations by Gaussian elimination or QR decomposition Benefit: in a single step or very few steps Shortcoming: not feasible when data are streaming in real time or of very large amount Iterative methods: use the stochastic or gradient descent Benefit: converging fast, more attactive in large practical problems Shortcoming: the learning rate α\alphaα should be carefully chosen. For probalilistic interpretation of Least Mean Square, by the independence assumption, LMS is equivalent to maximum likehood estimation (MLE) of θ\thetaθ. Locally weighted linear regression (LWR) The problem is how to get a minimum J(θ)J(\theta)J(θ) given as: J(θ)=∑i=1mw(i)(hθ(x(i))−y(i))2J(\theta) = \sum_{i=1}^m w^{(i)}(h_{\theta}(x^{(i)}) - y^{(i)})^2 J(θ)=​i=1​∑​m​​w​(i)​​(h​θ​​(x​(i)​​)−y​(i)​​)​2​​ where w(i)=exp(−(x(i)−x)22τ2)w^{(i)} = exp(-\frac{(x^{(i)} - x)^2}{2\tau^2}) w​(i)​​=exp(−​2τ​2​​​​(x​(i)​​−x)​2​​​​) where xxx is the query point for which we’d like to know its yyy. Essentially higher weights will be put on the training examples close to xxx. Parametric and Nonparametric A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at a parametric model, it won’t change its mind about how many parameters it needs. Nonparametric methods are good when you have a lot of data and no prior knowledge, and when you don’t want to worry too much about choosing just the right features. (Artificial Intelligence: A Modern Approach) Benefits of parametric models: Simpler: These methods are easier to understand and interpret results. Speed: Parametric models are very fast to learn from data. Less Data: They do not require as much training data and can work well even if the fit to the data is not perfect. Limitations of parametric models: Constrained: By choosing a functional form these methods are highly constrained to the specified form, because a parametric model has a fixed and finite number of parameters (θ\thetaθ). Limited Complexity: The methods are more suited to simpler problems. Poor Fit: In practice the methods are unlikely to match the underlying mapping function. Benefits of nonparametric models: Flexibility: Capable of fitting a large number of functional forms. Power: No assumptions (or weak assumptions) about the underlying function. Performance: Can result in higher performance models for prediction. Limitations of non-parametric models: More data: Require a lot more training data to estimate the mapping function. Slower: A lot slower to train as they often have far more parameters to train. Overfitting: More of a risk to overfit the training data and it is harder to explain why specific predictions are made. Locally weighted linear regression is a non-parametric learning algorithm. The term non-parametric means the model structure is not specified a priori but is instead determined from data. It is not meant to imply that such models completely lack parameters but that the number and nature of the parameters are flexible and not fixed in advance. If we use unweighted linear regression, Once θ\thetaθ is determined, we no longer need to keep the training data around to make future predictions. In contrast, if the weighted linear regression is applied, the entire training set should be kept around.]]></content>
      <categories>
        <category>笔记篇之夙兴夜寐</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
        <tag>Linear Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记 (1)]]></title>
    <url>%2Fblog%2F2018%2F02%2F28%2Fmachine-learning-1%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. Elementary concepts What is machine learning? Machine Learning is a scientific discipline that explores the construction and study of algorithms that can learn from data. Machine learning enables analysis of massive quantities of data. We all know “Machine”, i.e. computers or computer programs. Do you know what is “Learning”? Learning denotes “Changes in the system that are adaptive in the sense that they enable the system to do same task or tasks drawn from the same population more efficiently and more effectively the next time” [1]. As a pioneer in the field of artificial intelligence, Herbert A. Simon created with Allen Newell the Logic Theory Machine (1956), and the General Problem Solver (GPS) (1957) programs, which was thought as the first method developed for separating problem solving strategy from information about particular problems. Arthur Samuel created the world’s first successful self-learning programs, the Samuel Checkers-playing. He coined the term “Machine Learning” in 1959, which was defined as “the field of study that gives computers the alility to learn without being explicitly programmed”. Tom Mitchell (former Chair of the Machine Learning Department at CMU) gave a modern definition: “A computer program is said to learn from experience EEE with respect to some class of tasks T and performance measure PPP, if its performance at tasks in TTT, as measured by PPP, improves with experience EEE.” An example: playing checkers. EEE = the experience of playing many games of checkers. TTT = the task of playing checkers. PPP = the probability that the program will win the next game. What can machine learning do？ ML can be used to deal with Big Data deluge and predicitve analytics, e.g. Document Classification Spam Filtering Weather Prediction Stock Market Prediction Collaborative Filtering Clustering Images Human Genetics Decoding thoughts from brain scans Medical data analysis Finance Robotics Natural language processing, speech recognition Computer vision Web forensics Computational biology Sensor networks (new multi-modal sensing devices) Social networks Turbulence problem Three components of a machine learning algorithm Pedro Domingos, a CS professor at the University of Washington, decomposed machine learning into three components: Representation, Evaluation, and Optimization [2]. Table 1. The three components of learning algorithms Represnetation Evaluation Optimization Instances Accuracy/Error rate Combinatorial optimization ^K-nearest neighbor Precision and recall ^Greedy search ^Support vector machines Squared error ^Beam search Hyperplanes Likelihood ^Branch-and-bound ^Naive Bayes Posterior probability Continuous optimization ^Logistic regression Information gain ^Unconstrained (Convex) Decision trees K-L divergence ^^Gradient descent Sets of rules Cost/Utility ^^Conjugate gradient ^Propositional rules Margin ^^Quasi-Newton methods ^Logic programs ^Constrained Neural networks ^^Linear programming Graphical models ^^Quadratic programming ^Bayesian networks ^Conditional random fields Representation A classifier must be represented in some formal language that the computer can handle. A learner takes observations as inputs. The observation language is the language used to describe these observations. The hypotheses that a learner may produce, will be formulated in a language that is called the hypothesis language. The hypothesis space is the set of hypotheses that can be described using this hypothesis language [3]. The problem is how to represent the input, i.e. what features to use. For example, 3-layer feedforward neural networks (or computational graphs) form one type of representation, while support vector machines with RBF kernels form another. Evaluation Evaluation is essentially how you judge or prefer candidate programs (hypotheses). An evaluation function (also called objective function, utility function, loss function, fitness function or scoring function) is needed. Mean squared error (of a model’s output vs. the data output) or likelihood (the estimated probability of a model given the observed data) are examples of different evaluation functions. Optimization Finally, a method is needed to search among the candidate programs (in the space of represented models) for the highest-scoring one as the optimum. Stochastic gradient descent and genetic algorithms are two different ways of optimizing a model class. Machine Learning Tasks Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning “signal” or “feedback” available to a learning system: superivsed learning and unsupervised learning. Other tasks also include semi-supervised learning, active learning, reinforcement learning, etc. Supervised learning Supervised learning (or inductive learning) is the most mature, the most studied and the type of learning used by most machine learning algorithms. Learning with supervision is much easier than learning without supervision. Training data includes desired outputs. The goal is to learn a general rule that maps inputs to outputs. Typically, the inputs are transformed into a feature vector, which contains a number of features that are descriptive of the object. Classification Regression Reinforcement learning is concerned with how to take actions in an environment to maximize some notion of long-term reward, such as game-theory. It differs from standard supervised learning in that correct input/output pairs are not provided, nor sub-optimal actions explicitly corrected. Unsupervised learning Training data does not include desired outputs. It is hard to tell what is good learning and what is not. The program own will find the feature in its inputs as “unlabeled” data. Clustering Dimensionality reduction Anomaly dection Neural Networks Main purpose of studying ML To make money.]]></content>
      <categories>
        <category>笔记篇之夙兴夜寐</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日本海军舰艇命名]]></title>
    <url>%2Fblog%2F2018%2F02%2F14%2Fship-name-japan%2F</url>
    <content type="text"><![CDATA[出云 日本领土陆地面积约37.79万平方公里，由北海道、本州、四国、九州等其他6,852个岛屿组成。看似国土狭小的日本，却拥有可怕的领海面积，约447万平方公里，超过其陆地面积十一倍。由于岛国的局限性，日本自古以来极其重视发展海军，并于第二次世界大战时期达到了巅峰。二战以后至今，日本作为战败国，海军也随之衰落，仅仅保留海上自卫队。截至目前海上自卫队的最大军舰是22DDH和24DDH。其中，DDH代表“helicopter destroyer”，即直升机驱逐舰。22和24分别指的是平成22年（2010年）和平成24年（2012年）完成防卫预算下拨，由IHI（Ishikawajima-Harima Heavy Industries）海洋联合横滨工厂负责建造，单舰造价1200亿日元。首舰于2013年8月6日在神奈川县横滨市下水，2015年3月正式服役。该型舰全长达到248米，宽度38米，吃水7.5米，配备4台通用动力LM2500 IEC型燃气轮机，最高航速30节。满载排水量26000吨，相当于普通驱逐舰的3～5倍。搭载七架Sikorsky SH-60K海鹰直升机，其中SH代表“Seahwak”，主要任务是反潛作战（Anti-submarine warfare, ASW）。所以日本其实是在玩文字游戏，这么大的船早已超出驱逐舰的范畴，其实它就是一艘直升机航空母舰，但由于海上自卫队的性质，国际上是绝不允许日本建造巡洋舰、航空母舰等大型进攻型舰艇的，只好打打擦边球。从它可以看出日本根本不可能满足自卫这一战略目标，强行命名为驱逐舰也可以看出日本海上自卫队的脸皮比航空母舰的甲板钢板还要厚。有趣的是， 按照自卫队的传统，新舰的命名应该在下水日（8月6日）才公布，却因海上幕僚监部公文作业疏忽的缘故7月17日就提前曝光了。她就是出云（いずも，Izumo）。 不过“出云”不是破云而出的意思，“出云”是日本弥生时代一个令制国（相当于古代中国的州）名字。二号舰被命名为加贺（かが，Kaga），同样是一个古国名。另外，这两艘战舰的命名是沿袭二战期间的出云重巡洋舰和加贺航空母舰的历史。不过二战期间出云和加贺都参与了侵华战争，罪行累累，网上说该舰被命名为出云号是“恶魔舰复活”也不过分。相比现代的日本海上自卫队，二战时期日本海军（IJN）的命名有一整套独特的规则，很多名字听起来很和风，很美，比如“瑞鹤”、“夕云”、“南风”等等。 航空母舰命名规则 航空母舰是一种搭载飞机为主要武器的军舰，是目前海军最大的作战舰艇平台。日本与航空母舰有千丝万缕的羁绊。例如，一战二战期间中国没有航空母舰，中文里航空母舰这个词怎么来的呢？并不是直接翻译于“Aircraft Carrier”，而是来自于日语：航空母艦（こうくうぼかん），简称空母（くうぼ），可见日本航空母舰的影响之深远。所以提起航空母舰有时侯会引起误会，并不是会飞的船，而是省略了两个字，即“航空（器的）母舰”。世界上第一艘标准的航空母舰是日本最先建造完工的。二战爆发前夕，IJN是太平洋上实力最强，拥有十艘正规空母，相比之下英国有八艘，美国只有七艘。成名之战是1941年的日本的六艘航空母舰袭击珍珠港。后来美日之间的珊瑚海海战，中途岛海战，莱特湾海战，更是宣告了航空母舰时代的到来。 日本航空母舰一般按照龙（りゅう）、凤（ほう）、鹤（かく）等神话生物命名。中途岛海战前，日本的十艘航空母舰分别是：赤城，加贺，苍龙，飞龙，翔鹤，瑞鹤，凤翔，龙骧，祥凤，瑞凤。民用船舶改造而成的日本航母以各种鹰（よう）命名，例如大鹰、云鹰、飞鹰、神鹰、海鹰、隼鹰、冲鹰。千岁号与千代田号是两个特例，作为水上飞机母舰改装的航空母舰，保留了原名。 日语中有两种龙，龍（りゅう）是来自中国东方传说中的龙，是掌管风雨的小神，而另一种，竜（たつ，是龍的简化字）一般指西方神话中的dragon，是一种邪恶贪婪的生物（通常是宝藏的看守）。两种龙的共同点都是拥有强大的力量。一个冷知识是日本的龙通常是三爪，与中国唐代的龙的风格一致，中国的龙经历了从三爪到五爪的演变，是皇帝的象征，而韩国的龙是四爪。在日本，凤凰（鳳凰）是一种比龙更高阶的存在。日本人信奉天照大神，有记载称垂仁天皇26年，天照大神降临神风伊势国，于28年化为白凤。龙有邪恶的龙，比如传说里有勇者斗恶龙，但凤凰是绝对的神圣、吉祥、繁荣的象征和征兆，通常出现在皇室的建筑、服饰、纪念币等，例如天皇座驾丰田世纪的车标就是一只凤凰。西方文化里也有类似的生物（phoenix），但是与凤凰还是有根本的区别。Phoenix一般翻译为不死鸟、火鸟，全身赤红色，长得类似鹰的一种猛禽，特点是会浴火重生。而东方神话里凤凰是五彩的，栖息在梧桐上，象征爱情，类似孔雀的神鸟。山海经里南山经记载，“丹穴之山，有鸟焉，其状如鸡，五采而文，名曰凤皇。首文曰德，翼文曰义，背文曰礼，膺文曰仁，腹文曰信，是鸟也，饮食自然，自歌自舞，见则天下安宁。”与西方的不死鸟最大差别就是东方的凤凰不会浴火重生。至于凤凰涅磐，是郭沫若生造的，并非古代传说。日本的国鸟是绿雉，印在了一万元日元上。可以说日本的凤一般指的就是中国的凤凰，祥瑞的象征。讽刺的是，二战中第一艘沉没的日本航母就是祥凤号。东方文化里，鹤代表着健康长寿、吉祥高贵，也可以是一种为夫妻带来孩子的仙鸟（送子鹤）。宋徽宗画过一幅“瑞鹤图”。日本天皇的声音又叫做鹤音、玉音。一千元日元上也有丹顶鹤的图案。北海道的阿伊努人把生活在钏路湿地的丹顶鹤称为“湿地之神”。总之取名叫鹤显得仙气十足。 翔鹤与瑞鹤两姐妹是第五航空战队的主力，是当时日本最强，飞行员最优秀，战绩最好的航空母舰。作为最著名的两艘日本航母，赤城和加贺却并没有吉祥生物加持。赤城是巡洋舰改装过来的航母，赤城来自关东北部的赤城山來命名。第一第二航空战队的旗舰赤城号，是日本当时最大的航母，一度被视为日本海军机动部队的象征，航空兵的摇篮，山本五十六曾担任舰长。加贺是战列舰改装过来的航母，所以是按古代令制国名来命名。最大的特点是跑得慢，只有28节，以至于舰队里其他航母为了等她，还要额外携带大量油筒。赤城、加贺、苍龙、飞龙四舰葬身于中途岛，其中加贺号创造了伤亡惨重之最，全舰共811人死亡。在加贺号短暂的一生中，曾与国民党空军交过手，文末有彩蛋。 二战后期日本主要建造了三艘云龙级航母：云龙，天城和葛城。后面两艘取名自天城山、大和葛城山。天城山盛产衫树，是当时海军最常用的甲板木料。有趣的是，云龙1944年8月下水的时候，地主家也没有余粮了，日本已经没有可用的舰载机，云龙只好呆在吴港思考舰生，最后一直当作运输船使用。天城建成的时候物资匮乏，采用的是重巡洋舰的主机。天城也没有舰载机，一直无所事事，1945年7月8日在美军的轰炸中沉没，成为了至今为止全世界最后一艘战损的航空母舰。葛城建成的时候穷的甚至巡洋舰主机都没有了，只好用的是驱逐舰主机，主机功率从15万马力被迫下降为10万马力。天城和葛城这一对难兄难弟，为了躲避空袭，航空母舰的甲板上装饰了植物、田地、小房子之类，企图cosplay小岛，然而被炸弹爆炸掀起的巨浪吹掉。天城运气太差，被炸沉，葛城倒没有受到致命的损伤，幸存到了1946年11月。 1945年2月以后，所有日本的航空母舰因为没有飞行员不得不逗留在港口内，无法四处活动。 战列舰命名规则 战列舰有时候也直接称战舰。特点是厚重的装甲加上越来越大的巨炮。在飞机导弹出现以前，舰船以火炮作为主要作战武器，战时排列成一条线互相射击，因此称作战列舰。随着1906年英国的无畏号（HMS Dreadnought）服役，全世界海军开始进入全重型火炮（All-Big-Gun）和高航速主导的时代，按此思想建造的战舰统称为无畏舰。日德兰海战之后，随着主炮口径增加到13英寸以上，并且主炮射程已经超出了视力极限以外，统称为超无畏舰。日语中，根据外来语无畏舰的读音ドレッドノート取首字“ド”，并用同音字“弩”替代，故称为弩级战舰。超弩级战舰也即超无畏舰。尽管在一战二战时期战列舰是主力（Capital ship），越造越大，但现代已经全部淘汰，最后的露面是在1990年的海湾战争。不过未来随着电磁炮的出现和发展，倒可能使战列舰重新登上历史舞台。 1905年颁布的《日本海军舰艇命名办法》规定：战列舰以古国名命名。二战期间比较著名的日本战列舰有：大和，武藏，长门，陆奥，扶桑，山城，伊势，日向等。 从奈良时代开始，日本古代令制国分为五畿七道，后加上北海道。五畿包括：山城国（现京都府），大和国（现奈良县），河内国（现大阪府），和泉国（现大阪府南部），攝津国（现兵库县和大阪府）。七道分别是：东海道，东山道，北陆道，山阴道，山阳道，南海道，西海道。道是仿唐朝制度的，比国高一级的行政单位。每一个道下分数个国，例如东海道里有伊贺国，东山道里有信浓国，西海道还有一个萨摩国。 人类有史以来建成的最大的战列舰是日本海军的大和号，名字来自古代畿内五国之一的大和国。还有一艘同型舰武藏号。可以说她们体现了日本造船技术的巅峰。大和的满载排水量达到了惊人的72,809吨，吃水达到了10.4米，最高行速27节。赫赫有名的三座三联装94式45倍口径460毫米主炮（18.1英寸），是人类有史以来最大的舰炮，一枚炮弹重1.5吨，射程超过26英里。主炮齐射时，甲板上的防空炮手必须躲回舰体内，防止被冲击波杀伤。实际造价为1.37802亿日元，一艘相当于1937年GDP的0.29％。民间称“打伞的大姐姐”（舰队Collection）。戏剧性的是，作为日本帝国和民族的象征，她是决不允许被击沉的。因此实力最强的大和却始终无缘前往一线作战，一生里大多数时间都停在港口内消磨时光。1945年4月6日最后的时刻，帝国已是穷途末路，默默驶出港口的只剩孤身一人，而即将迎接她的是15艘美国航空母舰的攻击。 巡洋舰命名规则 巡洋舰最初指的是可以独立行动的战舰，续航力强，被用于巡逻海外殖民地，排水量、装甲及火力一般仅次于战列舰，但航速较高，机动性强，射速快，并且造价远低于战列舰。根据1921年《华盛顿海军条约》，主炮口径超过8英寸（203毫米）为重巡洋舰。1930年的《伦敦海军条约》为了削减海军装备，重新制定了主炮口径6.1英寸及以上为重巡洋舰，其他的划分为轻巡洋舰。重巡洋舰一般用于袭击敌军和保护主力舰，轻巡洋舰承担护航或防空等任务。为了保证火力和防护的平衡，一般要求军舰自身的装甲要能够抵挡自己的火炮攻击。但也有一种奇葩的船型，她拥有比肩战列舰的强大火力，但为了提升机动性，牺牲了自身的装甲防护，航速可以与巡洋舰相当，专门用于追击作战。这就是战列巡洋舰，典型的有英国的胡德号，号称英国皇家海军的骄傲（The mighty Hood）。胡德号战列巡洋舰的火力不亚于德国的俾斯麦号战列舰，但胡德的防护水平却相差很远，丹麦海峡一战中被命中弹药库造成剧烈爆炸沉没。为了给胡德号报仇，英国甚至出动了海军能动的所有战舰追杀俾斯麦号。现代海军已经很少有国家继续建造巡洋舰，随着导弹和雷达的广泛应用，巡洋舰的角色逐渐被大中型驱逐舰所替代。 日本海军的巡洋舰可以说是“山川”舰队，轻巡洋舰以“川”命名，战列巡洋舰和重巡洋舰以“山”命名。 日本的轻巡洋舰比较重视鱼雷作战。比较著名的有球磨级轻巡洋舰，球磨来自日本熊本县南部水系的球磨川，是日本三大急流之一。球磨级造了五艘，一般作为水雷战队旗舰进行掩护作战，也执行运输船队的护航任务，航速达到了36节。大井号和北上号于1941年进行了魔改，拆除了三门主炮，在两侧各加装了五座四联装九二式610毫米鱼雷发射管，称作“重装雷舰”。1942年北上号拆除了鱼雷发射管改为高速运输舰，搭载了8艘大型登陆艇。1944年北上号继续进行魔改，用于特攻作战，搭载了四艘“回天”人操鱼雷。所谓人操鱼雷，就是由人直接操舵，自杀式攻击的特殊鱼雷（潜艇）。“回天”来自当时对战局逆转的渴望（天を回らし、戰局を逆転させる）。虽然威力很大，直径达到1000毫米，由于操作困难，战果不大。 著名的战列巡洋舰有金刚级四姐妹：金刚，比睿，榛名，雾岛。金刚山在大阪府与奈良县的边境上，景色优美，吸引了众多的登山爱好者。比睿山位于京都府，自古被视作镇护京都的圣山，山上有延曆（历）寺，故有“日本佛教之母山”的美称。榛名山是火山，位于群马县，上毛三山之一（赤城、榛名、妙义），也是“头文字D”中秋名山的原型（日语里榛与春同音，对应秋名）。雾岛山是火山群，位于九州宫崎县附近。传说是日本天照大神的孙子降临的地方。首舰金刚号由英国维克斯公司建造，其他三艘在日本建造，1915年完工。所以金刚级具有英国战舰的特点，比如高大的三角主桅杆。有趣的是，一战期间英日属于同盟国，英国曾请求租借日本的金刚级以对抗德国，但被日本回绝。金刚级建成时属于战列巡洋舰，后来日本海军废除了战列巡洋舰这一舰种，1923～1933年金刚型四舰进行了大改，成为（高速）战列舰，但还是保留了之前的命名方式。不过装甲设计没有得到大规模改进，防御能力不如战列舰。其中，金刚曾担任第五代（1931～1933）日本联合舰队旗舰，1944年沉没于台湾海峡附近，是日本唯一一艘被潜艇击沉的战列舰。在“舰队Collection”中的人设为英国海归，爱喝红茶，外号大傻。比睿由横须贺海军工厂建造，因《伦敦海军条约》被裁成训练舰，在1930年代曾多次担任日本天皇检阅海军的“御召舰”，深受民众喜爱，其照片曾出现在纪念邮票上。榛名之前所有主力舰军在海外订购或海军工厂建造，而榛名是第一艘由民企，即川崎造船所（现川崎重工）承建，主机测试前发生了故障导致延期，川崎造船所造机工作部长篠田恒太郎因自责剖腹自尽，可见压力之大。另一家民企，三菱合资会社长崎造船所（现三菱重工）则承建了四号舰雾岛，与姐姐比睿在1942年所罗门海战中一同沉没，其中一波三折将在后文《雪风号的故事》的故事里再见。 驱逐舰命名规则 驱逐舰最初用来对付鱼雷艇，便宜、易造又好用，后来也承担起防空、反潜、攻击等多用途任务，是各国海军建造最多的一种军舰。英语里的Destroyer翻译成驱逐舰着实有点委屈它了。 日本驱逐舰分两种，一等驱逐舰以天气、潮汐、水流、月亮、季节、自然现象等命名，一个不是很恰当的概括就是“风花雪月”。二等驱逐舰以植物等命名。 很多驱逐舰名字的禅意不输各种艺术品。例如，吹雪型驱逐舰有：吹雪，白雪，初雪，深雪，从云，东云，薄云，白云，矶波，浦波；绫波型驱逐舰有绫波，敷波，朝雾，夕雾，天雾，狭雾，胧，曙，涟，潮；晓型驱逐舰有晓，响，雷，电，著名的第六小学生驱逐队。首舰吹雪号于1928年8月10日完工，排水量1980吨，全长118.5米，宽10.4米，吃水3.2米，最高航速38节。以植物命名的驱逐舰有：松，梨，若竹，楢（即橡树）等。众多日本驱逐舰里，最著名的恐怕是雪风号，阳炎型的8号舰，其传奇故事本文暂且不讲。这里介绍另一艘著名的日本驱逐舰：岛风。该型驱逐舰最初计划建造32艘，可由于她的动力系统过于复杂、生产跟不上等缘故，最后只建造了一艘，岛风没有姊妹舰，也没被编入任何驱逐舰队，从出生那个时刻开始就很孤独。岛风有两个特点，第一个特点是跑得特别快，装有试验涡轮机，最高航速达到了惊人的40.7节（约每小时72公里），创下日本驱逐舰的最快记录，人称海上飚车老司机。另一个特点是鱼雷特别强，装备了3座五连装九三式重型酸素鱼雷（日语中“酸素”意为氧气，“水素”意为氢气，“窒素”意为氮气），一举成为水雷战最强驱逐舰（日语中“水雷”是鱼雷、水雷、深水炸弹的统称）。这种鱼雷是当时全世界最先进的鱼雷，使用压缩氧气代替压缩空气作为推进，最大射程达到了40km，弹头重达1080磅，并且航迹难以被发现。同时期美军的MK15鱼雷最远射程仅14km，弹头也只有827磅。岛风号单次可以齐射15枚九三式，几乎是普通驱逐舰的2倍。服役后，太平洋上的局势已经不可逆转，而且鱼雷决战的思想也已经落后，因此岛风并没有什么骄人的战绩。1944年11月8日奥尔莫克湾战役中，虽然岛风以惊人的高速机动能力躲过了多次轰炸，最终还是不敌美军舰载机，中弹太多而后因锅炉过热引起爆炸，沉没在菲律宾附近海域。此后，太平洋再也没有岛风。不过在海底，岛风应该不再那么孤独了吧。 潜艇命名规则 潜水艇就是在水下航行的舰艇，小的有单人操作的水下航行器，大的有苏联建造的941“台风”弹道导弹核潜艇，水下排水量可达4.8万吨。二战期间大西洋战场上的“狼群”，指的就是德国海军邓尼茲领导的U型潜艇部队，对盟军的海上交通和补给线造成了巨大的破坏。在日本，潜水艇被称作“潜水艦”。 根据《日本海军舰艇命名办法》，一等潜艦按汉字“伊”后加上数字命名，二等潜艦按汉字“吕”后加上数字命，三等潜艦按汉字“波”后加上数字命。名称按顺序取自《伊呂波歌》名。这是一首日本平安时代的和歌，每句5音和7音相错。由于全歌以47个不重复的假名组成，后来常被用于书法的范文和假名的学习范文。有一个中译版本：花虽芬芳终须落，此世岂谁可常留。有为山深今日越，不恋醉梦免蹉跎。 二战中的日本潜艇虽然没有德国狼群那么臭名昭著，不过也有那么几条比较出名的。第一个例子是二战中最大的潜艇：伊四〇〇型。它的满载排水量达到了6560吨，达到了一般巡洋舰的水平。之所以造这么大，其特色是可以搭载三架小型轰炸机，计划是潜航到接近海岸线，然后释放“晴岚”水上飞机携带细菌武器攻击美国本土，因此伊400实际上算是“潜水空母”，不过尚未投入实战日本就已投降。第二个例子伊19号，1942年9月15日在所罗门群岛伊19使用六发鱼雷（三发命中）击沉了美军的胡蜂号航空母舰（CV-7)。第三个例子是伊58号则以击沉美军印第安纳波利斯号重巡洋舰而声名大噪，因为这艘巡洋舰刚刚完成了运输“小男孩”原子弹到提尼安岛的秘密任务，1945年8月6日“小男孩”由B-29轰炸机投掷在广岛，伤亡10万余人。2016年有一部尼古拉斯·凯奇主演的电影：《“印第安纳波利斯”号：勇者无惧》即是根据此事件改编（USS Indianapolis: Men of Courage）。 本文资料主要来自于维基百科和萌娘百科。在此一并致谢。 彩蛋 舰队Collection 一般译作“舰队收藏”，是由角川游戏开发、DMM.com提供及运营的网页游戏，以二战时期的日本军舰为题材，玩家需要收集称为“舰娘”（艦娘（かんむす））的军舰萌拟人化角色卡片，可以对舰娘进行强化及改造，并编制不同的舰队与敌人战斗务求获得胜利。作为PC浏览器运行的网页游戏，在运营半年用户即突破100万。立绘精美，画师在创作时也加入了很多历史元素。例如，大和的形象是一个“打伞的大姐姐”，里面的伞来源于大和上的一式三型电探天线。所谓的电探又是啥？日本的雷达在以前被翻译成电波探信仪。 “舰娘本娘” 吾妻 旧日本海军有一艘装甲巡洋舰名叫“吾妻”，订购自法国。1898年由法国Ateliers et Chantiers de la Loire造船厂建造。1900年回到日本服役，曾参与日俄战争中的旅顺口海战，蔚山海战和对马海战。到1945年已是服役四十多年的老舰，被美军击沉于港口内，第二年被解体。 鳳翔 凤翔（ほうしょう，Hōshō）就是上文提到的世界上第一艘服役的全通式甲板航空母舰，采用岛装上层建筑，技术上是从英国窃取而来。本来世界第一属于英国的竞技神号航空母舰，不过由于英国的进度拖拉，导致世界第一被日本海军抢走。凤翔于1920年12月16日开工，1922年12月27日服役，被尊称为“航母之母”。她是唯一一艘活到日本投降后还没有受损的航空母舰，战后作为复员运输舰到各地进行接运任务。后在大阪日立造船樱岛工厂解体，度过了幸运的一生。注意，凤翔跟祥凤是两码事。 武勋三舰客 旧日本海军二战期间有三艘公认的武勋舰（战绩好，活的久）：瑞鹤被称为“幸運の空母”。榛名一直活跃在一线战场，受到过不同损伤而始终没有被击沉。雪风被称为“奇跡の駆逐艦”，著名的扫把星（祥瑞），日常坑队友，通常在海战中自己毫发无损，克死友军无数。民间又称“抗日奇侠雪风号”。 相思不断笕桥东 如果您浏览了上面那么多枯燥的数据，还能坚持看到这里没有关掉页面，那真的十分感谢了。文章结尾，再讲一个故事。 加贺号航空母舰，侵华战争中多次参与对中国人民的屠杀，参加过对苏州、杭州、上海的轰炸。虽然耀武扬威无数，不过也有扑街的时候。1937年8月14日，正值台风过境，日军决定空袭杭州笕桥——中国的空军学校，就是来自加贺号航空母舰。那一天，21架国军空军霍克-3双翼机与45架日本舰载机于空中战斗，加贺损失了8架八九舰攻和2架九四舰轰，而年轻的中国空军竟无一伤亡，吊打日军，胜利凯旋（一说击落日军飞机三架击伤一架）。史称“八一四空战”，又称“笕桥空战”。这样一支英勇的空军，更是打到木更津航空队联队长石井义大佐剖腹自杀，被日军称为“耻辱”。 《冲天》是台湾2015年拍的一部纪录片，豆瓣评分9.2。没有慷慨激昂和洗脑宣传，只平平淡淡地讲述了抗日战争年代一群年轻飞行员的爱情与事业，令人泪流满面。如果有兴趣的话，可以去看一看高志航的故事。笕桥中央航空学校校门口石碑上立着：“我们的身体、飞机和炸弹，当与敌人兵舰阵地同归于尽！”与那个年代大多数参军的农民子弟不同，中央航空学校的毕业生们大多数家庭出身优渥，教育程度高，如果没有战争，他们也许将是各行各业的精英。可命运让他们生在了那个年代，成为了飞行员。《无问西东》中，王力宏演的沈光耀的人物原型，叫做沈崇诲。他就读著名的天津南开中学，18岁考入清华大学土木工程系，1932年毕业后不久放弃了在绥远舒适的工作，投考中央航校轰炸科，毕业后留校担任飞行教官。淞沪会战期间，1937年8月19日，沈崇诲驾机为地面日军炮火击中，难以返回，他决定与敌人同归于尽，瞄准了日军一艘战舰急速俯冲而下，撞舰牺牲，年仅26岁。而这艘被撞的日本军舰，就是本文开头所提到的历史同名战舰“出云”。 因为“笕桥空战”，每年8月14日被定为“空军节”。“相思不断笕桥东”是《西子姑娘》里的歌词，是中华民国空军军歌之一。抗日战争中，中华民国飞行员的平均牺牲年龄是23岁。有关我们飞行员的故事很多，那些故事里，最多的一句话是：“某天，他一去就再也没有回来。” 希望人类不要再有战争，希望所有人能和自己的爱人永远在一起。]]></content>
      <categories>
        <category>考据篇之木雁之间</category>
      </categories>
      <tags>
        <tag>Navy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH免密码登录]]></title>
    <url>%2Fblog%2F2018%2F02%2F07%2Fssh-login%2F</url>
    <content type="text"><![CDATA[本文介绍如何使用SSH进行远程登录以及设置SSH免密码登录。 SSH简介 首先简单介绍一下什么是SSH(Secure Shell)。 SSH是一个用于计算机间的远程登录会话和其他网络服务的加密网络安全协议，由互联网工程任务组 IETF(Internet Engineering Task Force)的网络工作小組（Network Working Group）所制定。相对于传统的网络服务，如FTP、 POP等， SSH协议避免了使用明文传输数据，帐号和口令，可以有效防止信息泄漏和中间人攻击，也能够防止DNS欺骗和IP欺骗。其具体工作原理本文不作详述。 使用SSH登录远程主机 首先判断是否安装SSH服务。打开Terminal试试： 1$ ssh localhost 若显示connection refused, 则需要手动安装。 安装SSH服务： 通常使用的是openssh, 以Ubuntu为例。 本机： 1$ sudo apt-get install openssh-client 服务器： 1$ sudo apt-get install openssh-server 安装完成后确认开启SSH服务： 1$ ps -e|grep ssh 启动服务器端的SSH服务： 1$ sudo /etc/init.d/ssh start 更改默认端口号： 1$ sudo vim /etc/ssh/sshd_config 使用SSH登录： 本机输入以下命令登录远程服务器(例如ip地址为10.20.120.10的远程服务器)： 1$ ssh user@10.20.120.10 开启图形界面： 1$ ssh -Y user@10.20.120.10 即可使用远程服务器上的图形界面应用。 一般情况下，每次登录必须输入密码。在涉及大量文件传输操作时，频繁输入密码会十分不便。以下介绍如何免密码登录。 设置SSH免密码登录 创建公钥 1$ ssh-keygen -t rsa 默认设置即可，创建id_rsa（私钥）和id_rsa.pub（公钥）文件。以后再次操作记得先行备份。 上传公钥至服务器 1$ rsync -avPz ~/.ssh/id_rsa.pub user@server:~/.ssh/ Note: 如果服务器上无.ssh目录则先创建（mkdir ~/.ssh） 复制公钥至authorized_keys 1$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 更改文件及目录权限 12$ chmod 600 ~/.ssh/authorized_keys$ chmod 700 ~/.ssh 完成后即可不输入密码直接登录。]]></content>
      <categories>
        <category>工技篇之吴带当风</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2F2018%2F02%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new "My New Post" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>工技篇之曹衣出水</category>
      </categories>
  </entry>
</search>
