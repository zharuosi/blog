<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习笔记 (2)]]></title>
    <url>%2Fblog%2F2018%2F03%2F06%2Fmachine-learning2%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. The Learning Problem Input variables / features : x(i)x^{(i)}x​(i)​​ Target variable : y(i)y^{(i)}y​(i)​​ A training example: (x(i)x^{(i)}x​(i)​​,y(i)y^{(i)}y​(i)​​) A training set: {(x(i)x^{(i)}x​(i)​​,y(i)y^{(i)}y​(i)​​); i=1,2,...,mi=1,2,...,mi=1,2,...,m} The space of input values: XXX The space of output values: YYY The goal is addressed as: Given a training set, learn a function h:X−&gt;Yh: X-&gt;Yh:X−&gt;Y so that h(x)h(x)h(x) is a good predictor for the corresponding value of yyy. If there are nnn features, like x=[x1,x2,...,xn]Tx=[x_1,x_2,...,x_n]^Tx=[x​1​​,x​2​​,...,x​n​​]​T​​, the training set will be Xm×nX_{m\times n}X​m×n​​ and Ym×1Y_{m\times 1}Y​m×1​​. Note that mmm is the number of samples and nnn is the number of features of each sample. Linear Regression Linear regression is a very simple approach for supervised learning. We use linear regression to predict a quantitative response YYY from the predictor variable XXX. The relationship between XXX and YYY is assumed to be linear. A linear function h(x)h(x)h(x) can be used to map from XXX to YYY: h(x)=∑i=0nθixi=θTϕ(x)h(x) = \sum_{i=0}^n \theta_i x_i = \theta ^T \phi(x) h(x)=​i=0​∑​n​​θ​i​​x​i​​=θ​T​​ϕ(x) The intercept term is: x0=1x_0 = 1x​0​​=1. Note that here iii is not the number of samples. multiple linear regression Multiple linear regression is a generalization of linear regression by considering more than one independent variable (XXX: n&gt;1n&gt;1n&gt;1). multivariate linear regression (General linear model) The multivariate linear regression is a generalization of multiple linear regression model to the case of more than one dependent variable (YYY will be a matrix). Linear basis function models In polynomial curve fitting, the feature extraction is represented as the polynomial basis functions: ϕj(x)=xj\phi_j(x) = x^jϕ​j​​(x)=x​j​​, jjj is the polynomial order (1≤j≤M1\leq j\leq M1≤j≤M). A small change in xxx affects all basis functions. Thus using a polynomial basis function is global. Consider the sigmoidal basis functions: ϕj(x)=σ(x−μjs)\phi_j(x) = \sigma (\frac{x - \mu_j}{s})ϕ​j​​(x)=σ(​s​​x−μ​j​​​​), where σ(x)=ex1+ex\sigma(x) = \frac{e^x}{1+e^x}σ(x)=​1+e​x​​​​e​x​​​​. A small change in xxx only affects nearby basis functions (near μj\mu_jμ​j​​). The Least Mean Square (LMS) method The cost function is defined as: J(θ)=12∑i=1m(hθ(x(i))−y(i))2J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 J(θ)=​2​​1​​​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)​2​​ The problem is how to get a minimum J(θ)J(\theta)J(θ). Gradient descent Apply a gradient descent algorithm for every jjj: θj:=θj−α∂∂θjJ(θ)\theta_j:= \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) θ​j​​:=θ​j​​−α​∂θ​j​​​​∂​​J(θ) where jjj is the number of basis function (1≤j≤M1\leq j\leq M1≤j≤M). For a single sample, the LMS update rule (or Widro-Hoff learning rule) can be obtained as: θj:=θj−α(hθ(x(i))−y(i))xj(i)\theta_j:= \theta_j - \alpha (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} θ​j​​:=θ​j​​−α(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ Batch gradient descent For many samples, we use the batch gradient descent as: θj:=θj−α∑i=1m(hθ(x(i))−y(i))xj(i)\theta_j :=\theta_j - \alpha \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)} θ​j​​:=θ​j​​−α​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ in the iteration: Repeat until convergence { ... (for every j) } The iteration can be written as: θjk+1=θjk−α∑i=1m(hθ(x(i))−y(i))xj(i)\theta_j^{k+1}=\theta_j^k - \alpha \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)} θ​j​k+1​​=θ​j​k​​−α​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ where iii is the number of samples (1≤i≤m1\leq i\leq m1≤i≤m), kkk is the iteration step. Note that xj(i)x_j^{(i)}x​j​(i)​​ is the feature extraction, x(i)x^{(i)}x​(i)​​ is the sample. until ∣θjk+1−θjk∣&lt;ϵ|\theta_j^{k+1} - \theta_j^{k}|&lt;\epsilon∣θ​j​k+1​​−θ​j​k​​∣&lt;ϵ, then θj\theta_jθ​j​​ reaches convergence. For every iteration, all samples will be traversed. If a new sample is added, the iteration has to start from the first sample again. Stochastic gradient descent When the training set is large, instead, we use Stochastic gradient descent as: θjk+1=θjk−α(hθ(x(i))−y(i))xj(i)\theta_j^{k+1} = \theta_j^k - \alpha (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} θ​j​k+1​​=θ​j​k​​−α(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ in Loop { for i = 1,m { ... (for every j) } } until ∣θjk+1−θjk∣&lt;ϵ|\theta_j^{k+1} - \theta_j^{k}|&lt;\epsilon∣θ​j​k+1​​−θ​j​k​​∣&lt;ϵ, then θj\theta_jθ​j​​ reaches convergence. For each θ\thetaθ, only one sample is used for iteration until the convergence. If a new sample is added, the iteration will continue only using the new sample. Finally, the new θ\thetaθ and hθh_{\theta}h​θ​​ are obtained. Feature scaling is a method used to standardize the range of independent variables or features of data (data normalization). Feature scaling can be used to improve the iteration rate in gradient descent, e.g., rescaling and standardiztion. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. Thus each feature contributes approximately proportionately to the final values. The normal equations To avoid interations, we can use the normal equations. Consider Xm×nX_{m\times n}X​m×n​​ and Ym×1Y_{m\times 1}Y​m×1​​ in matrix, the derivative of J(θ)J(\theta)J(θ) equals zero: ∇θJ(θ)=XTXθ−XTy⃗=0\nabla_\theta J(\theta) = X^T X\theta - X^T \vec{y} = 0 ∇​θ​​J(θ)=X​T​​Xθ−X​T​​​y​⃗​​=0 Finally, θ=(XTX)−1XTy⃗\theta = (X^T X)^{-1}X^T\vec{y} θ=(X​T​​X)​−1​​X​T​​​y​⃗​​ In most situations of practical interest, the number of data points mmm is larger than the dimensionality nnn of the input space, and the matrix XXX is of full column rank. A matrix is full column rank when each of the columns of the matrix are linearly independent. Then XTXX^TXX​T​​X is necessarily invertible and therefore positive definite. The minimum J(θ)J(\theta)J(θ) can be obtained at the critical point when ∇θJ(θ)=0\nabla_\theta J(\theta)=0∇​θ​​J(θ)=0. If m≤nm \leq nm≤n or XXX is not of full column rank, XTXX^TXX​T​​X is not invertible. Regularization is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting. E.g. In Quadratic ridge regression, the cost function is rewritten as: J(θ)=12∑i=1m(hθ(x(i))−y(i))2+λ2∑j=1n∣θj∣2J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2} \sum_{j=1}^n |\theta_j|^2 J(θ)=​2​​1​​​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)​2​​+​2​​λ​​​j=1​∑​n​​∣θ​j​​∣​2​​ According to ∇θJ(θ)=0\nabla_\theta J(\theta)=0∇​θ​​J(θ)=0, θ^\hat{\theta}​θ​^​​ can be calculated as: θ^=(XTX+λI)−1XTy⃗\hat{\theta} = (X^TX+\lambda I)^{-1}X^T\vec{y} ​θ​^​​=(X​T​​X+λI)​−1​​X​T​​​y​⃗​​ Direct methods: Solve the normal equations by Gaussian elimination or QR decomposition Benefit: in a single step or very few steps Shortcoming: not feasible when data are streaming in real time or of very large amount Iterative methods: use the stochastic or gradient descent Benefit: converging fast, more attactive in large practical problems Shortcoming: the learning rate α\alphaα should be carefully chosen. For probalilistic interpretation of Least Mean Square, by the independence assumption, LMS is equivalent to maximum likehood estimation (MLE) of θ\thetaθ. Locally weighted linear regression (LWR) The problem is how to get a minimum J(θ)J(\theta)J(θ) given as: J(θ)=∑i=1mw(i)(hθ(x(i))−y(i))2J(\theta) = \sum_{i=1}^m w^{(i)}(h_{\theta}(x^{(i)}) - y^{(i)})^2 J(θ)=​i=1​∑​m​​w​(i)​​(h​θ​​(x​(i)​​)−y​(i)​​)​2​​ where w(i)=exp(−frac(x(i)−x)22τ2)w^{(i)} = exp(-frac{(x^{(i)} - x)^2}{2\tau^2}) w​(i)​​=exp(−frac(x​(i)​​−x)​2​​2τ​2​​) where xxx is the query point for which we’d like to know its yyy. Essentially higher weights will be put on the training examples close to xxx. Locally weighted linear regression is a non-parametric learning algorithm.]]></content>
      <categories>
        <category>笔记篇之夙兴夜寐</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
        <tag>Linear Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记 (1)]]></title>
    <url>%2Fblog%2F2018%2F02%2F28%2Fmachine-learning-1%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. Elementary concepts What is machine learning? Machine Learning is a scientific discipline that explores the construction and study of algorithms that can learn from data. Machine learning enables analysis of massive quantities of data. We all know “Machine”, i.e. computers or computer programs. Do you know what is “Learning”? Learning denotes “Changes in the system that are adaptive in the sense that they enable the system to do same task or tasks drawn from the same population more efficiently and more effectively the next time” [1]. As a pioneer in the field of artificial intelligence, Herbert A. Simon created with Allen Newell the Logic Theory Machine (1956), and the General Problem Solver (GPS) (1957) programs, which was thought as the first method developed for separating problem solving strategy from information about particular problems. Arthur Samuel created the world’s first successful self-learning programs, the Samuel Checkers-playing. He coined the term “Machine Learning” in 1959, which was defined as “the field of study that gives computers the alility to learn without being explicitly programmed”. Tom Mitchell (former Chair of the Machine Learning Department at CMU) gave a modern definition: “A computer program is said to learn from experience EEE with respect to some class of tasks T and performance measure PPP, if its performance at tasks in TTT, as measured by PPP, improves with experience EEE.” An example: playing checkers. EEE = the experience of playing many games of checkers. TTT = the task of playing checkers. PPP = the probability that the program will win the next game. What can machine learning do？ ML can be used to deal with Big Data deluge and predicitve analytics, e.g. Document Classification Spam Filtering Weather Prediction Stock Market Prediction Collaborative Filtering Clustering Images Human Genetics Decoding thoughts from brain scans Medical data analysis Finance Robotics Natural language processing, speech recognition Computer vision Web forensics Computational biology Sensor networks (new multi-modal sensing devices) Social networks Turbulence problem Three components of a machine learning algorithm Pedro Domingos, a CS professor at the University of Washington, decomposed machine learning into three components: Representation, Evaluation, and Optimization [2]. Table 1. The three components of learning algorithms Represnetation Evaluation Optimization Instances Accuracy/Error rate Combinatorial optimization ^K-nearest neighbor Precision and recall ^Greedy search ^Support vector machines Squared error ^Beam search Hyperplanes Likelihood ^Branch-and-bound ^Naive Bayes Posterior probability Continuous optimization ^Logistic regression Information gain ^Unconstrained (Convex) Decision trees K-L divergence ^^Gradient descent Sets of rules Cost/Utility ^^Conjugate gradient ^Propositional rules Margin ^^Quasi-Newton methods ^Logic programs ^Constrained Neural networks ^^Linear programming Graphical models ^^Quadratic programming ^Bayesian networks ^Conditional random fields Representation A classifier must be represented in some formal language that the computer can handle. A learner takes observations as inputs. The observation language is the language used to describe these observations. The hypotheses that a learner may produce, will be formulated in a language that is called the hypothesis language. The hypothesis space is the set of hypotheses that can be described using this hypothesis language [3]. The problem is how to represent the input, i.e. what features to use. For example, 3-layer feedforward neural networks (or computational graphs) form one type of representation, while support vector machines with RBF kernels form another. Evaluation Evaluation is essentially how you judge or prefer candidate programs (hypotheses). An evaluation function (also called objective function, utility function, loss function, fitness function or scoring function) is needed. Mean squared error (of a model’s output vs. the data output) or likelihood (the estimated probability of a model given the observed data) are examples of different evaluation functions. Optimization Finally, a method is needed to search among the candidate programs (in the space of represented models) for the highest-scoring one as the optimum. Stochastic gradient descent and genetic algorithms are two different ways of optimizing a model class. Machine Learning Tasks Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning “signal” or “feedback” available to a learning system: superivsed learning and unsupervised learning. Other tasks also include semi-supervised learning, active learning, reinforcement learning, etc. Supervised learning Supervised learning (or inductive learning) is the most mature, the most studied and the type of learning used by most machine learning algorithms. Learning with supervision is much easier than learning without supervision. Training data includes desired outputs. The goal is to learn a general rule that maps inputs to outputs. Typically, the inputs are transformed into a feature vector, which contains a number of features that are descriptive of the object. Classification Regression Reinforcement learning is concerned with how to take actions in an environment to maximize some notion of long-term reward, such as game-theory. It differs from standard supervised learning in that correct input/output pairs are not provided, nor sub-optimal actions explicitly corrected. Unsupervised learning Training data does not include desired outputs. It is hard to tell what is good learning and what is not. The program own will find the feature in its inputs as “unlabeled” data. Clustering Dimensionality reduction Anomaly dection Neural Networks Main purpose of studying ML To make money.]]></content>
      <categories>
        <category>笔记篇之夙兴夜寐</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日本海军舰艇命名]]></title>
    <url>%2Fblog%2F2018%2F02%2F14%2Fship-name-japan%2F</url>
    <content type="text"><![CDATA[出云 日本领土陆地面积约37.79万平方公里，由北海道、本州、四国、九州等其他6,852个岛屿组成。看似国土狭小的日本，却拥有可怕的领海面积，约447万平方公里，超过其陆地面积十一倍。由于岛国的局限性，日本自古以来极其重视发展海军，并于第二次世界大战时期达到了巅峰。二战以后至今，日本作为战败国，海军也随之衰落，仅仅保留海上自卫队。截至目前海上自卫队的最大军舰是22DDH和24DDH。其中，DDH代表“helicopter destroyer”，即直升机驱逐舰。22和24分别指的是平成22年（2010年）和平成24年（2012年）完成防卫预算下拨，由IHI（Ishikawajima-Harima Heavy Industries）海洋联合横滨工厂负责建造，单舰造价1200亿日元。首舰于2013年8月6日在神奈川县横滨市下水，2015年3月正式服役。该型舰全长达到248米，宽度38米，吃水7.5米，配备4台通用动力LM2500 IEC型燃气轮机，最高航速30节。满载排水量26000吨，相当于普通驱逐舰的3～5倍。搭载七架Sikorsky SH-60K海鹰直升机，其中SH代表“Seahwak”，主要任务是反潛作战（Anti-submarine warfare, ASW）。所以日本其实是在玩文字游戏，这么大的船早已超出驱逐舰的范畴，其实它就是一艘直升机航空母舰，但由于海上自卫队的性质，国际上是绝不允许日本建造巡洋舰、航空母舰等大型进攻型舰艇的，只好打打擦边球。从它可以看出日本根本不可能满足自卫这一战略目标，强行命名为驱逐舰也可以看出日本海上自卫队的脸皮比航空母舰的甲板钢板还要厚。有趣的是， 按照自卫队的传统，新舰的命名应该在下水日（8月6日）才公布，却因海上幕僚监部公文作业疏忽的缘故7月17日就提前曝光了。她就是出云（いずも，Izumo）。 不过“出云”不是破云而出的意思，“出云”是日本弥生时代一个令制国（相当于古代中国的州）名字。二号舰被命名为加贺（かが，Kaga），同样是一个古国名。另外，这两艘战舰的命名是沿袭二战期间的出云重巡洋舰和加贺航空母舰的历史。不过二战期间出云和加贺都参与了侵华战争，罪行累累，网上说该舰被命名为出云号是“恶魔舰复活”也不过分。相比现代的日本海上自卫队，二战时期日本海军（IJN）的命名有一整套独特的规则，很多名字听起来很和风，很美，比如“瑞鹤”、“夕云”、“南风”等等。 航空母舰命名规则 航空母舰是一种搭载飞机为主要武器的军舰，是目前海军最大的作战舰艇平台。日本与航空母舰有千丝万缕的羁绊。例如，一战二战期间中国没有航空母舰，中文里航空母舰这个词怎么来的呢？并不是直接翻译于“Aircraft Carrier”，而是来自于日语：航空母艦（こうくうぼかん），简称空母（くうぼ），可见日本航空母舰的影响之深远。所以提起航空母舰有时侯会引起误会，并不是会飞的船，而是省略了两个字，即“航空（器的）母舰”。世界上第一艘标准的航空母舰是日本最先建造完工的。二战爆发前夕，IJN是太平洋上实力最强，拥有十艘正规空母，相比之下英国有八艘，美国只有七艘。成名之战是1941年的日本的六艘航空母舰袭击珍珠港。后来美日之间的珊瑚海海战，中途岛海战，莱特湾海战，更是宣告了航空母舰时代的到来。 日本航空母舰一般按照龙（りゅう）、凤（ほう）、鹤（かく）等神话生物命名。中途岛海战前，日本的十艘航空母舰分别是：赤城，加贺，苍龙，飞龙，翔鹤，瑞鹤，凤翔，龙骧，祥凤，瑞凤。民用船舶改造而成的日本航母以各种鹰（よう）命名，例如大鹰、云鹰、飞鹰、神鹰、海鹰、隼鹰、冲鹰。千岁号与千代田号是两个特例，作为水上飞机母舰改装的航空母舰，保留了原名。 日语中有两种龙，龍（りゅう）是来自中国东方传说中的龙，是掌管风雨的小神，而另一种，竜（たつ，是龍的简化字）一般指西方神话中的dragon，是一种邪恶贪婪的生物（通常是宝藏的看守）。两种龙的共同点都是拥有强大的力量。一个冷知识是日本的龙通常是三爪，与中国唐代的龙的风格一致，中国的龙经历了从三爪到五爪的演变，是皇帝的象征，而韩国的龙是四爪。在日本，凤凰（鳳凰）是一种比龙更高阶的存在。日本人信奉天照大神，有记载称垂仁天皇26年，天照大神降临神风伊势国，于28年化为白凤。龙有邪恶的龙，比如传说里有勇者斗恶龙，但凤凰是绝对的神圣、吉祥、繁荣的象征和征兆，通常出现在皇室的建筑、服饰、纪念币等，例如天皇座驾丰田世纪的车标就是一只凤凰。西方文化里也有类似的生物（phoenix），但是与凤凰还是有根本的区别。Phoenix一般翻译为不死鸟、火鸟，全身赤红色，长得类似鹰的一种猛禽，特点是会浴火重生。而东方神话里凤凰是五彩的，栖息在梧桐上，象征爱情，类似孔雀的神鸟。山海经里南山经记载，“丹穴之山，有鸟焉，其状如鸡，五采而文，名曰凤皇。首文曰德，翼文曰义，背文曰礼，膺文曰仁，腹文曰信，是鸟也，饮食自然，自歌自舞，见则天下安宁。”与西方的不死鸟最大差别就是东方的凤凰不会浴火重生。至于凤凰涅磐，是郭沫若生造的，并非古代传说。日本的国鸟是绿雉，印在了一万元日元上。可以说日本的凤一般指的就是中国的凤凰，祥瑞的象征。讽刺的是，二战中第一艘沉没的日本航母就是祥凤号。东方文化里，鹤代表着健康长寿、吉祥高贵，也可以是一种为夫妻带来孩子的仙鸟（送子鹤）。宋徽宗画过一幅“瑞鹤图”。日本天皇的声音又叫做鹤音、玉音。一千元日元上也有丹顶鹤的图案。北海道的阿伊努人把生活在钏路湿地的丹顶鹤称为“湿地之神”。总之取名叫鹤显得仙气十足。 翔鹤与瑞鹤两姐妹是第五航空战队的主力，是当时日本最强，飞行员最优秀，战绩最好的航空母舰。作为最著名的两艘日本航母，赤城和加贺却并没有吉祥生物加持。赤城是巡洋舰改装过来的航母，赤城来自关东北部的赤城山來命名。第一第二航空战队的旗舰赤城号，是日本当时最大的航母，一度被视为日本海军机动部队的象征，航空兵的摇篮，山本五十六曾担任舰长。加贺是战列舰改装过来的航母，所以是按古代令制国名来命名。最大的特点是跑得慢，只有28节，以至于舰队里其他航母为了等她，还要额外携带大量油筒。赤城、加贺、苍龙、飞龙四舰葬身于中途岛，其中加贺号创造了伤亡惨重之最，全舰共811人死亡。在加贺号短暂的一生中，曾与国民党空军交过手，文末有彩蛋。 二战后期日本主要建造了三艘云龙级航母：云龙，天城和葛城。后面两艘取名自天城山、大和葛城山。天城山盛产衫树，是当时海军最常用的甲板木料。有趣的是，云龙1944年8月下水的时候，地主家也没有余粮了，日本已经没有可用的舰载机，云龙只好呆在吴港思考舰生，最后一直当作运输船使用。天城建成的时候物资匮乏，采用的是重巡洋舰的主机。天城也没有舰载机，一直无所事事，1945年7月8日在美军的轰炸中沉没，成为了至今为止全世界最后一艘战损的航空母舰。葛城建成的时候穷的甚至巡洋舰主机都没有了，只好用的是驱逐舰主机，主机功率从15万马力被迫下降为10万马力。天城和葛城这一对难兄难弟，为了躲避空袭，航空母舰的甲板上装饰了植物、田地、小房子之类，企图cosplay小岛，然而被炸弹爆炸掀起的巨浪吹掉。天城运气太差，被炸沉，葛城倒没有受到致命的损伤，幸存到了1946年11月。 1945年2月以后，所有日本的航空母舰因为没有飞行员不得不逗留在港口内，无法四处活动。 战列舰命名规则 战列舰有时候也直接称战舰。特点是厚重的装甲加上越来越大的巨炮。在飞机导弹出现以前，舰船以火炮作为主要作战武器，战时排列成一条线互相射击，因此称作战列舰。随着1906年英国的无畏号（HMS Dreadnought）服役，全世界海军开始进入全重型火炮（All-Big-Gun）和高航速主导的时代，按此思想建造的战舰统称为无畏舰。日德兰海战之后，随着主炮口径增加到13英寸以上，并且主炮射程已经超出了视力极限以外，统称为超无畏舰。日语中，根据外来语无畏舰的读音ドレッドノート取首字“ド”，并用同音字“弩”替代，故称为弩级战舰。超弩级战舰也即超无畏舰。尽管在一战二战时期战列舰是主力（Capital ship），越造越大，但现代已经全部淘汰，最后的露面是在1990年的海湾战争。不过未来随着电磁炮的出现和发展，倒可能使战列舰重新登上历史舞台。 1905年颁布的《日本海军舰艇命名办法》规定：战列舰以古国名命名。二战期间比较著名的日本战列舰有：大和，武藏，长门，陆奥，扶桑，山城，伊势，日向等。 从奈良时代开始，日本古代令制国分为五畿七道，后加上北海道。五畿包括：山城国（现京都府），大和国（现奈良县），河内国（现大阪府），和泉国（现大阪府南部），攝津国（现兵库县和大阪府）。七道分别是：东海道，东山道，北陆道，山阴道，山阳道，南海道，西海道。道是仿唐朝制度的，比国高一级的行政单位。每一个道下分数个国，例如东海道里有伊贺国，东山道里有信浓国，西海道还有一个萨摩国。 人类有史以来建成的最大的战列舰是日本海军的大和号，名字来自古代畿内五国之一的大和国。还有一艘同型舰武藏号。可以说她们体现了日本造船技术的巅峰。大和的满载排水量达到了惊人的72,809吨，吃水达到了10.4米，最高行速27节。赫赫有名的三座三联装94式45倍口径460毫米主炮（18.1英寸），是人类有史以来最大的舰炮，一枚炮弹重1.5吨，射程超过26英里。主炮齐射时，甲板上的防空炮手必须躲回舰体内，防止被冲击波杀伤。实际造价为1.37802亿日元，一艘相当于1937年GDP的0.29％。民间称“打伞的大姐姐”（舰队Collection）。戏剧性的是，作为日本帝国和民族的象征，她是决不允许被击沉的。因此实力最强的大和却始终无缘前往一线作战，一生里大多数时间都停在港口内消磨时光。1945年4月6日最后的时刻，帝国已是穷途末路，默默驶出港口的只剩孤身一人，而即将迎接她的是15艘美国航空母舰的攻击。 巡洋舰命名规则 巡洋舰最初指的是可以独立行动的战舰，续航力强，被用于巡逻海外殖民地，排水量、装甲及火力一般仅次于战列舰，但航速较高，机动性强，射速快，并且造价远低于战列舰。根据1921年《华盛顿海军条约》，主炮口径超过8英寸（203毫米）为重巡洋舰。1930年的《伦敦海军条约》为了削减海军装备，重新制定了主炮口径6.1英寸及以上为重巡洋舰，其他的划分为轻巡洋舰。重巡洋舰一般用于袭击敌军和保护主力舰，轻巡洋舰承担护航或防空等任务。为了保证火力和防护的平衡，一般要求军舰自身的装甲要能够抵挡自己的火炮攻击。但也有一种奇葩的船型，她拥有比肩战列舰的强大火力，但为了提升机动性，牺牲了自身的装甲防护，航速可以与巡洋舰相当，专门用于追击作战。这就是战列巡洋舰，典型的有英国的胡德号，号称英国皇家海军的骄傲（The mighty Hood）。胡德号战列巡洋舰的火力不亚于德国的俾斯麦号战列舰，但胡德的防护水平却相差很远，丹麦海峡一战中被命中弹药库造成剧烈爆炸沉没。为了给胡德号报仇，英国甚至出动了海军能动的所有战舰追杀俾斯麦号。现代海军已经很少有国家继续建造巡洋舰，随着导弹和雷达的广泛应用，巡洋舰的角色逐渐被大中型驱逐舰所替代。 日本海军的巡洋舰可以说是“山川”舰队，轻巡洋舰以“川”命名，战列巡洋舰和重巡洋舰以“山”命名。 日本的轻巡洋舰比较重视鱼雷作战。比较著名的有球磨级轻巡洋舰，球磨来自日本熊本县南部水系的球磨川，是日本三大急流之一。球磨级造了五艘，一般作为水雷战队旗舰进行掩护作战，也执行运输船队的护航任务，航速达到了36节。大井号和北上号于1941年进行了魔改，拆除了三门主炮，在两侧各加装了五座四联装九二式610毫米鱼雷发射管，称作“重装雷舰”。1942年北上号拆除了鱼雷发射管改为高速运输舰，搭载了8艘大型登陆艇。1944年北上号继续进行魔改，用于特攻作战，搭载了四艘“回天”人操鱼雷。所谓人操鱼雷，就是由人直接操舵，自杀式攻击的特殊鱼雷（潜艇）。“回天”来自当时对战局逆转的渴望（天を回らし、戰局を逆転させる）。虽然威力很大，直径达到1000毫米，由于操作困难，战果不大。 著名的战列巡洋舰有金刚级四姐妹：金刚，比睿，榛名，雾岛。金刚山在大阪府与奈良县的边境上，景色优美，吸引了众多的登山爱好者。比睿山位于京都府，自古被视作镇护京都的圣山，山上有延曆（历）寺，故有“日本佛教之母山”的美称。榛名山是火山，位于群马县，上毛三山之一（赤城、榛名、妙义），也是“头文字D”中秋名山的原型（日语里榛与春同音，对应秋名）。雾岛山是火山群，位于九州宫崎县附近。传说是日本天照大神的孙子降临的地方。首舰金刚号由英国维克斯公司建造，其他三艘在日本建造，1915年完工。所以金刚级具有英国战舰的特点，比如高大的三角主桅杆。有趣的是，一战期间英日属于同盟国，英国曾请求租借日本的金刚级以对抗德国，但被日本回绝。金刚级建成时属于战列巡洋舰，后来日本海军废除了战列巡洋舰这一舰种，1923～1933年金刚型四舰进行了大改，成为（高速）战列舰，但还是保留了之前的命名方式。不过装甲设计没有得到大规模改进，防御能力不如战列舰。其中，金刚曾担任第五代（1931～1933）日本联合舰队旗舰，1944年沉没于台湾海峡附近，是日本唯一一艘被潜艇击沉的战列舰。在“舰队Collection”中的人设为英国海归，爱喝红茶，外号大傻。比睿由横须贺海军工厂建造，因《伦敦海军条约》被裁成训练舰，在1930年代曾多次担任日本天皇检阅海军的“御召舰”，深受民众喜爱，其照片曾出现在纪念邮票上。榛名之前所有主力舰军在海外订购或海军工厂建造，而榛名是第一艘由民企，即川崎造船所（现川崎重工）承建，主机测试前发生了故障导致延期，川崎造船所造机工作部长篠田恒太郎因自责剖腹自尽，可见压力之大。另一家民企，三菱合资会社长崎造船所（现三菱重工）则承建了四号舰雾岛，与姐姐比睿在1942年所罗门海战中一同沉没，其中一波三折将在后文《传奇战舰雪风号》的故事里再见。 驱逐舰命名规则 驱逐舰最初用来对付鱼雷艇，便宜、易造又好用，后来也承担起防空、反潜、攻击等多用途任务，是各国海军建造最多的一种军舰。英语里的Destroyer翻译成驱逐舰着实有点委屈它了。 日本驱逐舰分两种，一等驱逐舰以天气、潮汐、水流、月亮、季节、自然现象等命名，一个不是很恰当的概括就是“风花雪月”。二等驱逐舰以植物等命名。 很多驱逐舰名字的禅意不输各种艺术品。例如，吹雪型驱逐舰有：吹雪，白雪，初雪，深雪，从云，东云，薄云，白云，矶波，浦波；绫波型驱逐舰有绫波，敷波，朝雾，夕雾，天雾，狭雾，胧，曙，涟，潮；晓型驱逐舰有晓，响，雷，电，著名的第六小学生驱逐队。首舰吹雪号于1928年8月10日完工，排水量1980吨，全长118.5米，宽10.4米，吃水3.2米，最高航速38节。以植物命名的驱逐舰有：松，梨，若竹，楢（即橡树）等。众多日本驱逐舰里，最著名的恐怕是雪风号，阳炎型的8号舰，其传奇故事本文暂且不讲。这里介绍另一艘著名的日本驱逐舰：岛风。该型驱逐舰最初计划建造32艘，可由于她的动力系统过于复杂、生产跟不上等缘故，最后只建造了一艘，岛风没有姊妹舰，也没被编入任何驱逐舰队，从出生那个时刻开始就很孤独。岛风有两个特点，第一个特点是跑得特别快，装有试验涡轮机，最高航速达到了惊人的40.7节（约每小时72公里），创下日本驱逐舰的最快记录，人称海上飚车老司机。另一个特点是鱼雷特别强，装备了3座五连装九三式重型酸素鱼雷（日语中“酸素”意为氧气，“水素”意为氢气，“窒素”意为氮气），一举成为水雷战最强驱逐舰（日语中“水雷”是鱼雷、水雷、深水炸弹的统称）。这种鱼雷是当时全世界最先进的鱼雷，使用压缩氧气代替压缩空气作为推进，最大射程达到了40km，弹头重达1080磅，并且航迹难以被发现。同时期美军的MK15鱼雷最远射程仅14km，弹头也只有827磅。岛风号单次可以齐射15枚九三式，几乎是普通驱逐舰的2倍。服役后，太平洋上的局势已经不可逆转，而且鱼雷决战的思想也已经落后，因此岛风并没有什么骄人的战绩。1944年11月8日奥尔莫克湾战役中，虽然岛风以惊人的高速机动能力躲过了多次轰炸，最终还是不敌美军舰载机，中弹太多而后因锅炉过热引起爆炸，沉没在菲律宾附近海域。此后，太平洋再也没有岛风。不过在海底，岛风应该不再那么孤独了吧。 潜艇命名规则 潜水艇就是在水下航行的舰艇，小的有单人操作的水下航行器，大的有苏联建造的941“台风”弹道导弹核潜艇，水下排水量可达4.8万吨。二战期间大西洋战场上的“狼群”，指的就是德国海军邓尼茲领导的U型潜艇部队，对盟军的海上交通和补给线造成了巨大的破坏。在日本，潜水艇被称作“潜水艦”。 根据《日本海军舰艇命名办法》，一等潜艦按汉字“伊”后加上数字命名，二等潜艦按汉字“吕”后加上数字命，三等潜艦按汉字“波”后加上数字命。名称按顺序取自《伊呂波歌》名。这是一首日本平安时代的和歌，每句5音和7音相错。由于全歌以47个不重复的假名组成，后来常被用于书法的范文和假名的学习范文。有一个中译版本：花虽芬芳终须落，此世岂谁可常留。有为山深今日越，不恋醉梦免蹉跎。 二战中的日本潜艇虽然没有德国狼群那么臭名昭著，不过也有那么几条比较出名的。第一个例子是二战中最大的潜艇：伊四〇〇型。它的满载排水量达到了6560吨，达到了一般巡洋舰的水平。之所以造这么大，其特色是可以搭载三架小型轰炸机，计划是潜航到接近海岸线，然后释放“晴岚”水上飞机携带细菌武器攻击美国本土，因此伊400实际上算是“潜水空母”，不过尚未投入实战日本就已投降。第二个例子伊19号，1942年9月15日在所罗门群岛伊19使用六发鱼雷（三发命中）击沉了美军的胡蜂号航空母舰（CV-7)。第三个例子是伊58号则以击沉美军印第安纳波利斯号重巡洋舰而声名大噪，因为这艘巡洋舰刚刚完成了运输“小男孩”原子弹到提尼安岛的秘密任务，1945年8月6日“小男孩”由B-29轰炸机投掷在广岛，伤亡10万余人。2016年有一部尼古拉斯·凯奇主演的电影：《“印第安纳波利斯”号：勇者无惧》即是根据此事件改编（USS Indianapolis: Men of Courage）。 本文资料主要来自于维基百科和萌娘百科。在此一并致谢。 彩蛋 舰队Collection 一般译作“舰队收藏”，是由角川游戏开发、DMM.com提供及运营的网页游戏，以二战时期的日本军舰为题材，玩家需要收集称为“舰娘”（艦娘（かんむす））的军舰萌拟人化角色卡片，可以对舰娘进行强化及改造，并编制不同的舰队与敌人战斗务求获得胜利。作为PC浏览器运行的网页游戏，在运营半年用户即突破100万。立绘精美，画师在创作时加入了很多历史元素，非常适合考据党研究。例如，大和的形象是一个“打伞的大姐姐”，里面的伞来源于大和上的一式三型电探天线。电探又是啥？日本的雷达在以前被翻译成电波探信仪。 为了测试博客里的图片显示是否正常，附送一张全家福： “舰娘本娘” 吾妻 旧日本海军有一艘装甲巡洋舰名叫“吾妻”，订购自法国。1898年由法国Ateliers et Chantiers de la Loire造船厂建造。1900年回到日本服役，曾参与日俄战争中的旅顺口海战，蔚山海战和对马海战。到1945年已是服役四十多年的老舰，被美军击沉于港口内，第二年被解体。 鳳翔 凤翔（ほうしょう，Hōshō）就是上文提到的世界上第一艘服役的全通式甲板航空母舰，采用岛装上层建筑，技术上是从英国窃取而来。本来世界第一属于英国的竞技神号航空母舰，不过由于英国的进度拖拉，导致世界第一被日本海军抢走。凤翔于1920年12月16日开工，1922年12月27日服役，被尊称为“航母之母”。她是唯一一艘活到日本投降后还没有受损的航空母舰，战后作为复员运输舰到各地进行接运任务。后在大阪日立造船樱岛工厂解体，度过了幸运的一生。注意，凤翔跟祥凤是两码事。 武勋三舰客 旧日本海军二战期间有三艘公认的武勋舰（战绩好，活的久）：瑞鹤被称为“幸運の空母”。榛名一直活跃在一线战场，受到过不同损伤而始终没有被击沉。雪风被称为“奇跡の駆逐艦”，是著名的扫把星（祥瑞），日常坑队友，通常在海战中自己毫发无损，克死友军无数。民间又称“抗日奇侠雪风号”。 相思不断笕桥东 如果您浏览了上面那么多枯燥的数据，还能坚持看到这里没有关掉页面，那真的十分感谢了。文章结尾，再讲一个故事。 加贺号航空母舰，侵华战争中多次参与对中国人民的屠杀，参加过对苏州、杭州、上海的轰炸。虽然耀武扬威无数，不过也有扑街的时候。1937年8月14日，正值台风过境，日军决定空袭杭州笕桥——中国的空军学校，就是来自加贺号航空母舰。那一天，21架国军空军霍克-3双翼机与45架日本舰载机于空中战斗，加贺损失了8架八九舰攻和2架九四舰轰，而年轻的中国空军竟无一伤亡，吊打日军，胜利凯旋（一说击落日军飞机三架击伤一架）。史称“八一四空战”，又称“笕桥空战”。这样一支英勇的空军，更是打到木更津航空队联队长石井义大佐剖腹自杀，被日军称为“耻辱”。 《冲天》是台湾2015年拍的一部纪录片，豆瓣评分9.2。以抗日战争为大背景，没有慷慨激昂和洗脑宣传，只平平淡淡地讲述了那个年代一群年轻飞行员的爱情与事业，令人泪流满面。有一个名字不应该被遗忘：高志航，有兴趣的可以去搜索了解一下。笕桥中央航空学校校门口石碑上立着：“我们的身体、飞机和炸弹，当与敌人兵舰阵地同归于尽！”与那个年代大多数参军的农民子弟不同，中央航空学校的毕业生们大多数家庭出身优渥，教育程度高，如果没有战争，他们也许将是各行各业的精英。可命运让他们生在了那个年代，成为了飞行员。《无问西东》中，王力宏演的沈光耀的人物原型，叫做沈崇诲。他就读著名的天津南开中学，18岁考入清华大学土木工程系，1932年毕业后不久放弃了在绥远舒适的工作，投考中央航校轰炸科，毕业后留校担任飞行教官。淞沪会战期间，1937年8月19日，沈崇诲驾机为地面日军炮火击中，难以返回，他决定与敌人同归于尽，瞄准了日军一艘战舰急速俯冲而下，撞舰牺牲，年仅26岁。而这艘被撞的日本军舰，就是本文开头所提到的历史同名战舰“出云”。 因为“笕桥空战”，每年8月14日被定为“空军节”。“相思不断笕桥东”是《西子姑娘》里的歌词，是中华民国空军军歌之一。抗日战争中，中华民国飞行员的平均牺牲年龄是23岁。有关我们飞行员的故事很多，那些故事里，最多的一句话是：“某天，他一去就再也没有回来。” 希望人类不要再有战争，希望所有人能和自己的爱人永远在一起。]]></content>
      <categories>
        <category>考据篇之木雁之间</category>
      </categories>
      <tags>
        <tag>Navy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH免密码登录]]></title>
    <url>%2Fblog%2F2018%2F02%2F07%2Fssh-login%2F</url>
    <content type="text"><![CDATA[本文介绍如何使用SSH进行远程登录以及设置SSH免密码登录。 SSH简介 首先简单介绍一下什么是SSH(Secure Shell)。 SSH是一个用于计算机间的远程登录会话和其他网络服务的加密网络安全协议，由互联网工程任务组 IETF(Internet Engineering Task Force)的网络工作小組（Network Working Group）所制定。相对于传统的网络服务，如FTP、 POP等， SSH协议避免了使用明文传输数据，帐号和口令，可以有效防止信息泄漏和中间人攻击，也能够防止DNS欺骗和IP欺骗。其具体工作原理本文不作详述。 使用SSH登录远程主机 首先判断是否安装SSH服务。打开Terminal试试： 1$ ssh localhost 若显示connection refused, 则需要手动安装。 安装SSH服务： 通常使用的是openssh, 以Ubuntu为例。 本机： 1$ sudo apt-get install openssh-client 服务器： 1$ sudo apt-get install openssh-server 安装完成后确认开启SSH服务： 1$ ps -e|grep ssh 启动服务器端的SSH服务： 1$ sudo /etc/init.d/ssh start 更改默认端口号： 1$ sudo vim /etc/ssh/sshd_config 使用SSH登录： 本机输入以下命令登录远程服务器(例如ip地址为10.20.120.10的远程服务器)： 1$ ssh user@10.20.120.10 开启图形界面： 1$ ssh -Y user@10.20.120.10 即可使用远程服务器上的图形界面应用。 一般情况下，每次登录必须输入密码。在涉及大量文件传输操作时，频繁输入密码会十分不便。以下介绍如何免密码登录。 设置SSH免密码登录 创建公钥 1$ ssh-keygen -t rsa 默认设置即可，创建id_rsa（私钥）和id_rsa.pub（公钥）文件。以后再次操作记得先行备份。 上传公钥至服务器 1$ rsync -avPz ~/.ssh/id_rsa.pub user@server:~/.ssh/ Note: 如果服务器上无.ssh目录则先创建（mkdir ~/.ssh） 复制公钥至authorized_keys 1$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 更改文件及目录权限 12$ chmod 600 ~/.ssh/authorized_keys$ chmod 700 ~/.ssh 完成后即可不输入密码直接登录。]]></content>
      <categories>
        <category>工技篇之吴带当风</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2F2018%2F02%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new "My New Post" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>工技篇之曹衣出水</category>
      </categories>
  </entry>
</search>
