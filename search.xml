<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Permutation_combination]]></title>
    <url>%2Fblog%2F2019%2F09%2F07%2Fpermutation-combination%2F</url>
    <content type="text"><![CDATA[有重复的排列组合问题： 从n个元素中选出m个可重复的元素，一共有多少组合？ 思路：建立一个映射，使m个可重复的元素变成m个不可重复的元素，再使用公式。 具体来说，原始的m个元素的编号取值区间为1&lt;x&lt;n. 通过映射: xxx: x1x_1x​1​​, x2x_2x​2​​, x3x_3x​3​​, …, xmx_mx​m​​ f(x)f(x)f(x): x1x_1x​1​​+0, x2x_2x​2​​+1, x3x_3x​3​​+2, …, xmx_mx​m​​+m-1 使得新的m个元素的编号取值区间为1&lt;x&lt;(n+m-1)，一一对应并且元素不再重复，所以组合数为Cn+m−1mC_{n+m-1}^mC​n+m−1​m​​. 从n个元素中选出m个可重复的元素，并排序，一共有多少序列？ 是nmn^mn​m​​，还是An+m−1mA_{n+m-1}^mA​n+m−1​m​​，思考一下。 从n个元素中选出m个可重复的元素，并排成一个圆，旋转算作重复，一共有多少组合？ 从n个元素中选出m个可重复的元素，并排成一个圆，旋转+反转均算作重复，一共有多少组合？]]></content>
  </entry>
  <entry>
    <title><![CDATA[Logbook]]></title>
    <url>%2Fblog%2F2019%2F06%2F17%2Flogbook%2F</url>
    <content type="text"><![CDATA[何必等明天，从今天开始坚持写工作日志。 /* 2020.02.13 工作日志 */ Meeting Summary Thesis Project Propeller defect OpenFOAM Learning and working /* 2020.02.12 工作日志 */ 整理MPS结果 /* 2020.02.11 工作日志 */ 购买内存+SSD /* 2020.02.10 工作日志 */ Start Thesis Project /* 2020.02.09 工作日志 */ PPT update for 3D foil /* 2020.02.08 工作日志 */ 1 English 2 OpenFOAM 3 Slamming 1hour 4 Tecplot postprocessing /* 2020.02.07 工作日志 */ Dr. Li, an ophthalmologist, posted his story on Weibo from a hospital bed a month after sending out his initial warning about the novel Coronavirus. He had noticed seven cases of a virus that he thought looked like SARS - the virus that led to a global epidemic in 2003. On 30 December he send a message to fellow doctors in a chat group warning them to wear protective clothing to avoid infection. Four days later he was summoned to the Public Security Bureau where he was told to sign a letter. In the letter he was accused of “making false comments” that had “severely disturbed the social order”. Local authorities later apologised to Dr. Li. In his Weibo post he describes how on 10 January he started coughing, the next day he had a fever and two days later he was in hospital. He was diagnosed with the coronavirus on 30 January. /* 2020.02.06 工作日志 */ Submit 3D cases Validate MPS cases, Revise the paper OpenFOAM Mesh Run it first Revise the PPT, output Tecplot files /* 2020.02.04/05 工作日志 */ 重新明确各部职责，建立新部门。 /* 2020.02.03 工作日志 */ 安装OpenFOAM-dev /* 2020.02.01/02 工作日志 */ 2月加油！ /* 2020.01.31 工作日志 */ 今日解决review of hydroelastic tests /* 2020.01.30 工作日志 */ Make every effort to do NOT make all efforts to do !!! 牢记！！ /* 2020.01.27/28/29 工作日志 */ 整理3D cases results, compare with 2D and Four sections JOMAE paper Hydroelasticity /* 2020.01.26 工作日志 */ With recovery from an unprecedented blizzard in Eastern Newfoundland progressing, classes and other normal activity is set to resume on Memorial’s St. John’s campuses /* 2020.01.25 工作日志 */ 调整边界压力差值方程 整理3D Case 数据 /* 2020.01.24 工作日志 */ 该做好自己的事 /* 2020.01.23 工作日志 */ Review 为主 /* 2020.01.22 工作日志 */ debug为主，JOMAE还剩一周 /* 2020.01.20/21 工作日志 */ 今日任务 Compute Canada cases | Literature Review Check the results and evaluate | Do some post-processing Read Book of MPS / Structures | Make it run /* 2020.01.19 工作日志 */ No work /* 2020.01.18 工作日志 */ 解决了npsearch的问题，下面解决sendRecv的问题 /* 2020.01.17 工作日志 */ 知道了decomposePar的问题，下面解决npsearch的问题 /* 2020.01.16 工作日志 */ 找到问题出错的位置了，可是为什么？ /* 2020.01.15 工作日志 */ debug了一天，毫无头绪 /* 2020.01.14 工作日志 */ 为期一周JOMAE Cases调试开始 /* 2020.01.13 工作日志 */ Submit OMAE2020 Start Literature review /* 2020.01.11/12 工作日志 */ 抢救OMAE 论文 /* 2020.01.10 工作日志 */ 抢救OMAE Cases /* 2020.01.09 工作日志 */ it is with great sadness that I inform you that Professor John Shirokoff passed away today. Dr. Shirokoff was a valuable member of our Faculty of Engineering and Applied Science. He will be greatly missed. Our deepest sympathies and prayers are with his family at this extremely difficult time. Case 从V2 改为V4 试验数据见Panciroli_Riccardo_thesis /* 2020.01.08 工作日志 */ OMAE2020文章开工，一边写一边等数据 /* 2020.01.07 工作日志 */ 弄好slamming-fsi计算的reload 确定哪些case，与实验值对比 /* 2020.01.06 工作日志 */ 3D develop Tuck fellowship case review /* 2020.01.04/05 工作日志 */ OMAE2020 Cases submit two steps forward, one step back // making progress towards achieving something is not always going to be positive progress, and there may be minor set-backs. /* 2020.01.03 工作日志 */ OMAE2020 Case Summary /* 2020.01.02 工作日志 */ Cases submitted 翻译这个(Mars Uncovered Ancient God of War)严重低估了工作量 https://www.dailymotion.com/video/x7mrocz ultor ultor, ultoris masculine noun avenger, revenger /* 2020.01.01 工作日志 */ Long Pond Congress /* 2019.12.30/31 工作日志 */ 上午 下午 晚上 Code Code Code /* 2019.12.29 工作日志 */ 上午 下午 晚上 OMAE 2020 开工 /* 2019.12.23/24/25/26/27/28 工作日志 */ 上午 下午 晚上 撸代码; 提交Case /* 2019.12.22 工作日志 */ 上午 下午 晚上 3D pressure data / iwwwfb revision /* 2019.12.21 工作日志 */ 上午 下午 晚上 Exchange! /* 2019.12.20 工作日志 */ 上午 下午 晚上 What a sad day I will never come back /* 2019.12.19 工作日志 */ 上午 下午 晚上 增加算例 整理pressure data /* 2019.12.18 工作日志 */ 上午 下午 晚上 iwwwfb摘要 /* 2019.12.17 工作日志 */ 上午 下午 晚上 iwwwfb摘要 /* 2019.12.16 工作日志 */ 上午 下午 晚上 圣诞表演 /* 2019.12.15 工作日志 */ 上午 下午 晚上 Cedar Cases 下载 /* 2019.12.14 工作日志 */ 上午 下午 晚上 iwwwfb大摘要 /* 2019.12.13 工作日志 */ 上午 下午 晚上 autorun_3D_goingon /* 2019.12.12 工作日志 */ 上午 下午 晚上 Cases: No defect N4: cluster Node: 26 No defect P4: Niagara/beluga No defect Z0: Niagara/beluga 0.5mm defect N4: cluster Node: 25 0.5mm defect P4: cluster Node: 27 0.5mm defect Z0: cluster Node: 28 注意log文件命名错误 p distribtion iwwwfb开工 /* 2019.12.11 工作日志 */ 上午 下午 晚上 p distribtion /* 2019.12.10 工作日志 */ 上午 下午 晚上 report 检查reduction figure 画fs identification图 提交Case Pressure data 整理 计算GCI的公式有bug; r32与r21差别不能太大，否则对于震荡收敛的数据 p无法用固定点法求解。 /* 2019.12.08/09 工作日志 */ 上午 下午 晚上 3D case 重新计算 数据整理 Report iwwwfb论文 /* 2019.12.07 工作日志 */ 上午 下午 晚上 fsRatio convergence studies /* 2019.12.06 工作日志 */ 上午 下午 晚上 改话剧剧本 /* 2019.12.05 工作日志 */ 上午 下午 晚上 修正mpsStructure里的isotropic pressure计算 Schopenhauer’s saying, that “a man can do as he will, but not will as he will,” has been an inspiration to me since my youth up, and acontinual consolation and unfailing well-spring of patience in the face of the hardships of life, my own and others’. This feeling mercifully mitigates the sense of responsibility which so easily becomes paralysing, and it prevents us from taking ourselves and other people too seriously; it conduces to a view of life in which humour, above all, has its due place. 分别13年后，秒速无厘米：0.05×13×365×24×60×60=20498.4km 等于南极和北极的距离 /* 2019.12.03/04 工作日志 */ 上午 下午 晚上 安装OpenFOAM 发射任务 整理数据 /* 2019.12.02 工作日志 */ 上午 下午 晚上 压力不光顺还是不太行 /* 2019.12.02 工作日志 */ 上午 下午 晚上 改用系数cn(1)=1.5,cn(2)=0.5 for fluid; cn(1)=1.0,cn(2)=0 for structure 调整strain gauge计算方法 /* 2019.12.01 工作日志 */ 上午 下午 晚上 确认：边界条件压力差值只需g，加body_acce的话会导致更大的oscillation 简化：extrapolation 为啥要定义个全局particle extra变量 /* 2019.11.29/30 工作日志 */ 上午 下午 晚上 forceFtoS 改为压力梯度 结构求解器用新的时间步进 对比以前的case设置，找到抖动原因 /* 2019.11.28 工作日志 */ 上午 下午 晚上 Linear multistep method /* 2019.11.26/27 工作日志 */ 上午 下午 晚上 理解PISO 理解flux limiter /* 2019.11.25 工作日志 */ 上午 下午 晚上 g+acce for the boundary dummy particles interface压力光滑,法向量求解 (done) 秋风清，秋月明， 落叶聚还散，寒鸦栖复惊。 相思相见知何日？此时此夜难为情！ 入我相思门，知我相思苦， 长相思兮长相忆，短相思兮无穷极， 早知如此绊人心，何如当初莫相识。 /* 2019.11.24 工作日志 */ 上午 下午 晚上 debug(done) Arc method for free surface identification /* 2019.11.23 工作日志 */ 上午 下午 晚上 dynamite 炸药 tar 柏油 leech 水蛭 slob 搞清楚piso如何修正速度的 /* 2019.11.22 工作日志 */ 上午 下午 晚上 测试arc method 流水不腐，户枢不蠹(du) 书虫即衣鱼（silverfish） /* 2019.11.21 工作日志 */ 上午 下午 晚上 1.对比pbicgstab与bicgstab，简单的precondition matrix效果不大 2.优化free surface identify (done) 3.继续昨天的任务 BUG: mpsFluid 第652行 sendRecvData 数组长度 注意同时更新V和r时，前面有没有重复使用r这个变量 /* 2019.11.20 工作日志 */ 上午 下午 晚上 Brainstorm: a 取平均值 Newmark-beta (done) interface压力光滑,法向量求解 (done) poisson equation源项 XMPS应用到structure求解 增加structure时间步长 注意计算顺序 待完成： 半隐式求解structural equations 参考SPH-FSI 高阶的梯度和散度公式 structural particles 简单并行 起步： JSR cases 3D db /* 2019.11.19 工作日志 */ 上午 下午 晚上 整理JSR cases 务必未雨绸缪 测试MPS for structure, 参考SPH计算 测试bicgstab与pbicgstab /* 2019.11.18 工作日志 */ 上午 下午 晚上 eternal recurrence is a theory that the universe and all existence and energy has been recurring, and will continue to recur, in a self-similar form an infinite number of times across infinite time or space. Nietzsche’s first mention of eternal recurrence, in aphorism 341 of The Gay Science (cited below), presents this concept as a hypothetical question rather than postulating it as a fact. Review FSI用SPH怎么耦合的 现有结果优化 3D dambreak with wave impact /* 2019.11.15/16/17 工作日志 */ 上午 下午 晚上 学生营 /* 2019.11.14 工作日志 */ 上午 下午 晚上 搞定makefile 正式决定加入字幕组翻译 /* 2019.11.12/13 工作日志 */ 上午 下午 晚上 Euler Implicit 重写bicgstable加precondition 测试简单的3D parallel /* 2019.11.11 工作日志 */ 上午 下午 晚上 reprot /* 2019.11.10 工作日志 */ 上午 下午 晚上 In my opinion, there are a few possible meanings: the person doesn’t know the answer and is trying to think of an appropriate way to answer the question without sounding like they don’t know the answer. the person hasn’t thought of this question before and genuinely thinks it’s a good question, because it is insightful or creative. the person uses it sarcastically to indicate that the person’s question is dumb and not worth answering. the person anticipated being asked that question and is excited to answer it. /* 2019.11.09 工作日志 */ 上午 下午 晚上 修改PPT，报告，整理case 画Cp contour和curves /* 2019.11.08 工作日志 */ 上午 下午 晚上 修改PPT，报告，整理case /* 2019.11.07 工作日志 */ 上午 下午 晚上 The induced velocity of a rotating propeller includes three velocity components, radial, tangential and axial. The radial component of the induced velocity can be ignored based on the assumptions that there is no contraction or reduction in diameter of the slipstream. The other two components have a major influence on the angle of attack although the induced velocity is small as compared to the inflow velocities for a moderately loaded propeller. For estimation, the propeller blade can be considered as a lifting line. The components of the induced velocity at the lifting line are one half the values downstream. It is also assumed that the resultant induced velocity is perpendicular to the resultant inflow velocity. /* 2019.11.05/06 工作日志 */ 上午 下午 晚上 实现Newmark method and Wilson-theta method She explains that even a short exposure to the unhealthy or worse pollution levels can trigger an asthma attack or increase the risk of a stroke. The longer the exposure, the greater the risks. The ways to protect oneself are limited. There’s the advice to stay indoors, to reduce physical exercises and to wear a mask - but in many poorer parts of the world, none of these options really work for regular people. It must be very uncomfortable - especially if you’re having to work outside and if you’re having to do jobs that require quite a lot of energy. /* 2019.11.04 工作日志 */ 上午 下午 晚上 Test 2D wedge slamming with FSI That’s because the 29-year-old Tillsonburg, Ont., resident has Down syndrome, and devices like Google Home often have trouble understanding people who don’t use typical speech patterns. /* 2019.11.03 工作日志 */ 上午 下午 晚上 设置reinforced tip并测试 计算beta_i of propeller /* 2019.11.02 工作日志 */ 上午 下午 晚上 搞懂流体和固体中的Strain-Stress关系 /* 2019.11.01 工作日志 */ 上午 下午 晚上 改报告，改ppt Case set-up，加上reinforced tip /* 2019.10.31 工作日志 */ 上午 下午 晚上 Halloween address the efficiency: L/D Case set-up /* 2019.10.30 工作日志 */ 上午 下午 晚上 改PPT，算uncertainty 改code–&gt;FSI /* 2019.10.29 工作日志 */ 上午 下午 晚上 回答问题 用KCS螺旋桨数据 done 改PPT，算uncertainty 改code–&gt;FSI /* 2019.10.28 工作日志 */ 上午 下午 晚上 回答问题 改PPT，算uncertainty 改code–&gt;FSI /* 2019.10.27 工作日志 */ 上午 下午 晚上 20世纪60年代末，一位名叫利福德·盖尔茨的美国人类文化学家，曾在爪哇岛生活过。这位学者，无心观赏岛上美景，只潜心研究当地的农耕生活。他眼中看到的都是犁耙收割，日复一日，年复一年，原生态农业在维持着田园景色的同时，长期停留在一种简单重复、没有进步的轮回状态。这位学者把这种现象冠名为“内卷化”。第二次世界大战之后的民族独立未有带来印度尼西亚的快速发展，人口涌入城市却没有带来工业化，引起的只不过是以原来在农业中出现的共同分担贫困的模式拓展到城市中跨国公司通过开采国家资源控制贫困人口，同时提升了精英的权势，使得贫苦民众生活更难以受到制度保障，经济的增长也无益于大众。 (Involution) /* 2019.10.26 工作日志 */ 上午 下午 晚上 debug 别人多找给你钱的例子：活在别人的论断当中 /* 2019.10.25 工作日志 */ 上午 下午 晚上 Send PPT and give the supervisor report Prepare for the VISA to Norway /* 2019.10.24 工作日志 */ 上午 下午 晚上 ppt literature review /* 2019.10.23 工作日志 */ 上午 下午 晚上 hull girder vibration induced by bow slamming impacts is called whipping. There is growing interest in CFD application for seakeeping and load analysis. /* 2019.10.22 工作日志 */ 上午 下午 晚上 Develop PPT Revise PPT Uncertainty analysis cases OpenFOAM data /* 2019.10.21 工作日志 */ 上午 下午 晚上 develop restart feature: still have k values to be done. /* 2019.10.20 工作日志 */ 上午 下午 晚上 CFD是白箱模型，ML是黑箱模型。白箱模型是不需要数据的物理模型，你有一系列的公式可以将input转化为output。黑箱是你已知一定量的input和output，通过两者之间的correlation来建立的模型，but correlation does not imply causation，所以用特定的field data去train的时候，得到的模型不一定适用于所有的情况。 /* 2019.10.20 工作日志 */ 上午 下午 晚上 做mps-proposal.ppt /* 2019.10.19 工作日志 */ 上午 下午 晚上 改动： nomenclature Table 3.3 Table 3.4 Table 3.5 Table 3.6 Figure 3.14 Fig. 3.15 Fig. 3.16 /* 2019.10.18 工作日志 */ 上午 下午 晚上 计算cavitation bucket 测试功能restart /* 2019.10.17 工作日志 */ 上午 下午 晚上 计算cavitation bucket 测试功能 delay and restart /* 2019.10.16 工作日志 */ 上午 下午 晚上 打电话催才拿到成绩单，提交Canada Visa 接下来是Norway Visa，调查并联系Yuzhu 改report，这周算完。 继续Develop /* 2019.10.15 工作日志 */ 上午 下午 晚上 Update intel fortran Serial killer 连环杀手 retaliate 报复 -&gt; retaliation 保持每天的英语输入 /* 2019.10.14 工作日志 */ 上午 下午 晚上 Progress Progress Development 1 提交cases，不同AR at trailing edge 2 计算inception speed reduction 3 progress report 4 Develop code /* 2019.10.13 工作日志 */ 上午 下午 晚上 Progress Progress Development 开始面向对象训练：python3 /* 2019.10.12 工作日志 */ 上午 下午 晚上 Progress Progress Development Project /* 2019.10.11 工作日志 */ 上午 下午 晚上 Progress Progress Development 改论文 /* 2019.10.10 工作日志 */ 上午 下午 晚上 Progress Progress Development 改论文 /* 2019.10.9 工作日志 */ 上午 下午 晚上 Progress Progress Development 改论文 /* 2019.10.8 工作日志 */ 上午 下午 晚上 Progress Progress Development 改论文 /* 2019.10.7 工作日志 */ 上午 下午 晚上 Progress Progress Development 检查foil结果，提交openfoam续算 /* 2019.10.6 工作日志 */ 上午 下午 晚上 Progress Progress Development Journal of Ship Research 加油！ /* 2019.10.5 工作日志 */ 上午 下午 晚上 Progress Progress Development The fluid is isotropic, thus its properties are independent of direction. /* 2019.10.4 工作日志 */ 上午 下午 晚上 Progress Progress Development 剪刀差（price scissors）指工农业产品交换时，工业品价格高于价值，农产品价格低于价值所出现的差额。剪刀差是发达国家在国际贸易中的一种重要交换手段之一。 它是体现在以掌握高端生产技术为主的资本密集产业利用垄断地位来获得相较于劳力密集产业的产品利润优势控制发展中国家的对外贸易，一方面压低发展中国家生产的初级产品的國際市场价格，另一方面又透过专利保护法案等一系列防止己国较先进技术流入到其他国家的有效保护措施来提高自身生产的拥有高附加值的工业制成品的世界市场的价格。如果把这一现象用价格走势图表示出来就像一把张开的剪刀。主要集中在发达国家的资本密集型产业链所产出的相关产品在遵循国际贸易法规的前提之下通过跨境交易的手段赚取高额利润。 /* 2019.10.3 工作日志 */ 上午 下午 晚上 Progress Progress Development Learn FDM -&gt; FVM /* 2019.10.2 工作日志 */ 上午 下午 晚上 Progress Progress Development talked with Muk Chen Ong, very astonishing know what should I do learn these in three months /* 2019.10.1 工作日志 */ 上午 下午 晚上 Progress Progress Development Write reply/progress report Develop restart feature/Kalman Prepare for Norway (Visa, Accomodation info, funding, International student advisor) /* 2019.09.29/30 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper submit paper and progress report /* 2019.09.25/26/27/28 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper 改写。 /* 2019.09.23/24 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper 测试blog中的code block /* 2019.09.22 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper 最后两天 /* 2019.09.21 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper 中秋迎新 /* 2019.09.20 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper 我也要学小波分析 /* 2019.09.19 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper 在最后的修改。 /* 2019.09.18 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper 写文章发现很多基本的格式没有搞清楚为什么，比如为什么乘以dimension，正负号的定义问题。有时间必须好好推导一下。 /* 2019.09.17 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper 今天无论如何必须完成Introduction/Numerical methods /* 2019.09.16 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper 今天无论如何必须完成Introduction/Numerical methods /* 2019.09.15 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA know better than to do sth: to be wise or moral enough not to do something /* 2019.09.14 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA laugh all the way to the bank 狠赚一笔（贬） /* 2019.09.13 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA /* 2019.09.12 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper 终于处理完了数据，下面就是写文字部分了. /* 2019.09.11 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper afoil with 0.25mm defect at -4 degrees is too difficult to converge. /* 2019.09.10 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper Teacher’s day /* 2019.09.09 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper technical glitch /* 2019.09.08 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper /* 2019.09.07 工作日志 */ 上午 下午 晚上 Journal paper Journal paper Journal paper 有重复的排列组合问题： 1 从n个元素中选出m个可重复的元素，一共有多少组合？ 2 从n个元素中选出m个可重复的元素，并排序，一共有多少序列？ 3 从n个元素中选出m个可重复的元素，并排成一个圆，旋转算作重复，一共有多少组合？ 4 从n个元素中选出m个可重复的元素，并排成一个圆，旋转+反转均算作重复，一共有多少组合？ /* 2019.09.06 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 最后一个周末 /* 2019.09.05 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 最后一天！ /* 2019.09.04 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 最后两天！ We need to reduce our carbon footprints so that Prince Harry can still use his private jet. Each year, to offset the royal family’s annual carbon footprint, a resident of the UK must give up driving for 21 years. /* 2019.09.03 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 最后两天！ 顺便准备progress report /* 2019.08.31 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 最后三天！ /* 2019.08.30 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 暂停improve wedge05, 先写完paper再改。 /* 2019.08.29 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 无干扰写一天的paper /* 2019.08.28 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 无干扰写一天的paper /* 2019.08.27 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 无干扰写一天的paper /* 2019.08.26 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper Cp on foil surface for 0.5mm defect at 1 degree was oscillating between 0&lt;x&lt;0.01. /* 2019.08.25 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 闻鸡起舞：东晋祖逖（ti四声） 与习空刘琨俱为司州主簿，情好绸缪，共被同寝，中夜闻荒鸡鸣，蹴琨觉曰：‘此非恶声。’因起舞（剑）。逖琨并有英气，每语世事或中宵起坐，相谓曰：‘若四海鼎沸，豪杰并起，吾与足下当相避于中原。’ /* 2019.08.24 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 写文章理论部分。 /* 2019.08.23 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 写文章理论部分。 /* 2019.08.22 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 把wedge05 的速度曲线平滑一下，重新插值看看结果有没有改进 /* 2019.08.21 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper Meeting /* 2019.08.20 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper wedge5又出问题。 We had an (连读) all night bull session. /* 2019.08.19 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper AR80 at Trailing Edge勉强还行 segment=0.1c /* 2019.08.18 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 整理Cp，哪些不收敛。 /* 2019.08.17 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 回邮件中提到的问题。 /* 2019.08.16 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 写proposal on exchange /* 2019.08.15 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 处理完wedge cases /* 2019.08.14 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper 补办mcp，准备学费，计算消费。 /* 2019.08.13 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Project NACA 整理wedge cases 想想proposal怎么写，去挪威提上日程。 /* 2019.08.12 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Project NACA 整理wedge cases 钓鱿鱼 /* 2019.08.11 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Project NACA 减小trailing edge上的网格点： 增大AR /* 2019.08.10 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Project NACA 对比新旧grids的结果 /* 2019.08.09 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Project NACA update项目进度，减小机翼表面节点数的计算结果与收敛性。update wedge05的计算结果. /* 2019.08.08 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Project NACA 坎城就是Canas，戛纳，jia na电影节/坎城影展 交流电： Alternating current (AC) 直流电： Direct current (DC) /* 2019.08.07 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Project NACA 重大更新 kalm.py: add a delay for the impact pressure add x0 * Q/R * 10 无论如何这周要给出文章。 /* 2019.08.06 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Project NACA 解决了wedge01,进军wedge03 /* 2019.08.05 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Project NACA 忧郁的感觉越来越严重了，不过相信我，everything will have a solution. scum /* 2019.08.04 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Project NACA 补充画图 3号在家休息，没到学校，未更新日志。 /* 2019.08.02 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Project NACA 补充算例 /* 2019.08.01 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Project NACA 画图的一天。。 /* 2019.07.31 工作日志 */ 上午 下午 晚上 Project NACA Journal paper Journal paper It was a challenge, keeping ten boisterous seven-year-olds amused. How do you deal with persistent salesmen who won’t take no for an answer? The student’s unassuming behavior meant that the teacher hardly noticed her. /* 2019.07.30 工作日志 */ 上午 下午 晚上 — Journal paper Journal paper 注释应该首先是对动机的描述，其次才是对代码的补充描述。 使用带delay的correction Men are constantly fiddling around with their new gadgets. Women are more down to earth and see devices more rationally as a means to communicate or to perform a particular function. This is pretty stereotyped, but it seems to hold true for most men and women I know. /* 2019.07.29 工作日志 */ 上午 下午 晚上 — Journal paper Journal paper result of wedge01 粒子数多时结果反而偏小，尝试解决。 /* 2019.07.28 工作日志 */ 上午 下午 晚上 — Journal paper Journal paper 解决wedges and ship09, 还没有完成的算例继续计算，现有的完成处理。 /* 2019.07.27 工作日志 */ 上午 下午 晚上 Project NACA/Journal paper Project NACA Project NACA/Journal paper 解决ship07 No excuse 周六周日两天特别作战 江流宛转绕芳甸，月照花林皆似霰。 空里流霜不觉飞，汀上白沙看不见。 /* 2019.07.26 工作日志 */ 上午 下午 晚上 Project NACA/Journal paper Project NACA Project NACA/Journal paper 今日任务继续，解决ship07, 特别是uncertainty analysis 周六周日两天特别作战 春江潮水连海平，海上明月共潮生 滟滟随波千万里，何处春江无月明 /* 2019.07.25 工作日志 */ 上午 下午 晚上 Project NACA/Journal paper Project NACA Project NACA/Journal paper 今日任务，解决ship07, 特别是uncertainty analysis /* 2019.07.24 工作日志 */ 上午 下午 晚上 Project NACA/Journal paper Project NACA Project NACA/Journal paper paper writing处理数据，计算uncertainty 修改报告精简版 Boris Johnson’s first speech as prime minister /* 2019.07.23 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA paper writing今日两小时 修改报告精简版 /* 2019.07.22 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA Shell 数组很麻烦，不如用python 抓紧paper writing /* 2019.07.21 工作日志 */ 上午 下午 晚上 - Project NACA Project NACA summarize要修改的内容，要计算的工作 后处理 journal paper 如果你不能直面自己逃避的东西，生活会重复，哪里都没有出口 /* 2019.07.20 工作日志 */ 上午 下午 晚上 睡觉 睡觉 Journal 2019 Clannad： Clann as d：凯尔特语中的家族 d=dango团子 团子大家族 凉宫春日的忧郁，漫无止境的八月 /* 2019.07.19 工作日志 */ 会议总结：还是不太会说话。 做presentation：1.尽量不要对着念。2.保持流利的话说话要连贯，要把逻辑关系理顺，逻辑的地位要在语言表达技巧之上。 3.好好想想如何多多参与别人的讨论，即使说的不好也比一直沉默的好。机会难得，要多发声。 /* 2019.07.18 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA Meeting /* 2019.07.17 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA 最后修改PPT 凌晨3点出发前往Halifax /* 2019.07.16 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA 最后修改PPT 准备演讲稿 /* 2019.07.15 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA 准备周四汇报的PPT 2D + 3D Final Report /* 2019.07.14 工作日志 */ 上午 下午 晚上 - - 夏令会结束 准备周四汇报的PPT。 解决 potential flow part 2D + 3D 身无长处唯勇往直前。 /* 2019.07.12 工作日志 */ 上午 下午 晚上 Project NACA Project NACA 夏令会开始 准备周四汇报的PPT。 7月13日放假。 /* 2019.07.11 工作日志 */ 上午 下午 晚上 Project NACA Project NACA journal paper 检查report; 检查OpenFOAM/NACA66-with-best-practice算例 Latex目录标题太长一般会自动换行。但如果使用了\usepackage{hyperref},会导致标题挤在一行。解决方法有： %make page number, not text, be link on toc,lof,lot \usepackage[linktocpage]{hyperref} %Break the long titles, but lose the hyerlink %\usepackage[breaklinks=true]{hyperref} \usepackage{tocbasic} \DeclareTOCStyleEntry[dynnumwidth]{tocline}{figure} \DeclareTOCStyleEntry[dynnumwidth]{tocline}{table} /* 2019.07.10 工作日志 */ 上午 下午 晚上 Project NACA Project NACA journal paper 检查report; 检查OpenFOAM算例 “直面困难的勇气”，我好像失去了，如今我要找回来。 \addcontentsline{toc}{section}{} \addcontentsline{toc}{subsection}{} /* 2019.07.09 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA 今天终于要finish了，剩下来的工作就是修修补补和做PPT了。 周五之前搞定自己的paper，确定所有的数据。 It really appeals to me/ tickles my fancy/ floats my boat. /* 2019.07.08 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA LaTex表格画横线：\cline; 画竖线：\hline。注意写在\之后。\cline{2-3}表示在第2到第3列当中画横线。 /* 2019.07.07 工作日志 */ 上午 下午 晚上 … … Project NACA Flatrock Read the paper by Terry Brockett, 1966. /* 2019.07.06 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA Finish the plots the results for 2D rectangular domain /* 2019.07.05 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA Plot the results for 2D rectangular domain /* 2019.07.04 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA 筋疲力尽，还是先昨晚这个2D report再说吧。 7月18日去哈法开会。 /* 2019.07.03 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA；journal paper Niagara测试stopping criteria(暂停） 3D boundary conditions 2D report, add rectangle domain results, add Iteration Error of Cpmin slamming journal Write the reply, must be done today 晚上内容：Give cases to Jin /* 2019.07.02 工作日志 */ 上午 下午 晚上 Project NACA journal paper Project NACA；journal paper Niagara测试stopping criteria 3D boundary conditions 2D report, double-check slamming journal Write the reply 晚上内容：update 3D absolute pressure /* 2019.07.01 工作日志 */ 上午 下午 晚上 Project NACA journal paper Project NACA；journal paper 晚上内容： Project NACA finish report, check it tomorrow 提交cases：使用Cpmin的monitor plot作为stopping criteria, 设置一个 Asymptotical criteration Write the paper, write the reply 一点思考：导致焦虑的原因之一是担心这几年虚度年华，啥也没有学到。其实，至少学到了一点，如何应对tough life和mean boss，疏导bad mood，更高阶的是取悦自己。所以对自己的最低要求就是学好英语以及表达能力。 /* 2019.06.30 工作日志 */ 上午 下午 晚上 … journal paper Project NACA；journal paper 晚上内容： wedge domain investigation; 查看residual of Cpmin bragging: 吹牛 /* 2019.06.29 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA；journal paper 晚上内容： 测试monitor Cp_min 回答Dr Qiu’s comments. Life begins at the edge of your comfort zone. /* 2019.06.28 工作日志 */ 上午 下午 晚上 NRC-MUN Collabration Project NACA Project NACA；journal paper 晚上内容： finsh laTex code, journal paperwriting 回答Dr Qiu’s comments. /* 2019.06.27 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA；journal paper 晚上内容： finsh laTex code 回答Dr Qiu’s question. /* 2019.06.26 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA；journal paper 晚上内容： finsh laTex code MUSCL stands for Monotonic Upwind Scheme for Conservation Laws. Convection scheme in STAR-CCM+: 1st/2nd order upwind, central differencing (CD), Hybrid MUSCL/CD. Select it in Contunua-Physics-Models-Segregated Flow-Properties. 专注提升效率和英语。这是最低要求。在学校可以找借口、慢热，出了学校那就是实时的比拼，效率的高低决定自己的水平。一方面要做Project养活自己，另一方面也要做出自己的东西。 This above all: to thine own self be true. /* 2019.06.25 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA；journal paper 晚上内容： finsh laTex code Foil表面wall边界条件的omega不是0, 而是个很大的数， in OpenFOAM。 /* 2019.06.24 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA；journal paper 晚上内容：finish ploting, finsh laTex code 司马光《资治通鉴》 /* 2019.06.23 工作日志 */ 上午 下午 晚上 nothing Project NACA Project NACA；journal paper 晚上内容：重新读一下Cd Cl，图不画完不睡觉 python读数组a[0:2]指的是第一列0和第二列1,不包括2 /* 2019.06.22 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA；journal paper 提交了extension of study permit 晚上内容： Copy data using shell scripts 把旧的计算结果PressureOnFoil输出格式改为PressureOnFoilWithXYZ Project harddisk有坏数据input/output error /* 2019.06.21 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA；journal paper 晚上内容：整理一个template for all cases，2表4图加标题说明。 /* 2019.06.20 工作日志 */ 上午 下午 晚上 Project NACA Project NACA Project NACA；journal paper 晚上内容： 不同turbulence model计算的残差项不同。 每个case整理：2个table加4个图。 续学签。 /* 2019.06.19 工作日志 */ 上午 下午 晚上 MPS cases MPS cases Project NACA；journal paper 晚上内容： wedge05 case pressure curves的第二个凸起可能来自于速度曲线的拐点。找到原因：domain width不够大。 plot naca cases. /* 2019.06.18 工作日志 */ 上午 下午 晚上 Project NACA 2D cases; MPS cases Project NACA and MPS cases Project NACA；journal paper 晚上内容： journal case 新增列表、提交排队 2D cases 数据整理 extend study permit 查看requirement 在紧张的情况下，当你的思维变得混乱（thinking is cloudy)时，如何避免重大的错误: https://www.ted.com/talks/daniel_levitin_how_to_stay_calm_when_you_know_you_ll_be_stressed?language=zh-cn 你所追求的爱情，只不过是人生的幻光。 /* 2019.06.17 工作日志 */ 上午 下午 晚上 Project NACA: review 3D outlet B.C. Project NACA and MPS cases Project NACA 何必等明天，从今天开始坚持写工作日志。立个flag看能不能写满一个月。中文为主，尽最大可能集中到内容上，保持简单。 准备两篇journal来结束2D的MPS工作。 每天一句话充实自己内心。&quot;后世贤,师吾俭;不贤,毋为势家所夺。&quot;萧何。 晚上内容： journal case查看，补漏 2D cases 数据整理 3D cases in literature 列表与计算 (A numerical and experimental study on the drag of a cavitating underwater vehicle in cavitation tunnel) 3D with foil 计算结果下载]]></content>
      <categories>
        <category>旅程</category>
      </categories>
      <tags>
        <tag>Seed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction to Bernoulli's principle]]></title>
    <url>%2Fblog%2F2019%2F03%2F19%2FBernoulli%2F</url>
    <content type="text"><![CDATA[Introduction Within a fluid flowing horizontally, the highest speed occurs where the pressure is lowest, and the lowest speed occurs where the pressure is highest. Why? In fluid mechanics, Bernoulli’s principle states that, in a steady flow, the sum of all forms of energy in a fluid () along a streamline remains constant. The forms of energy involves the kinetic energy, the potential energy and the internal energy. Bernoulli’s principle can also be derived directly from Newton’s Second Law of Motion. kinetic energy: dynamic pressure (0.5ρU20.5\rho U^20.5ρU​2​​) potential energy: static pressure, including the gravitational potential internal energy: heat The principle is named after Daniel Bernoulli who published it in his book Hydrodynamica in 1738. Leonhard Euler derived Bernoulli’s equation in its usual form in 1752. The principle is only applicable for isentropic flows: when the effects of irreversible processes (like turbulence) and non-adiabatic processes (e.g. heat radiation) are small and can be neglected. Pressure classification Static pressure Dynamic pressure Total pressure static pressure + dynamic pressure = total pressure p0=p+12ρU2p_0 = p + \frac{1}{2}\rho U^2 p​0​​=p+​2​​1​​ρU​2​​ Hydrostaic pressure (gravitational potential) ps=ρghp_s = \rho gh p​s​​=ρgh Bernoulli’s equation]]></content>
      <tags>
        <tag>Marine Hydrodynamics</tag>
        <tag>Fluid mechanics</tag>
        <tag>Propulsion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谢道韫]]></title>
    <url>%2Fblog%2F2019%2F02%2F07%2FDaoyun-Xie%2F</url>
    <content type="text"><![CDATA[谢道韫，字令姜，生卒年不详，东晋（317年4月6日－420年7月10日）时期有名的才女，典型的白富美。韫有收藏，蕴藏之意，如「石韫玉而山辉，水怀珠而川媚」。 谢道韫有不少有名的典故。《世说新语•言语》中记载了这么一个故事： 谢太傅寒雪日内集，与儿女讲论文义。俄而雪骤，公欣然曰：“白雪纷纷何所似？”兄子胡儿曰：“撒盐空中差可拟。”兄女曰：“未若柳絮因风起。”公大笑乐。即公大兄无奕女，左将军王凝之妻也。 谢太傅就是宰相谢安。胡儿是谢道韫的哥哥谢朗的小名。红楼梦中有用典「可叹停机德，堪怜咏絮才!玉带林中挂，金簪雪里埋。」这个故事也就是“咏絮之才”的来源。可以说千年来咏雪之诗词，没有哪句能超过这句「未若柳絮因风起」。 谢家乃名门望族。唐刘禹锡的《乌衣巷》写道，「旧时王谢堂前燕，飞入寻常百姓家。」其中的「谢」就是东晋时期的陈郡谢氏。谢家出过不少将军、宰相、诗人。谢道韫的父亲是安西将军谢奕，哥哥是大将军谢玄，叔父是当时的宰相谢安。比较有名的还有才高八斗的发明人谢灵运，李白的爱豆大诗人谢朓等。 说到这里，不得不提下谢道韫的丈夫，王凝之。此公名气稍微差点，不过也不算太差，他爸是王羲之。虽然谢家与王家可以说门当户对，王羲之的几个儿子大部分也还凑合，但这个王凝之却十分草包。谢道韫极其看不起王凝之。《晋书·列女传》记载了谢道韫第一次回娘家时候跟家人抱怨丈夫的趣事： 初适凝之，还，甚不乐。安曰：“王郎，逸少子，不恶，汝何恨也？”答曰：“一门叔父则有阿大、中郎，群从兄弟复有封、胡、羯、末，不意天壤之中乃有王郎！”封谓谢韶，胡谓谢朗，羯谓谢玄，末谓谢川，皆其小字也。 谢安让谢道韫嫁给王凝之，本以为他是个稳重靠谱的老实人，没想到看走了眼。谢道韫出嫁后第一次回娘家，很不开心，埋怨说自己的兄弟和长辈都很有名气与才华，自己的丈夫理应同样优秀，想不到却嫁了个脑残。王凝之历任江州刺史、左将军、会稽内史等职。他狂热信奉五斗米道。晋安帝隆安三年（399年）孙恩造反，兵临会稽城下，会稽内史王凝之毫无战意，只是一直烧香拜佛。他对部下说，“我已经向大仙请示，大仙会借给我数万天兵天将驻守各个要塞，不用担心反贼。”十一月孙恩攻陷会稽，王凝之出逃未遂被抓，与子女一起被杀。谢道韫听说敌人来了，甚至自己拿刀杀敌数人，后才被俘。《资治通鉴》记载： 凝之妻谢道蕴，弈之女也，闻寇至，举措自若，命婢肩舆，抽刀出门，手杀数人，乃被执。 当时她的外孙刘涛才几岁大，反贼也要杀之，谢道韫怒道，“事在王门，何关他族！必其如此，宁先见杀。”孙恩这个人虽然毒辣，但也感其节义也敬这位才女，赦免了谢道韫等非王姓族人。王凝之死后谢道韫没有再嫁，也没有回娘家，留在会稽（大概是今天的绍兴）度过了余生。 谢道韫的代表作有《拟嵇中散咏松》等。《拟嵇中散咏松》是模仿她的偶像嵇康的《游仙诗》诗而作的。 遥望山上松，隆冬不能凋。 愿想游下憩，瞻彼万仞条。 腾跃未能升，顿足俟王乔。 时哉不我与，大运所飘揺。 谢道韫自幼聪识，有才辩。后半生却在叛乱中失去了丈夫和子女，大起大落。生于乱世，人生苦短，命途难测。只愿归来还是那个咏雪的少女。]]></content>
      <categories>
        <category>木雁之间</category>
      </categories>
      <tags>
        <tag>History</tag>
        <tag>Figure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction to Partial Differential Equation (PDE)]]></title>
    <url>%2Fblog%2F2019%2F01%2F08%2FPartial-Differential-Equations%2F</url>
    <content type="text"><![CDATA[Introduction PDE can be classified as elliptic, parabolic or hyperbolic. In fluids dynamics, the governing equations contain first and second derivatives in the spatial coordinates and first derivatives only in time. The spatial derivatives often appear nonlinearly while the time derivatives appear linearly. For linear PDE of second-order in two independent variables xxx and yyy, A∂2u∂x2+B∂2u∂x∂y+C∂2u∂y2+D∂u∂x+E∂u∂y+Fu+G=0A\frac{\partial^2 u}{\partial x^2} + B\frac{\partial^2 u}{\partial x \partial y} + C\frac{\partial^2 u}{\partial y^2} + D\frac{\partial u}{\partial x} + E\frac{\partial u}{\partial y} + Fu + G =0 A​∂x​2​​​​∂​2​​u​​+B​∂x∂y​​∂​2​​u​​+C​∂y​2​​​​∂​2​​u​​+D​∂x​​∂u​​+E​∂y​​∂u​​+Fu+G=0 A simple classification is as follows. If B2−4AC&lt;0B^2 - 4AC &lt; 0B​2​​−4AC&lt;0: elliptic PDE If B2−4AC=0B^2 - 4AC = 0B​2​​−4AC=0: parabolic PDE If B2−4AC&gt;0B^2 - 4AC &gt; 0B​2​​−4AC&gt;0: hyperbolic PDE In many cases, B=0B=0B=0. Under this condition, If AC&gt;0AC &gt; 0AC&gt;0: elliptic PDE If AC=0AC = 0AC=0: parabolic PDE If AC&lt;0AC &lt; 0AC&lt;0: hyperbolic PDE Why? Lets see the characteristics for a single first-order PDE (*): A∂u∂t+B∂u∂x=CA\frac{\partial u}{\partial t} + B\frac{\partial u}{\partial x} = C A​∂t​​∂u​​+B​∂x​​∂u​​=C The characteristic direction is defined by dxdt=BA\frac{d x}{d t} = \frac{B}{A} ​dt​​dx​​=​A​​B​​ in which the relation between xxx and ttt can be built (See CCC=0). It is known that the definition of the total differential is: du=∂u∂tdt+∂u∂xdxdu = \frac{\partial u}{\partial t}dt + \frac{\partial u}{\partial x}dx du=​∂t​​∂u​​dt+​∂x​​∂u​​dx dudt=∂u∂t+∂u∂xdxdt\frac{du}{dt} = \frac{\partial u}{\partial t} + \frac{\partial u}{\partial x} \frac{dx}{dt} ​dt​​du​​=​∂t​​∂u​​+​∂x​​∂u​​​dt​​dx​​ Adudt=A∂u∂t+A∂u∂xdxdtA \frac{du}{dt} = A \frac{\partial u}{\partial t} + A \frac{\partial u}{\partial x} \frac{dx}{dt} A​dt​​du​​=A​∂t​​∂u​​+A​∂x​​∂u​​​dt​​dx​​ If we can find the solution of Adudt=CA\frac{du}{dt}= CA​dt​​du​​=C and dxdt=BA\frac{dx}{dt}=\frac{B}{A}​dt​​dx​​=​A​​B​​, the problem (*) can be solved. Therefore, the equations by dxdt=BA\frac{dx}{dt}=\frac{B}{A}​dt​​dx​​=​A​​B​​ plus dudt=CA\frac{du}{dt}=\frac{C}{A}​dt​​du​​=​A​​C​​ give the characteristic curve of the PDE. For a two-order PDE with two independent variables xxx and yyy: A∂2u∂x2+B∂2u∂x∂y+C∂2u∂y2+H=0A\frac{\partial^2 u}{\partial x^2} + B\frac{\partial^2 u}{\partial x \partial y} + C\frac{\partial^2 u}{\partial y^2} + H = 0 A​∂x​2​​​​∂​2​​u​​+B​∂x∂y​​∂​2​​u​​+C​∂y​2​​​​∂​2​​u​​+H=0 where HHH contains all the first derivative terms and constant terms. The total differential can be written as: du=∂u∂xdx+∂u∂ydydu = \frac{\partial u}{\partial x}dx + \frac{\partial u}{\partial y}dy du=​∂x​​∂u​​dx+​∂y​​∂u​​dy dudx=ux+uyy′\frac{du}{dx} = u_x + u_y y&#x27; ​dx​​du​​=u​x​​+u​y​​y​′​​ The characteristic equation is given as: Ay′2−By′+C=0A {y&#x27;}^2 - By&#x27; + C = 0 Ay​′​​​2​​−By​′​​+C=0 The solutions B±B2−4AC2A\frac{B\pm \sqrt{B^2 - 4AC}}{2A}​2A​​B±√​B​2​​−4AC​​​​​ are the characteristic curves (A≠0A\neq 0A≠0). If B2−4AC&gt;0B^2 - 4AC &gt; 0B​2​​−4AC&gt;0, two real characteristics exist, lead to a hyperbolic PDE. If B2−4AC=0B^2 - 4AC = 0B​2​​−4AC=0, only one real characteristic exists, lead to a parabolic PDE. If B2−4AC&lt;0B^2 - 4AC &lt; 0B​2​​−4AC&lt;0: two complex characteristics exist, lead to an elliptic PDE. By the way, using characteristics is a dimensionality reduction. A coordinate transformation does not change the type of PDE. It can be proved that B22−4A2C2=J2(B12−4A1C1)B_2^2-4A_2C_2 = J^2(B_1^2-4A_1C_1)B​2​2​​−4A​2​​C​2​​=J​2​​(B​1​2​​−4A​1​​C​1​​). JJJ is the Jacobian of the transform. For a general case, a two-order PDE with NNN independent variables: ∑i=1N∑j=1Naij∂2u∂xi∂xj+H=0\sum_{i=1}^N \sum_{j=1}^N a_{ij} \frac{\partial^2 u}{\partial x_i \partial x_j} + H = 0 ​i=1​∑​N​​​j=1​∑​N​​a​ij​​​∂x​i​​∂x​j​​​​∂​2​​u​​+H=0 Let λ\lambdaλ be the eigenvalues of the coefficient matrix with elements aija_{ij}a​ij​​. If all λ\lambdaλ are non-zero and all but one are the same sign, it leads to a hyperbolic PDE. If any of λ\lambdaλ is zero, it leads to a parabolic PDE. If all λ\lambdaλ are non-zero and of the same sign, it is an elliptic PDE. Hyperbolic PDE A simple example is the 1-D wave equation. ∂2u∂t2−c2∂2u∂x2=0\frac{\partial^2 u}{\partial t^2} - c^2\frac{\partial^2 u}{\partial x^2} = 0 ​∂t​2​​​​∂​2​​u​​−c​2​​​∂x​2​​​​∂​2​​u​​=0 where the characteristic directions are give by dx/dt=±cdx/dt=\pm cdx/dt=±c. ccc is the wave speed. The equation has the property that, if uuu and ∂u∂t\frac{\partial u}{\partial t}​∂t​​∂u​​ are arbitrarily specified initial data on the line t=0t = 0t=0 (with sufficient smoothness properties), then there exists a solution for all time ttt. The solutions of hyperbolic equations are “wave-like”. Disturbances of the initial or boundary conditions have a finite propagation speed. On the contrary, a perturbation of an elliptic or parabolic equation is felt at once by essentially all points in the field. No dissipative mechanism exists for hyperbolic equations. If the initial or boundary data contain discontinuities they will be transmitted into the interior along characteristics without attenuation. Parabolic PDE The unsteady Navier-Stokes equations are parabolic. A simple example is the 1-D heat conduction equation (diffusion equation). ∂u∂t−α∂2u∂x2=0\frac{\partial u}{\partial t} - \alpha \frac{\partial^2 u}{\partial x^2} = 0 ​∂t​​∂u​​−α​∂x​2​​​​∂​2​​u​​=0 where α\alphaα is the thermal diffusivity. The heat conduction signal is felt immediately throughout the system. The parabolic equation has a single characteristic direction defined by dx/dt=∞dx/dt=\inftydx/dt=∞(dt/dx=0dt/dx=0dt/dx=0). Following the characteristics would never advance the solution in time. The solutions of hyperbolic equations are dissipative in space while marching forward in time. It means a disturbance introduced at PPP can influence anywhere in the computational domain for t&gt;tit&gt;t_it&gt;t​i​​ with an attenuated magnitude. Even if the initial conditions include a discontinuity, the solution in the interior will always be continuous. Note that the parabolic PDEs in two or three dimensions are elliptic for the steady state (∂u∂t=0\frac{\partial u}{\partial t} = 0​∂t​​∂u​​=0). The 2-D or 3-D heat conduction equation is also paraolic. The 2-D or 3-D steady state heat conduction equation are elliptic. Elliptic PDE For fluid dynamics, as mentioned before, elliptic PDE are associated with steady-state problems. A simple example is the Laplace’s equation. ∂2ϕ∂x2+∂2ϕ∂y2=0\frac{\partial^2 \phi}{\partial x^2} + \frac{\partial^2 \phi}{\partial y^2} = 0 ​∂x​2​​​​∂​2​​ϕ​​+​∂y​2​​​​∂​2​​ϕ​​=0 For the second order elliptic PDEs, the maximum principle says, both the maximum and minimum values of ϕ\phiϕ must occur on the boundary, except the case when ϕ=\phi=ϕ=constant. Traditional solution methods]]></content>
      <categories>
        <category>曹衣出水</category>
      </categories>
      <tags>
        <tag>Marine Hydrodynamics</tag>
        <tag>CFD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Boundary layer theory]]></title>
    <url>%2Fblog%2F2018%2F12%2F04%2FBoundary-Layer%2F</url>
    <content type="text"><![CDATA[Introduction A boundary layer refers to the layer of fluid in the immediate vicinity of a bounding surface where the effects of viscosity are significant [1]. Proposed by Ludwig Prandtl, the flow past a body can be divided into two regions: a very thin layer close to the body (boundary layer) where the viscosity is important, and the remaining region outside this layer where the viscosity can be neglected [2]. Since the inviscid flow solution does not satisfy the no-slip condition at the wall, boundary-layer theory is called a singular perturbation method. The transition from laminar flow to turbulent flow was first examined in pipe flow by O. Reynolds (1883). Prandtl’s comprehensive contribution appeared in Aerodynamic Theory, edited by W.F. Durand, L. Prandtl (1935). A separation of the boundary layer from the body and the formation of large or small eddies at the back of the body can then occur. This kind of separation could be detrimental. Laminar and turbulent flow Laminar flow: At Reynolds numbers below the critical Reynolds number, layers of fluid move with different velocities without great exchange of fluid particles perpendicular to the flow direction. Turbulent flow: When the critical Reynolds number is exceeded, the turbulent flow occurs with large amounts of mixing perpendicular to the flow direction. Turbulence flow is characterised by a high irregular, random, fluctuating motion. Critical Reynolds Number: A Reynolds number at which the flow of a fluid changes from laminar to turbulent. For a pipe flow, Red,crit=2300Re_{d,crit}=2300Re​d,crit​​=2300. At Reynolds numbers between about 2000 and 4000 the flow is unstable as a result of the onset of turbulence. If the Reynolds number is greater than 3500, the flow is turbulent. For a flow over a flat plate, Rex,crit≃5×105Re_{x,crit}\simeq 5\times 10^5Re​x,crit​​≃5×10​5​​. It is dependent especially on the surface roughness. Boundary layer flow begins as a smooth laminar flow. As the flow continues, the laminar boundary layer increases in thickness. At some distance the smooth laminar flow breaks down and transitions to a turbulent flow. The distance between the lamniar boundary layer and the turbulent boundary layer is called the transition region. Law of the wall The law of the wall was first proposed by von Kármán [3]. The average velocity of a turbulent flow at a certain point is proportional to the logarithm of the distance from the wall boundary of the fluid region. For the non-dimensional analysis, wall shear stress: τw\tau_wτ​w​​ friction velocity: uτ=τwρu_\tau = \sqrt{\frac{\tau_w}{\rho}}u​τ​​=√​​ρ​​τ​w​​​​​​​ dimensionless velocity: u+=uuτu^+ = \frac{u}{u_\tau}u​+​​=​u​τ​​​​u​​ wall coordinate: y+=yuτμy^+ = \frac{y u_\tau}{\mu}y​+​​=​μ​​yu​τ​​​​ Viscous sublayer The viscous sublayer is a region near the wall in which the flow is laminar. The flow velocity decreases towards the no-slip boundary. The Reynolds number decreases until less than the Rex,critRe_{x,crit}Re​x,crit​​. In the viscous sublayer, i.e., y+&lt;5y^+ &lt; 5y​+​​&lt;5: u+=y+u^+ = y^+ u​+​​=y​+​​ where y+y^+y​+​​ is the dimensionless distance to the wall, and u+u^+u​+​​ is the dimensionless velocity parallel to the wall. Buffer layer (blending layer) 5&lt;y+&lt;305 &lt; y^+ &lt; 30 5&lt;y​+​​&lt;30 Log law region The logarithmic law of the wall is valid for flows at high Reynolds numbers — in an overlap region with approximately constant shear stress and far enough from the wall for (direct) viscous effects to be negligible, i.e., 500&gt;y+&gt;30500 &gt; y^+ &gt; 30500&gt;y​+​​&gt;30: u+=1κlny++C+u^+ = \frac{1}{\kappa} \ln y^+ + C^+ u​+​​=​κ​​1​​lny​+​​+C​+​​ where ln\lnln is the natural logarithm, κ\kappaκ is the Von Kármán constant, C+C^+C​+​​ is a constant. Estimation of boundary thickness Reference [1] Wikipedia - Boundary layer [2] Schlichting, H. and Gersten, K., 2017. Fundamentals of Boundary–Layer Theory. In Boundary-Layer Theory (pp. 29-49). Springer, Berlin, Heidelberg. [3] von Kármán, T., 1931. Mechanische Ähnlichkeit und Turbulenz, Nachr. Ges. Wiss Gottingen, Math. Phys. Klasse; 1930, 5. English translation, NACA TM, 611, pp.58-76. [4] Applied Computational Fluid Dynamics by André Bakker]]></content>
      <categories>
        <category>曹衣出水</category>
      </categories>
      <tags>
        <tag>Marine Hydrodynamics</tag>
        <tag>CFD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction to RANS equations]]></title>
    <url>%2Fblog%2F2018%2F11%2F08%2FRANS-model%2F</url>
    <content type="text"><![CDATA[What is RANS The Reynolds-averaged Navier–Stokes equations (RANS equations) for the three-dimensional incompressible viscous flow consists of the continuity equation and the momentum equations as follows: ∂ui∂xi=0\frac{\partial u_i}{\partial x_i} = 0 ​∂x​i​​​​∂u​i​​​​=0 ρ∂ui∂t+ρuj∂ui∂xj=−∂p∂xi+∂∂xj[μ(∂ui∂xj+∂uj∂xi)]+∂∂xj(−ρui′uj′‾)\rho \frac{\partial u_i}{\partial t} + \rho u_j \frac{\partial u_i}{\partial x_j} = -\frac{\partial p}{\partial x_i} + \frac{\partial}{\partial x_j} \left[ \mu \left(\frac{\partial u_i}{\partial x_j} + \frac{\partial u_j}{\partial x_i} \right) \right] + \frac{\partial}{\partial x_j} (-\rho \overline{u&#x27;_i u&#x27;_j}) ρ​∂t​​∂u​i​​​​+ρu​j​​​∂x​j​​​​∂u​i​​​​=−​∂x​i​​​​∂p​​+​∂x​j​​​​∂​​[μ(​∂x​j​​​​∂u​i​​​​+​∂x​i​​​​∂u​j​​​​)]+​∂x​j​​​​∂​​(−ρ​u​i​′​​u​j​′​​​​​) where uiu_iu​i​​, i=1,2 in two-dimensional flow, denotes the velocity components along x-,y-axis, respectively. ppp is the pressure. rhorhorho is the water density. μ\muμ is the dynamic viscosity of water. −ρui′uj′‾-\rho \overline{u&#x27;_i u&#x27;_j}−ρ​u​i​′​​u​j​′​​​​​ are the Reynolds stresses or the apparent turbulent shear stress. Turbulence Modeling The Reynolds stresses can be solved based on the Boussinesq hypothesis using the eddy viscosity turbulence models, or be solved from the transport equation based on Reynolds stress models. In the present studies, one-equation and two-equation eddy viscosity models as well as Reynolds stress models were employed to solve the RANS equations. In the eddy viscosity models, it is assumed that the Reynolds stresses are related to the mean velocity gradients, the turbulence kinetic energy and eddy viscosity, i.e., −ρui′uj′‾=μt(∂ui∂xj+∂uj∂xi)−23(ρk+μt∂ui∂xi)δij-\rho \overline{u&#x27;_i u&#x27;_j} = \mu_t \left( \frac{\partial u_i}{\partial x_j} + \frac{\partial u_j}{\partial x_i} \right) - \frac{2}{3}(\rho k + \mu_t \frac{\partial u_i}{\partial x_i}) \delta_{ij} −ρ​u​i​′​​u​j​′​​​​​=μ​t​​(​∂x​j​​​​∂u​i​​​​+​∂x​i​​​​∂u​j​​​​)−​3​​2​​(ρk+μ​t​​​∂x​i​​​​∂u​i​​​​)δ​ij​​ where μt\mu_tμ​t​​ represents the eddy viscosity, δij\delta_{ij}δ​ij​​ is the Kronecker delta, k=12ui′uj′‾k=\frac{1}{2}\overline{u&#x27;_i u&#x27;_j}k=​2​​1​​​u​i​′​​u​j​′​​​​​ is the turbulent kinetic energy that can be solved from the transport equations. The Reynolds stress tensor is linearly proportional to the mean strain rate. Note that the term ∂ui∂xi\frac{\partial u_i}{\partial x_i}​∂x​i​​​​∂u​i​​​​ equals to zero for incompressible flows. The simple one-equation model, Spalart-Allmaras (SA) model is implemented. The SA turbulence model solves a transport equation for the modified diffusivity ν~\tilde{\nu}​ν​~​​ to determine the turbulence eddy viscosity, μt\mu_tμ​t​​, i.e., μt=ρfν1ν~\mu_t=\rho f_{\nu 1} \tilde{\nu} μ​t​​=ρf​ν1​​​ν​~​​ where fν1f_{\nu 1}f​ν1​​ is a damping function. The transport equation for the modified diffusivity is: ∂∂t(ρν~)+∇⋅(ρν~v¯)=1σν~∇⋅[(μ+ρν~)∇ν~]+Pν~+Sν~\frac{\partial}{\partial t} (\rho \tilde{\nu}) + \nabla \cdot (\rho \tilde{\nu} \bar{v}) = \frac{1}{\sigma_{\tilde{\nu}}} \nabla \cdot [(\mu + \rho \tilde{\nu}) \nabla \tilde{\nu}] + P_{\tilde{\nu}} + S_{\tilde{\nu}} ​∂t​​∂​​(ρ​ν​~​​)+∇⋅(ρ​ν​~​​​v​¯​​)=​σ​​ν​~​​​​​​1​​∇⋅[(μ+ρ​ν​~​​)∇​ν​~​​]+P​​ν​~​​​​+S​​ν​~​​​​ where v¯\bar{v}​v​¯​​ is the mean velocity, σν~\sigma_{\tilde{\nu}}σ​​ν​~​​​​ is a model coefficient, μ\muμ is the dynamic viscosity, Pν~P_{\tilde{\nu}}P​​ν​~​​​​ is the production term and Sν~S_{\tilde{\nu}}S​​ν​~​​​​ is the source term. SA model has good convergence and robustness for specialized flows. However, the turbulence length and time scales are not well defined as they are in other two-equation models. Two-equation models are widely used solve the RANS equations, in which both the velocity and length scale are solved using separate transport equations. The turbulence length scale is estimated from the kinetic energy and its dissipation rate. The standard k−ϵk-\epsilonk−ϵ, standard k−ωk-\omegak−ω and the Shear Stress Transport (SST) k−ωk-\omegak−ω models are investigated. In the k−ϵk-\epsilonk−ϵ model, the turbulent eddy viscosity is calculated as: μt=ρCμfμkT\mu_t=\rho C_{\mu}f_{\mu} kT μ​t​​=ρC​μ​​f​μ​​kT where CμC_{\mu}C​μ​​ is a model coefficient, fμf_{\mu}f​μ​​ is a damping function and TTT is the turbulent time scale, which is calculated as: T=max(Te,Ctνϵ)T = \max(T_e,C_t\sqrt{\frac{\nu}{\epsilon}}) T=max(T​e​​,C​t​​√​​ϵ​​ν​​​​​) where Te=kϵT_e=\frac{k}{\epsilon}T​e​​=​ϵ​​k​​ is the large-eddy time scale, CtC_tC​t​​ is a model coefficient, ν\nuν is the kinematic viscosity. The transport equations for the turbulent kinetic energy kkk and the turbulence dissipation rate ϵ\epsilonϵ are written as: ∂∂t(ρk)+∇⋅(ρkv¯)=∇⋅[(μ+μtσk)∇k]+Pk−ρ(ϵ−ϵ0)+Sk\frac{\partial}{\partial t} (\rho k) + \nabla \cdot (\rho k \bar{v}) = \nabla \cdot \left[ (\mu + \frac{\mu_t}{\sigma_k}) \nabla k \right] + P_k -\rho(\epsilon-\epsilon_0) + S_k ​∂t​​∂​​(ρk)+∇⋅(ρk​v​¯​​)=∇⋅[(μ+​σ​k​​​​μ​t​​​​)∇k]+P​k​​−ρ(ϵ−ϵ​0​​)+S​k​​ ∂∂t(ρϵ)+∇⋅(ρϵv¯)=∇⋅[(μ+μtσϵ)∇ϵ]+1TeCϵ1Pϵ−Cϵ2f2ρ(ϵTe−ϵ0T0)+Sϵ\frac{\partial}{\partial t} (\rho \epsilon) + \nabla \cdot (\rho \epsilon \bar{v}) = \nabla \cdot \left[ (\mu + \frac{\mu_t}{\sigma_{\epsilon}}) \nabla \epsilon \right] + \frac{1}{T_e} C_{\epsilon 1} P_{\epsilon} - C_{\epsilon 2} f_2 \rho(\frac{\epsilon}{T_e}-\frac{\epsilon_0}{T_0}) + S_{\epsilon} ​∂t​​∂​​(ρϵ)+∇⋅(ρϵ​v​¯​​)=∇⋅[(μ+​σ​ϵ​​​​μ​t​​​​)∇ϵ]+​T​e​​​​1​​C​ϵ1​​P​ϵ​​−C​ϵ2​​f​2​​ρ(​T​e​​​​ϵ​​−​T​0​​​​ϵ​0​​​​)+S​ϵ​​ where σk\sigma_kσ​k​​, σϵ\sigma_{\epsilon}σ​ϵ​​, Cϵ1C_{\epsilon 1}C​ϵ1​​ and Cϵ2C_{\epsilon 2}C​ϵ2​​ are model coefficients, PkP_kP​k​​ and PϵP_{\epsilon}P​ϵ​​ are production terms, f2f_2f​2​​ is a damping function, SkS_kS​k​​ and SϵS_{\epsilon}S​ϵ​​ are source terms. In the realizable k−ϵk-\epsilonk−ϵ model, the equation for turbulence dissipation rate is modified and the coefficient CμC_{\mu}C​μ​​ is expressed as a function of mean flow and turbulence properties instead of constant in the standard model. The effect of the mean flow distortion on turbulent dissipation is introduced to improve the performance for rotation and streamline curvature. It also improves the boundary layer under strong adverse pressure gradients or separation. The renormalization group (RNG) k−ϵk-\epsilonk−ϵ model is based on renormalization group analysis of the Navier-Stokes equations. Different constants are used in the transportation equations for the turbulence kinetic energy and dissipation. The RNG k−ϵk-\epsilonk−ϵ model leads to lower turbulence levels and generated less viscous flows. In the k−ωk-\omegak−ω model, the turbulent eddy viscosity is related to the turbulence kinetic energy, kkk, and the specific turbulence dissipation rate, ω\omegaω, which is also referred to the mean frequency of the turbulence. The turbulent eddy viscosity is calculated as: μt=ρkT\mu_t=\rho kT μ​t​​=ρkT where T=α∗ωT=\frac{\alpha^*}{\omega}T=​ω​​α​∗​​​​ is the turbulence time scale in the standard k−ωk-\omegak−ω model. α∗\alpha^*α​∗​​ is a model coefficient. The transport equations for the turbulent kinetic energy kkk and the specific dissipation rate ω\omegaω are written as: ∂∂t(ρk)+∇⋅(ρkv¯)=∇⋅[(μ+σkμt)∇k]+Pk−ρβ∗fβ∗(ωk−ω0k0)+Sk\frac{\partial}{\partial t} (\rho k) + \nabla \cdot (\rho k \bar{v}) = \nabla \cdot \left[ (\mu + \sigma_k \mu_t) \nabla k \right] + P_k -\rho \beta^*f_{\beta^*}(\omega k - \omega_0 k_0) + S_k ​∂t​​∂​​(ρk)+∇⋅(ρk​v​¯​​)=∇⋅[(μ+σ​k​​μ​t​​)∇k]+P​k​​−ρβ​∗​​f​β​∗​​​​(ωk−ω​0​​k​0​​)+S​k​​ ∂∂t(ρω)+∇⋅(ρωv¯)=∇⋅[(μ+σωμt)∇ω]+Pω−ρβfβ(ω2−ω02)+Sk\frac{\partial}{\partial t} (\rho \omega) + \nabla \cdot (\rho \omega \bar{v}) = \nabla \cdot \left[ (\mu + \sigma_\omega \mu_t) \nabla \omega \right] + P_\omega -\rho \beta f_{\beta}(\omega^2 - \omega_0^2) + S_k ​∂t​​∂​​(ρω)+∇⋅(ρω​v​¯​​)=∇⋅[(μ+σ​ω​​μ​t​​)∇ω]+P​ω​​−ρβf​β​​(ω​2​​−ω​0​2​​)+S​k​​ where σk\sigma_kσ​k​​, σω\sigma_{\omega}σ​ω​​ are model coefficients, PkP_kP​k​​ and PωP_{\omega}P​ω​​ are production terms, fβ∗f_{\beta^*}f​β​∗​​​​ is the free-shear modification factor.is the vortex-stretching modification factor, k0k_0k​0​​ and ω0\omega_0ω​0​​ are the ambient turbulence values that counteract turbulence decay, SkS_kS​k​​ and SωS_{\omega}S​ω​​ are source terms. The k−ωk-\omegak−ω model predicts strong vortices and the near-wall interactions more accurately than the k−ϵk-\epsilonk−ϵ models. The limitations of the original k−ωk-\omegak−ω model include the over-prediction of shear stresses of adverse pressure gradient boundary layers, and the sensitivity to initial conditions and inlet boundary conditions. For the SST k−ωk-\omegak−ω model, the transport equations are the same as those of the standard k−ωk-\omegak−ω model by setting the damped cross-diffusion derivative term as zero in the near region. In the far field, the transport equations are the same as those of the standard k−ϵk-\epsilonk−ϵ model, which can avoid the problem that the model is too sensitive to the inlet turbulence properties. Detailed formulations can be found in the work by Menter (1993). The SST k−ωk-\omegak−ω model introduces the transport of the turbulence shear stress and improves the prediction of the onset and the flow separation under adverse pressure gradients. In the Reynolds stress models (RSM), the transport equations are solved for all the components of the Reynolds stress tensor and the turbulence dissipation rate, i.e., ∂∂t(ρui′uj′‾)+∂∂xk(ρukui′uj′‾)=Pij+Fij+DijT+ϕij−ϵij\frac{\partial}{\partial t} (\rho \overline{u&#x27;_i u&#x27;_j}) + \frac{\partial}{\partial x_k} (\rho u_k \overline{u&#x27;_i u&#x27;_j} ) = P_{ij} + F_{ij} + D_{ij}^T + \phi_{ij} - \epsilon_{ij} ​∂t​​∂​​(ρ​u​i​′​​u​j​′​​​​​)+​∂x​k​​​​∂​​(ρu​k​​​u​i​′​​u​j​′​​​​​)=P​ij​​+F​ij​​+D​ij​T​​+ϕ​ij​​−ϵ​ij​​ where PijP_{ij}P​ij​​ is the stress production, FijF_{ij}F​ij​​ is the rotation production, DijTD_{ij}^TD​ij​T​​ is the turbulent diffusion, ϕij\phi_{ij}ϕ​ij​​ is the pressure strain tensor and ϵij\epsilon_{ij}ϵ​ij​​ is the dissipation rate tensor. The isotropic turbulent dissipation rate is solved from a transport equation analogous to the k−ϵk-\epsilonk−ϵ model with various model coefficients. In order to resolve the viscous sublayer, two RSM models can be implemented, including the elliptic blending model (EB-RSM) and the linear pressure-strain two-layer model (LPS-RSM). EB-RSM model applies only one scalar elliptic equation instead of the original six transport equations for all stress components, which is based on the relaxation formulations of the pressure-strain tensor using a blending function. In the LPS-RSM model, the pressure-strain term ϕij\phi_{ij}ϕ​ij​​ comprises a slow term (also known as the return-to-isotropy term), a rapid term, and wall-reflection terms. The Reynolds stress models can predict complex flows with swirl rotation and high strain rates more accurately than eddy viscosity models. Solve the RANS equations The incompressible flow can be solved by the segregated solver for pressure-velocity coupling. The pressure-correction equation can be constructed from the continuity equation and the momentum equations. The SIMPLE (Semi-Implicit Method for Pressure Linked Equations) algorithm is used to solve the pressure and velocity for steady and unsteady problems. The PISO (Pressure-Implicit with Splitting of Operators) algorithm is applied for unsteady problems. The SIMPLE algorithm is summarized as follows. Set the boundary conditions. Compute the velocity and pressure gradients. Calculate the intermediate velocity v∗v^*v​∗​​ field by solving the discretized momentum equation. Compute the uncorrected mass fluxes at faces. Solve the pressure correction equation, which is constructed from the continuity equation. Update the pressure field with the pressure correction. Correct the mass fluxes at faces. Correct the cell velocities using the gradient of pressure corrections. Update density due to pressure changes. Advancing to next step iteration.]]></content>
      <categories>
        <category>曹衣出水</category>
      </categories>
      <tags>
        <tag>Marine Hydrodynamics</tag>
        <tag>CFD</tag>
        <tag>Turbulence Models</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Convert images to EPS format]]></title>
    <url>%2Fblog%2F2018%2F10%2F18%2FConvertEPS%2F</url>
    <content type="text"><![CDATA[Introduction .eps is a common used format of images in academic writting. If you want to convert hundreds of .jpg or .png. formats to .eps, the command convert in Linux system provides a good solution. ImageMagick The command convert is a tool from the tool ImageMagick, includes a number of command-line utilities for manipulating images. Please install it if convert is not found. Another utility from is identify. It can output the size and other information of the interested image, e.g., identify Behdinan1998.png The output is: Behdinan1998.png PNG 1103x792 1103x792+0+0 8-bit sRGB 96.2KB 0.000u 0:00.000 Convert The simplest way is, $ convert src.png dst.eps To change the image size, $ convert -resize 1920×1080 src.png dst.eps # Change the size according to the ratio $ convert -resize 50%x100%! src.png dst.eps # Change the size according to the width and keep the origin aspect ratio $ convert -resize 50%x100% src.png dst.eps To change the dpi of the image, $ convert -density 300 -resize 1920×1080 src.png dst.eps To rotate the figure (degree), $ convert -rotate 90 src.png dst.eps To add some text in the figure, $ convert -fill black -pointsize 60 -font Times -draw 'text 100,800 &quot;NAOE&quot; ‘ src.png dst.png However, the -destiny will lead to as large as hundreds of MB for one image. To solve this problem, we use a pdf file as a temporary file. $ convert -density 300 -resize 1920×1080 src.png dst.pdf &amp;&amp; pdftops -eps dst.pdf The output file is not large and clear enough. Error during converting When you first use conert src.png dst.pdf, maybe the error message occurs: convert: not authorized `pictures.pdf' @ error/constitute.c/WriteImage/1028 As a temporary fix, you can edit /etc/ImageMagick-6/policy.xml and change the PDF rights from none to read|write there. [Reference] The EPS file can not be converted to PNG using this tool. Please tell me if you know how. An example Here is a shell script for you. It can convert all the png files in the current directory to EPS files. The original images are not removed. for origin_image in `ls *.png` do file_name = $(basename $origin_image .png) convert -density 300 -resize 1920×1080 $file_name.png $file_name.pdf pdftops -eps $file_name.pdf done Enjoy it.]]></content>
      <categories>
        <category>吴带当风</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝尔法斯特的故事]]></title>
    <url>%2Fblog%2F2018%2F10%2F09%2FCL-HMS-Belfast%2F</url>
    <content type="text"><![CDATA[引言 英国是个老牌海军强国，曾拥有世界最强的海军舰队，先后击败西班牙、荷兰和法国，成为新的“日不落帝国”。日不落帝国，即The empire on which the sun never sets，指的是全天任何时候都有领土或海外殖民地处于白昼，出自西班牙国王卡洛斯一世：在我的领土上，太阳永不落下。 1660年5月，查理二世继位，英国恢复君主制，并在同年成立了英国皇家海军和英国皇家学会。在大名鼎鼎的英国皇家海军里面，随便挑几个，无畏号、胡德号、厌战号、胜利号、光辉号等等，都是闪闪发光的名字。本文要讲的故事的主角不算最有名的那一档，但也实打实是一艘名舰。如果你去过伦敦，看过白金汉宫和泰晤士河上的伦敦塔桥（Tower Bridge)，那一定记得，有个略微破坏浪漫气息的军舰静静地停靠在市政厅旁边，她就是贝尔法斯特号轻巡洋舰，现已是帝国战争博物馆（Imperial War Museum）。大英帝国什么奇珍异宝没见过，为什么要把一艘退役军舰摆到伦敦最显眼的位置呢？她够格吗？ 轻巡洋舰可不轻 贝尔法斯特号出生于1938年3月17日，比姐姐爱丁堡号还要早几天。身长187米，宽19.30米，吃水6.02米，航速32节。作为被寄予厚望的轻巡洋舰，她的体重达到了11550吨，比一般人家的重巡洋舰还大。区分轻巡洋舰还是重巡洋舰还真不看排水量，看的是主炮口径，大于203毫米的算重，反之就是轻巡洋舰。主要武器是四座三联装的152毫米口径 MKXXIII主炮和两座三联装533毫米口径的鱼雷发射管。 她的名字HMS Belfast，来自英国北爱尔兰的最大海港及爱尔兰岛上最大的城市Belfast市，旁边是Belfast湾。Belfast来自爱尔兰语Béal Feirste，意为Farset河的河口。Belfast市始建于1888年，自1922年起成为北爱尔兰的首府，北爱尔兰政治、文化中心和最大的工业城市和港口城市，也是世界最大的亚麻布产地。该市造船业发达，当年世界上最大的船：泰坦尼克号就是1909-1912年在这里的哈兰德·沃尔夫重工（Harland and Wolff）建造的。 1939年8月5日贝尔法斯特号正式完工。11月21日，年幼的贝尔法斯特在福特湾溜达的时候不幸触雷，不得不回炉大修，并且进行了大大小小的各种改造，这一改就改了三年，直到1942年的11月才重新服役，这一年正是二战打得最火热的时候。中国远征军为了保护最后的生命线滇缅公路伤亡惨重。太平洋上的中途岛海战刚刚结束，美国一举扭转了劣势，残酷血腥的瓜岛争夺战开始。大西洋上德军的潜艇狼群已经没落。北非那边第二次阿拉曼战役结束，蒙哥马利把沙漠之狐隆美尔狠狠揍了一顿。欧洲大陆那边斯大林格勒战役已经开战，令纳粹恐惧的冬天即将到来。 此时的贝尔法斯特可谓春风得意马蹄疾，在第10巡洋舰队分舰队开始了她辉煌的旗舰生涯。旗舰（Flagship）就是指挥舰，海军编队的指挥所，或是舰队司令所在的舰。 进击的旗舰 1939年触雷以后贝尔法斯特一直默默无闻，这期间发生了几件事情。1941年5月，格陵兰岛附近，德国的U110号潜艇被英国皇家海军俘获，最重要的战利品是潜艇电报柜的密码电报。英国发动几百名数学家破译了德军使用的恩尼格玛密码机Enigma。依靠信息上的不对称优势，从此英国海军占据了主动权。1942年4月姐姐爱丁堡号在从苏联回英国的航线上被德国潜艇击沉，船上有93个木箱，装了465块金条，当时的价值约150万英镑。 1942年11月3日，贝尔法斯特重新服役，并被分配到了第10巡洋舰队分舰队担当旗舰。 北角海战 The Battle of the North Cape 1943年12月25日，圣诞节，北角海战爆发。贝尔法斯特号通过雷达发现了德国战列舰沙恩霍斯特，皇家海军开始集结，展开追击。北角海战是海面上英德主力舰之间的最后一战。 这里插叙一下，纳粹的沙恩霍斯特是个奇葩。当年的德国为了对抗日益强大的法国海军（注意不是英国），急需发展海军主力战舰。然而德国是第一次世界大战的战败国，受凡尔赛和约的限制，新舰被限制在10,160吨以内，主炮口径必须小于280毫米。希特勒很急，声称不挑战英国的海洋霸主地位，只为抵抗法国的威胁。于是英国同意跟德国签了一个英德海军协定，打破了限制。沙恩霍斯特就是在这样的背景中出生的，不过已经来不及研制新的381毫米（15英寸）口径主炮，就用了比较成熟的283毫米（11英寸）口径主炮，满载排水量为38,700吨。总体来说，沙恩霍斯特号火力不如其他国家的主力战列舰，但是速度快，装甲厚，别人火力强的追不上，能追上的却打不动。 北角在挪威，距离北极2102.3公里。德国海军跑北冰洋干啥呢？这是因为苏联和纳粹正进行艰苦地拉锯战，英美必须援助苏联，而从北冰洋运输物资最方便。此时挪威仍被纳粹控制，德军潜艇和战舰盘踞在挪威，随时准备破坏盟军的运输航线。 1943年12月22日，圣诞快到了，北极航道加紧输送物资，其中编号为JW 55B护航队被德国飞机发现。希特勒的海军元帅邓尼茨向前线指挥作战的海军少将埃里希·贝下令：“敌人试图通过为俄国人提供粮食和武器的重要商船队为我们的东线圣战增加困难。为此，我们必须向我们的东线部队提供帮助。我们寄希望于沙恩霍斯特号上的无敌重炮群。我相信你们的进攻意志。”此时纳粹海军已经大势已去，邓尼茨手里的大型战舰只剩沙恩霍斯特了，此次出击风险极大。他又加了一道命令：“一旦英军主力出现，便立即撤出战斗。”在五艘驱逐舰的护航下，沙恩霍斯特奉命悄悄离开港口，奔向这口肥肉。 德国人做梦也没有想到，物资已经由JW 55A护航队运送到了北极圈内最大的城市，苏联的摩尔曼斯克。JW 55B护航队其实是英国派出的诱饵，就是用来钓沙恩霍斯特这条大鱼的，一切都按英国的剧本进行。皇家海军派出猎杀的力量有第一分队：旗舰-轻巡洋舰贝尔法斯特，轻巡洋舰谢菲尔德，重巡洋舰诺福克。以及第二分队：旗舰-战列舰约克公爵，轻巡洋舰牙买加，和几艘驱逐舰。 12月26日，北冰洋恶劣的海况让人作呕。皇家海军竭尽全力搜索沙恩霍斯特号，但迟迟没有消息。9点钟贝尔法斯特发现了这只怪兽，在距离大约11887米处开火，一发炮弹摧毁了沙恩的雷达控制系统，直接把她打成了瞎子。在北极的暴风雪中沙恩霍斯特只能靠判断对手火炮发出的火焰闪光来瞄准。然而英国军舰已经使用了无焰发射药，加之德军少将错误地判断对方是艘战列舰，沙恩霍斯特开始加速逃离。期间沙恩霍斯特的283毫米炮弹命中了重巡洋舰诺福克，摧毁了她的雷达和炮塔。诺福克和谢菲尔德逐渐跟不上沙恩霍斯特的航速，落在后面。贝尔法斯特靠着雷达的优势，正面对抗比她大三倍多的沙恩霍斯特。 下午友军赶到，英国战列舰约克公爵发现了沙恩霍斯特。贝尔法斯特发射照明弹使得沙恩霍斯特完全暴露，约克公爵上的10门356毫米（14英寸）火炮趁机齐射，摧毁了沙恩霍斯特的前主炮炮塔和飞机机库。沙恩霍斯特无法进行有效还击，只得开足马力，以31节的高速向东逃窜。沙恩霍斯特到目前为止还算幸运，期间，两发炮弹擦过约克公爵的桅杆，雷达电缆被破坏，但沙恩霍斯特并不知情。傍晚时分沙恩霍斯特的锅炉室被命中，航速急剧下降，掉到了10节，经损管抢修后恢复到了22节。无望中，沙恩霍斯特向纳粹司令部发了告别电报，写到：“我们将战斗到最后一发炮弹。”其后，驱逐舰编队发射的五条鱼雷击中挣扎的沙恩霍斯特，使之航速降到了10节。约克公爵迅速赶上，在照明弹的帮助下集中了全部的火力。沙尔霍斯特的最后一座炮塔被贝尔法斯特击破。晚间19:45，战列舰沙尔霍斯特终沉没在北角附近，1968名舰员阵亡1938名。英国本土舰队司令海军上将弗雷泽这样评价：先生们，与沙恩霍斯特号的战斗已经以我们的胜利而告终。我希望当你们之中的任何人指挥一艘战舰面对强大数倍的对手时，会像今天沙恩霍斯特号的指挥官一样勇敢地指挥你的船。 钨行动 Operation Tungsten 虽然皇家海军刚刚成功消除了沙恩霍斯特的威胁，但是纳粹海军的王牌战舰：提尔皮茨号战列舰，还龟缩在挪威北部的阿尔滕峡湾内。不拔掉这颗钉子，大西洋航线就永远不得安宁。麦金托什海军上校提出“钨”作战方案。 趁机介绍下提尔皮茨号（Tirpitz）吧。她是唯二的俾斯麦级超级战列舰的第二艘，1941年出生于威廉港的战争海军造船厂。同姐姐俾斯麦号一样，装备有四座双联装炮塔，搭载380毫米C/34型舰炮，满载排水量52600吨，不仅是德国海军最大的战舰，也是整个欧洲海军有史以来所最重的战列舰。姐姐以著名的铁血宰相Otto von Bismarck命名，妹妹则以Alfred von Tirpitz命名。这个人是第一位德国海军元帅，是帝国海军“公海舰队”的建设者，对海军的扩张有决定性影响。姐姐俾斯麦1941年丹麦海峡一役就沉了，顺手带走了皇家海军的骄傲：开头提到的胡德号。此后提尔皮茨号就成了唯一的依靠，轻易不敢使用，一直驻泊在挪威。这一举措也牵制了英国海军大量的兵力，民间称之为“北方孤狼”、“北方的孤独女王”、“北方的宅女”（误）。1944年11月，英国海军的钨行动开始，敲响了北宅的丧钟。 当年，1941年5月24日，为了追杀一艘俾斯麦加一艘欧根亲王，英国掏空了所有家底，动用了８艘战列舰和战列巡洋舰、２艘航空母舰、几十艘巡洋舰、驱逐舰，结果惨胜。对付比俾斯麦还重2000吨的提尔皮茨，1944年4月3日，英国再次集结了2艘战列舰、6艘航空母舰、4艘巡洋舰、15艘驱逐舰。英国的想法是，阿尔滕峡湾的水雷屏障不利于战列舰行动，而且即使进行炮战，也会像上次那样损失惨重，于是航母成为这次行动的主角，可见英国海军的战略思想已经由大舰巨炮转变为航母制胜了。攻击部队的主力是胜利号和暴怒号航空母舰，前者也是猎杀俾斯麦的重要力量，这次行动全体航母共搭载了42架梭鱼式轰炸机，挂载1,600英磅（720公斤）穿甲弹，负责撕开提尔皮茨厚重的甲板装甲带。贝尔法斯特担任航空母舰的护航任务。 战列舰的防空炮火根本抵挡不住轰炸机的空袭，还有大批战斗机进行洗甲板，残破的碎片、刺耳的爆炸声、混乱的浓烟到处都是。提尔皮茨遭受重创，大量进水，船员死伤惨重，虽然她没有沉没，但也彻底地失去了作战能力，难以修复。用兰斯海军少校的话来说：“将军，我们阉了那头野猪，可惜没能宰掉它！”整个轰炸行动英国只损失了两架战斗机，可谓大获全胜。 提尔皮茨的结局如何呢？钨行动之后，德国人拼命地维修，英国人也拼命地骚扰。1944年11月12日，英国开始了教义问答行动（Operation Catechism）。这次英国人使用了神器：高脚柜炸弹（Tallboy），也叫高脚杯，重达五吨，装药量高达5,200英磅（2,358公斤）。这种炸弹最初是用来贯穿混凝土层的，被称为地震波炸弹、大满贯炸弹，可以炸穿山头，在18米深的地下隧道爆炸，威力十分恐怖。英国空军兰开斯特重型轰炸机共向提尔皮茨投掷了29枚高脚柜炸弹，直接炸沉，这位昔日的北方的孤独女王底朝天倾覆在特罗姆瑟，结束了其憋屈的一生。提尔皮茨的舰首被炸出一个14.6米×9.7米的大洞，甚至连海底都有清晰的弹坑。 杂记 1944年6月6日，这个特殊的日子正是诺曼底登陆的第一天。诺曼底登陆是人类历史上规模最大的登陆作战，288万的盟军士兵横渡英吉利海峡后在法国诺曼底地区登陆，盟军海军由西部特混舰队、东部特混舰队和五个火力支援编队组成，总的军舰约5300艘，其中13艘战列舰，47艘巡洋舰，134艘驱逐舰，登陆舰艇4126艘，还有5000余艘运输船。东部特混舰队由美国海军军舰组成，在贝尔法斯特市集结。东部特混舰队由英国军舰组成，在格里诺克海港集结。D-DAY的清晨5点27分，旗舰贝尔法斯特展开对德国海岸设施的炮击，为进攻黄金海滩和朱诺海滩的盟军提供支援。贝爷的身边还有我们前文所介绍的二代拉菲号驱逐舰。火力支援持续到6月25日，盟军基本上取得了胜利，向着解放巴黎进发。贝尔法斯特的船员被派到滩头打扫战场。这是她最后一次开炮。 后来的生涯里她曾被德国的一艘XXI型潜艇发现，当时德国已经要战败，所以这艘潜艇没有出手将贝尔法斯特号击沉。1945年，欧洲战事渐渐结束，4月30日纳粹恶魔希特勒自杀。之后，贝尔法斯特被派往远东参加太平洋战役，日本投降后，贝尔法斯特主要负责维和、运输任务。直到1947年10月15日，功成名就的贝尔法斯特退役，成为少数能够存活到二战胜利的幸运舰之一。 1948年9月22日，贝爷重新服役。为了支援联合国军在朝鲜的行动，英国派出了包括1艘航母在内的21艘舰艇参加朝鲜战争。贝尔法斯特作为第5巡洋舰分舰队旗舰，提供火力援护。陪伴贝尔法斯特的还有伯明翰号和纽卡斯尔号。1956年到1959年间，贝尔法斯特进行了现代化改造，继续奋战在远东地区。1963年8月24日，贝尔法斯特再次退役。 终章 二战英国的老本都快的打光了，海军实在是太烧钱了。战后，丘吉尔代表的保守党下台，工党开始执政，开始对皇家海军进行大刀阔斧的裁撤。战列舰统统被拖到拆船厂变成废铁出售，仅留了一艘前卫号最为皇家邮轮（不可理喻）。航空母舰就更惨，在建造的被勒令停工，图纸上的被取消建造计划。曾经世界第一的海军只剩下几艘轻巡洋舰和驱逐舰。贝尔法斯特就是其中幸运的一艘。 神龟虽寿，犹有竟时。螣蛇乘雾，终为土灰。1971年初，英国政府准备废弃贝尔法斯特。其前任舰长吉尔斯·摩根组织成立了贝尔法斯特基金会（Belfast Trust），从英国政府手里争取到了贝尔法斯特的所有权。终于，贝尔法斯特来到了她最后的归属地，美丽安详的泰晤士河畔，于1971年10月开放参观。贝尔法斯特不仅是为数不多能够活到二战胜利的幸运舰，而且是逃脱了拆迁队的超级幸运舰。同学们考试前可以拜拜贝爷。]]></content>
      <categories>
        <category>故事</category>
      </categories>
      <tags>
        <tag>Navy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Writing shell scripts]]></title>
    <url>%2Fblog%2F2018%2F09%2F18%2Fshell-script%2F</url>
    <content type="text"><![CDATA[Introduction Shell scripting makes life more comfortable. The command sequences of Linux can be combined in one shell script and be executed automatically without wasting any time. For example, if you would like to rename 9,999,999 files and put them into different directories during one week, clicking the mouse will be devastating. By shell scripting, all you need is to run one command in a loop. After that, you can have a vacation and enjoy coffee in the next seven days. Suppose we have known at least a little bit about Linux. What is a Linux shell? Shell is an command language interpreter that executes commands. The most commonly used shells are SH (Bourne SHell) and Bash (Bourne again shell), released in 1989. Bash is not a perfect scripting language, but it is very useful. Life is short, get rid of the mouse. Shell Scripting Basics Here are some examples and templates of the shell scripting to take care of the files and directories. shell1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# reading the content of file1 and the output is written to file2 (instead of being displayed on screen)cat file1 &gt; file2# reading the content of file1 and the output is added to the end of file2cat file1 &gt;&gt; file2# concatenation and alphabetize (from a to z or from 1 to infinite) the lines of text after concatenationcat file1 file2 file3 | sort &gt; file4# deleting a directory excluding files (no "-r")rmdir directory# unpacktar -zxvf file.tar.gz# pack directories or files to a file archivertar -zcvf test.tar.gz directory file1 file2...# search a file name in a directoryfind ~/ -name "*.sh"# search for a certain text within files and show the line numbergrep "money*" filename -n# search for a special file without a character, e.g. we have a@0.sim, a@1.sim, a.simls ./*.sim | grep -v @# displaying all currently running processes (-f is equal to -A) / show details / certain process / by certain userps -Aps -auxps -ef | grep "firefox"ps -u root# conditional statement used to examine a status of a fileif [ ! -f "$myFile" ]; thentouch "$myFile"fi# -a file Returns true if file exists (or -e)# -d file Returns true if file exists and is a directory# -s file Returns true if file exists and has a greater size that zero# -r file Returns true if file exists and is readable# -w file Returns true if file exists and is writable# -x file Returns true if file exists and is executable# -N file Returns true if the file exists and has been modified since it was last read# change file name using sedfor file in `ls *.eps`do mv $file `echo $file | sed 's/old/new/g'`done File Handling Primer Note that “{” with a hashtag '#&quot; will be a bug for Markdown editor. In the following code, a space is added before the hash. Please delete the space when running the code. Example 1: delete, replace, grep… shell1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#!/bin/bash# This is a script to read the pressure information from a large amount of log files and write the results.#----------------------------------------#echo '--&gt; Start reading the minimum pressure and its location: (x,y)'# get the geometry name:var=./*.IGS# add .IGS means to ignore the file typeregion_name=$(basename $var .IGS)# '#*-' means delete the left side from the first left '-'# '##*-' means delete the left side from the last left '-'case_name_temp=$&#123;region_name#*-&#125;# // means replacing all, / means replacing the first element, the second / is a separatorcase_name=$&#123;case_name_temp//-/_&#125;"_steady@15000.sim"log_name="minimum_pressure_log_"$&#123;region_name&#125;".log"echo '--&gt; The geometry in this case is named as:' $region_nameecho '--&gt; The case name using the latest .sim file is:' $case_name# use " instead of ' to get the variable in shellsed -i "s/geometry/$region_name/g" Test_Minimum_Pressure_Location.java# note that ` is not '# search for the first line containing "Total:" in the log fileminimum_cp_temp=`grep "Total:" $log_name | grep -n "" | grep "^1"` # delete the left side from the last left space characterminimum_cp_origin=$&#123;minimum_cp_temp##* &#125;# transfer the data from .12345678 to 0.12345678minimum_cp=`echo $minimum_cp_origin | awk '&#123;printf("%.8f\n",$0)&#125;'` echo 'minimum_cp is:' $minimum_cp# search for the second line containing "Total:" in the log filex_minimum_cp_temp=`grep "Total:" $log_name | grep -n "" | grep "^2"` x_minimum_cp_origin=$&#123;x_minimum_cp_temp##* &#125;x_minimum_cp=`echo $x_minimum_cp_origin | awk '&#123;printf("%.9f\n",$0)&#125;'`echo 'x_mp is:' $x_minimum_cp# search for the third line containing "Total:" in the log filey_minimum_cp_temp=`grep "Total:" $log_name | grep -n "" | grep "^3"` y_minimum_cp_origin=$&#123;y_minimum_cp_temp##* &#125;y_minimum_cp=`echo $y_minimum_cp_origin | awk '&#123;printf("%.9f\n",$0)&#125;'`echo 'y_mp is:' $y_minimum_cp# note: &gt; means replace the old file, &gt;&gt; will add the old fileecho 'minimum_cp x_mp y_mp' &gt;&gt; result_minimum_pressure_location.datecho "$minimum_cp $x_minimum_cp $y_minimum_cp" &gt;&gt; result_minimum_pressure_location.dat Example 2: array, loop… shell123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#!/bin/bash# This is a job submitting script on the cluster of Skipper# The array can be used only in bash, not in sh#----------------------------------------## get the subdirectory and ignoring the filessub_directory=`ls -l | egrep '^d' | awk '&#123;print $(NF)&#125;'`echo 'The directories of the angle of attack are:' $sub_directory# get the avaliable node_info of skippernode_number=(26 27 28 22 23)core_number=(52 52 52 28 28)# index is from 0 to N-1 of all casesindex=0for allangle in $sub_directory do # arrange the node and cores of skipper # $[] means arithmetic computation: index%5 loop # $&#123;node_number[@]&#125; returns the element of array # $&#123; #node_number[@]&#125; returns the element number of array i=$[index%$&#123; #node_number[@]&#125;] done # example: cd ./P4.00/ cd $allangle node_info="nodes=compute-0-"$&#123;node_number[$i]&#125;":ppn="$&#123;core_number[$i]&#125; # the present directory directory_name=`pwd` # get the geometry name, case name, job name and log name csv_name=./*.csv geometry_name=$(basename $csv_name .csv) case_name="steady_"$&#123;geometry_name&#125;".sim" submit_job_name="job_"$&#123;geometry_name&#125;".sh" log_name="logfile_"$&#123;geometry_name&#125;".log" # configure the job script cp ../submit_job_template_skipper.sh ./$submit_job_name # the directory name has the symbol "/", so we use "#" as the separator in sed command sed -i "s#arg_case_directory#$directory_name#g" ./$submit_job_name sed -i "s/arg_node_info/$node_info/g" ./$submit_job_name sed -i "s/arg_case_name/$case_name/g" ./$submit_job_name sed -i "s/arg_log_name/$log_name/g" ./$submit_job_name # configure the StarCCM+ script cp ../autorun_naca66mod.java ./ sed -i "s/arg_geometry_name/$geometry_name/g" ./autorun_naca66mod.java sed -i "s#arg_case_directory#$directory_name#g" ./autorun_naca66mod.java echo "--&gt; Present case is: $case_name" echo "--&gt; Present directory is: $directory_name" echo "--&gt; Present computation is on the node: $node_info" # qsub $submit_job_name echo "--&gt; Present case is submitted!" cd ../ # update the index of the node array # many ways to calculate i++: let i+=1; ((i++)); i=`expr $i + 1`; i=$(( $i + 1 )); i=$[i+1] index=$[index+1]done Run a Shell Script Set the execute permission on your script: 12chmod u+x yourname.sh./yourname.sh Enjoy it. Advanced Shell Script Remove all files/directories except for one file 1234# remove files except 'file.txt'find . ! -name 'file.txt' -type f -exec rm -f &#123;&#125; +# remove directories except 'test'find . ! -name 'test' -type d -exec rm -rf &#123;&#125; + In zsh, you must enable extendedglob 1setopt extendedglob &amp;&amp; rm -- ^file.txt]]></content>
      <categories>
        <category>吴带当风</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elision in Engilish pronunciation]]></title>
    <url>%2Fblog%2F2018%2F09%2F01%2Felision-pronunciation%2F</url>
    <content type="text"><![CDATA[引言 来看一个简单的例子，fifth怎么发音的？(美音)不是/ˈfɪfθ/，而是/ˈfɪθ/。那么什么是elision呢？来看维基百科上的解释： In linguistics, an elision or deletion is the omission of one or more sounds (such as a vowel, a consonant, or a whole syllable) in a word or phrase.The word elision is frequently used in linguistic description of living languages, and deletion is often used in historical linguistics for a historical sound change. 其中vowel指的是元音，如/a/,/e/,/i/,/o/,/u/等。consonant指的是辅音，除了元音剩下的就是辅音。syllable意为音节（the word water is composed of two syllables: wa and ter）。 规则 辅音+元音的连读（Consonant + Vowel） 辅音+辅音的连读 元音+元音的连读 T, D, S 或 Z + Y的连读 容易读错的单词： Silent letters in English Silent B bomb: /bɑm/ aplomb: /ə’plɑm/ : great coolness and composure under strain climb: /klaɪm/ -&gt; climbing: /'klaɪmɪŋ/ comb: /kom/ : a flat device with narrow pointed teeth on one edge; disentangles or arranges hair crumb: /krʌm/: a very small quantity of something; break into crumbs debt: /dɛt/ -&gt; debit: /'dɛbɪt/ doubt: /daʊt/ -&gt; doubting: /'daʊtɪŋ/ dumb: /dʌm/ : unable to speak numb: /nʌm/ : lacking sensation; unresponsive; frightened or stunned with terror lamb: /læm/ : young sheep limb: /lɪm/ : arm; flipper; leg; branch plumb: /plʌm/ : adjust with a plumb line so as to make vertical subtle: /'sʌtl/ : elusive; faint and difficult to analyze succumb: /sə’kʌm/ : be fatally overwhelmed; consent reluctantly (unwillingly) thumb: /θʌm/ tomb: /tum/ womb: /wum/ : (==uterus) A woman’s womb is the part inside her body where a baby grows before it is born; contains the developing fetus Silent C abscess: /'æb’sɛs/ : a painful swelling containing pus ascend: /ə’send/ : slope upwards -&gt; ascent (noun.) : an upward slope or grade; a movement upward descend: /dɪ’sɛnd/ : move downward conscience: /kɑːnʃəns/ : not /kɑːn+'saɪəns/ : motivation deriving from ethical or moral principles conscious: /'kɑnʃəs/ : intentionally conceived; be conscious of something==you notice it or realize that it is happening crescent: /'krɛsnt/ : shape of a new moon discipline -&gt; disciple: /dɪ’saɪpl/ : the man who believes and helps to spread the doctrine of another fascinate: /'fæsə’net/ : cause to be interested or curious, enamored fluorescent: /flɔ’rɛsnt/ : emitting light -&gt; a fluorescent lamp incandescent: /'ɪnkən’dɛsnt/ : emitting light and being heated -&gt; an incandescent lamp luminescent: /lʊmə’nɛsnt/ : emitting light not caused by heat isosceles: /aɪ’sɑsə’liz/ : an isosceles triangle has two sides of equal length miscellaneous: /'mɪsə’lenɪəs/ : a haphazard assortment of many different kinds of things haphazard: /hæpˈhæzərd/ : dependent on by chance, great carelessness hazardous: /'hæzɚdəs/ : involving risk or danger, especially to people’s health or safety obscene: /əb’sin/ : something offends you because it relates to sex or violence which is unpleasant and shocking resuscitate: /rɪ’sʌsɪtet/ : return to consciousness; become active or successful again; recover (economic growth) scenario: /sə’nærɪo/ : an outline or synopsis of a play or event scene: /sin/ scent: /sɛnt/ : a distinctive odor that is pleasant; be smelly… Silent D handkerchief: /'hæŋkɚtʃɪf/ : a square piece of cloth used for wiping the eyes or nose or as a costume accessory sandwich: /'sænwɪtʃ/ — Warning: not sanwitch! — Wednesday: /wɛnzdɪ/ — Warning: not wendsday! — Silent G align: /ə’laɪn/ : place in a line; align yourself with a particular group (with the same political aim) benign: /bɪ’naɪn/ : not dangerous to health, pleasant and beneficial; kindness of disposition or manner campaign: /kæm’pen/ : a connected series of military operations; a race between candidates for elective office; the candidate kicked off his campaign with a speech… champagne: /ʃæm’pen/ cologne: /kə’ləʊn/ : 古龙。。。香水 a perfumed liquid composed of alcohol and fragrant oils consign: /kən’saɪn/ : commit irrevocably; to give over to another’s care; deliver into the hands or control of another feign: /fen/ : make believe with the intent to deceive, feigned sleep, feigned illness. gnail: /nɑːl/ : something twisted and tight and swollen into a state of deformity; make complaining noises gnash: /næʃ/ : grind together of teeth, mean they are angry or frustrated about something gnat: /næt/ : A gnat is a very small flying insect that bites people and usually lives near water. strain at a gnat (focusing on small things) while swallowing a camel (ignoring larger things) penny-wise and pound-foolish gnaw: /nɔ/ : bite or chew on with the teeth; become ground down or deteriorate gnome: /nəʊm/ : a legendary creature resembling a tiny old man; lives in the depths of the earth and guards buried treasure; a short pithy saying or maxim expressing a general truth or principle reign: /ren/ : a period during which something or somebody is dominant or powerful (monarch, e.g. during the reign of Henry VIII), have sovereign power, be larger in number, quantity, power, status or importance resign: /rɪ’zaɪn/ : quit, leave (a job, post, or position) voluntarily, give up or retire Silent H anchor archeology: /ɑrki’ɑlədʒi/ : the branch of anthropology that studies prehistoric people and their cultures architect: /'ɑrkɪtɛkt/ : an architect is a person who designs buildings archive: /'a:kaivz/ : collection of records especially about an institution (An institution is a large important organization such as a university, church, or bank.) chaos characteristic charisma: /kə’rɪzmə/ : a personal attractiveness or interestingness that enables you to influence others chrome echo leprechaun: /lɛprəkɔn/ : a mischievous elf in Irish folklore mischievous: naughtily or annoyingly playful; deliberately causing harm or damage deliberately: in an intentional manner loch: /lɑk/ : 洛克; a long narrow inlet of the sea in Scotland (especially when it is nearly landlocked) mechanical melancholy: /'mɛlənkɑli/ : a constitutional tendency to be gloomy and depressed, thoughtful sadness monarch: /'mɑnɚk/ : a nation’s ruler or head of state usually by hereditary right. The monarch of a country is the king, queen, emperor, or empress monochrome: /'mɑnəkrom/ : shows black, white, and shades of grey, but no other colours; painting done in a range of tones of a single color orchestra: /'ɔrkɪstrə/ : a musical organization consisting of a group of instrumentalists including string players Philharmonic: 爱乐乐团 perform symphonies orchid: /'ɔrkɪd/ : 'Girls need a laboratory, like an orchid needs a nursery, ’ says Ms. Beyer. psychic: /'saɪkɪk/ : human mind or something beyond the natural range of perception scheme school stomach; stomachache: /'stʌmək,ek/ technical; technique; technology Silent K or M or N knack: /næk/ : know-how; a particularly clever or skilful way of doing something successfully, especially something which most people find difficult. knapsack: /'næpsæk/ : a leather bag carried by a strap on your back or shoulder knave: /nev/ : dishonest and should not be trusted; a deceitful and unreliable scoundrel knead: /nid/ : press and squeeze it to make uniform kneel: /nil/ : rest one’s weight on one’s knees, bend your legs so that your knees are touching the ground. knell: /nɛl/ : bell ring as in announcing death knickers: /'nɪkɚz/ : trousers ending above the knee (short pants) knight knit: /nɪt/ : a fabric made by knitting knob: /nɑb/ : a round handle knoll: /nol/ : a small natural hill knot: /nɒt/ : a unit of speed equal to one nautical mile per hour; a piece of string, rope twisted and tight and swollen knuckle: /'nʌkl/ : a joint of a finger when the fist is closed. e.g. He would not knuckle down under their pressure. mnemonic: /nɪ’mɑnɪk/ : a rhyme or acronym used to aid recall; a word, short poem, or sentence that is intended to help you remember things such as scientific rules or spelling rules. autumn: /'ɔtəm/ column: /'kɑləm/ condemn: /kən’dɛm/ : (Vt) express strong disapproval of; declare or judge unfit; you say that it is very bad and unacceptable damn hymn: /hɪm/ : a song of praise (to God or to a saint or to a nation) solemn: /'sɑləm/ : very serious rather than cheerful; dignified, humorless and somber Silent P psychology: /saɪ’kɑlədʒi/ : the scientific study of the human mind and the reasons for people’s behaviour pneumonia: /nʊ’monɪə/ : a respiratory disease characterized by inflammation of the lung pseudo: /'sʊdo/ : a person who makes deceitful pretenses --&gt; pseudo code （伪代码） psychiatrist: /saɪ’kaɪətrɪst/ : a physician who specializes in psychiatry psychiatry: dealing with the diagnosis and treatment of mental disorders psychotic: /saɪ’kɑtɪk/ (silent p and silent h) : a type of severe mental illness; a person afflicted with psychosis receipt: /rɪ’sit/ : a piece of paper that you get from someone as proof that they have received money or goods from you. Silent T debut: /'deibju:/ : the act of beginning something new apostle: /ə’pɑsl/ : the apostles were the followers of Jesus Christ who went from place to place telling people about him and trying to persuade them to become Christians. bristle: /'brɪsl/ : a stiff hair; rise up as in fear bustle: /'bʌsl/ : a rapid active commotion; move or cause to move energetically or busily; in a hurried way castle: /'kæsl/ : a large and stately mansion; a large building formerly occupied by a ruler and fortified against attack fasten: /'fæsən/ : to fix firmly, to attach especially by pinning, tying, or nailing glisten: /'ɡlɪsn/ : give off a sparkling. If something glistens, it shines, usually because it is wet or oily. hustle: /'hʌsl/ : to crowd or push roughly, forcibly or hurriedly, to urge forward **precipitately ** precipitate: to throw violently jostle: /'dʒɑsl/ : to come in contact or into collision; to force by pushing moisten: /'mɔɪsn/ : make sth moist which means slightly or moderately wet mortgage: /'mɔrɡɪdʒ/ : a lien against property as for securing a loan, e.g. housing mortgage nestle: /'nɛsl/ : 雀巢 to settle snugly or comfortably; to lie in an inconspicuous or sheltered manner conspicuous: obvious to the eye or mind; attracting attention rustle: /'rʌsl/ : to act or move with energy or speed, a rustling sound: 沙沙声 soften: /'sɔfn/ thistle: /'θɪsl/ : a wild plant which has leaves with sharp points and purple flowers. 蓟 trestle: /'trɛsl/ : a braced frame of timbers, piles, or steelwork serving as a support wrestle: /'rɛsl/ : to engage in deep thought, consideration, or debate, as if in a violent or determined struggle. e.g. wrestle with a difficult problem. Origional meaning: a kind of sport Silent U baguette: /bæˈgɛt/ : a type of long, thin loaf of white bread which is traditionally made in France. biscuit: /'bɪskɪt/ : the small round bread leavened with baking-powder or soda circuit: /'sɝkɪt/ : an electrical device that provides a path for electrical current to flow disguise: /dɪs’ɡaɪz/ : the act of concealing the identity of something by modifying its appearance, misrepresenting the true nature of something guess guide guild: /ɡɪld/ : a formal association of people with similar interests; an organization of people who do the same job. the Writers’ Guild of America; the Lawyers’ Guild; the Commerce Guild guile: /ɡaɪl/ : being skilled in deception; crafty; fraud guillotine: /'ɡɪlətin/ : 1. kill by cutting the head off with a device called guillotine; 2.closure imposed on the debate of specific sections of a bill guilt; guilty guise: /ɡaɪz/ : an artful or simulated semblance, e.g., under the guise of friendship he betrayed them guitar rogue: /roɡ/ : deceitful and unreliable, a rogue is a man who behaves in a dishonest or criminal way, a rogue element is someone or something that behaves differently from others of its kind, often causing damage. silhouette: /,sɪlu’ɛt/ : the outline of an object. e.g. He drew the city in silhouette. Silent W sword: /sɔrd/ awry: /ə’raɪ/ : turned or twisted toward one side, not functioning properly/correctly. Go awry = fail write playwright: /'pleirait / : someone who writes plays wrack: /ræk/ : n, the destruction or collapse of something, ruin. vt/vi, smash or break forcefully. wrangle: /ræŋɡəl/ : to quarrel noisily, angrily or disruptively. wrath: /ræθ/ : n, intense anger on an epic scale. .e.g. wrath of god wreak: /rik/ : vt, to cause a great amount of disorder or damage, e.g., wreak havoc wreath: /riθ/ : flower arrangement consisting of a circular band of foliage or flowers for ornamental purposes, sometimes lying on the coffin. laurel wreath: 桂冠 wreck: /rɛk/ : == wrack. shipwreck is an accident that destroys a ship at sea. A wreck is something such as a ship, car, plane, or building that has been destroyed, usually in an accident. The remains/debris is called wreckage. wrench: /rɛntʃ/ : a hand tool that is used to hold or twist a nut or bolt wrest: /rɛst/ : obtain by seizing forcibly or violently wrestle: /'rɛsl/ : See above. wren: /rɛn/ : kind of a bird wretch: /rɛtʃ/ : poor soul, bad gay, adj: wretched wriggle: /'rɪɡl/ : move in a twisting or contorted motion, especially when struggling wring: /‘rɪŋ’/ : a twisting squeeze wrinkle: /'rɪŋkl/ : a slight depression/crumpled/creased in the smoothness of a surface; a clever method of doing something wrist: /rɪst/ : a joint between your hand and your arm ; elbow: 肘hinge joint between the forearm and upper arm writ: /rɪt/ : (law) a legal document issued by a court or judicial officier that orders a person to do a particular thing. writhe: /raɪð/ : to move in a twisting or contorted motion, especially when struggling. e.g. He was writhing in agony. wrought: /rɔt/ : make sth happen or change. original meaning: shaped to fit (锻造、加工） wrung： /rʌŋ/ : past tense of wring wry: /raɪ/ : distort; bent to one side. If someone has a wry expression, it shows that they find a bad situation or a change in a situation slightly amusing. 需要连读的短语或句子 Reference Wiki-Elision Definition and Examples of Elision in English Silent letter words]]></content>
      <categories>
        <category>吴带当风</category>
      </categories>
      <tags>
        <tag>English skills</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[东风夜放花千树]]></title>
    <url>%2Fblog%2F2018%2F07%2F17%2Fweapon-name-China%2F</url>
    <content type="text"><![CDATA[引言 有一个段子讲：一位日本网友了解到中国强大的“东风”系列导弹，误认为东风二字出自宋代词人辛弃疾的《青玉案·元夕》中的“东风夜放花千树，更吹落，星如雨。”并将其解读为描述分导式弹头再入的壮观场景，以为妙绝。说实话这解释还算浪漫。然而真相是当年科研人员在给中国首款弹道导弹命名时，想到毛主席引用过《红楼梦》中的一句话“不是东风压倒西风，就是西风压倒东风”，于是东风1号就诞生了。还有个段子讲的是，一个日本朋友说特别喜欢中国现代海军的名字，比如辽宁号，代表镇辽永宁。济南号，代表达济天南等等，然后说日本军舰的命名土的要死，用小土包、小溪流当名字。之前写过一篇介绍日本海军舰艇命名的文章，这篇文章再介绍一下中国的命名法。 舰船篇 之前提到日本海军命名看起来都很有意思，比如金刚号，赤城号，其实也就是些山名、地名或者天气现象等。所以别看中国海军取名叫辽宁号、合肥号，其实都是一样。 中国海军舰船命名基于海军舰艇是浮动的国土这一原则。1978年11月18日颁发的《海军舰艇命名条例》，规定了海军舰艇的命名标准。一级战舰，包括航空母舰、战列舰、巡洋舰、核动力潜艇，由总参谋部命名，其中巡洋舰以上由国务院特别命名。只有二级舰艇，包括驱逐舰、护卫舰、常规动力潜艇、大型登陆舰等，与二级以下的舰艇才能由海军命名。 北海舰队舰艇，用华北、东北、西北等十四省市的地名：辽宁、吉林、黑龙江、内蒙、青海、甘肃、宁夏、陕西、山西、北京、天津、河北、山东、河南。东海舰队舰艇，用华东、华中、西北等八省市的地名：上海、江苏、浙江、安徽、福建、江西、湖北、新疆。南海舰队舰艇，用华南及西南九省的地名：湖南、广东、广西、海南、四川、重庆、贵州、云南、西藏。航空母舰、巡洋舰按省级行政区或直辖市命名。驱逐舰按大城市命名，护卫舰按小城市命名。登陆舰按山川命名，补给舰按湖泊名命名，这一点上倒是跟日本的命名方式很像。 潜艇由于其特殊地位，单独作战任务较多，极少公布潜艇的型号。一般只用长征加代号的方式称呼。而北约为了方便，给中国潜艇按历史朝代取名，例如常规潜艇有明、宋、元等系列，第一代攻击型核潜艇和战略核潜艇分别为091型和092型，北约称汉级和夏级。第二代攻击型核潜艇和战略核潜艇分别为093型和094型，北约称商级和晋级。 著名的辽宁号航空母舰不属于三大舰队，而是由中国人民解放军海军领导机构直接指挥管理。辽宁号的名字来自其出生地：辽宁大连。 导弹篇 导弹（missile）是自身具有动力和制导系统的火箭+炸弹，能够控制其飞行弹道，追踪并摧毁目标的武器。全世界有能力独立研制洲际导弹的国家不超过五个，要问现在中国的导弹水平如何？Top3是没什么问题的，毕竟被誉为中国导弹之父的那个男人，曾任美国空军上校、麻省理工学院教授、加州理工学院教授，叫钱学森。 中国的导弹名字大部分来自毛泽东的诗词。 最经典的是反舰导弹：鹰击系列。反舰导弹也就是Anti-ship missile，用来攻击敌方舰船的导弹。放眼世界，法国最著名的一款叫飞鱼，美国自家用的叫鱼叉，俄国比较中二，如玄武岩、冥河、天王星。日本的没有名字，就叫80式、90式。我们的鹰击，来自毛泽东的《沁园春 长沙》。这首词写于1925年12月。当时年轻的毛离开故乡韶山，去往广州主持农民运动讲习所，途经长沙，重游橘子洲，作此词。 独立寒秋，湘江北去，橘子洲头。看万山红遍，层林尽染；漫江碧透，百舸争流。鹰击长空，鱼翔浅底，万类霜天竞自由。怅寥廓，问苍茫大地，谁主沉浮？携来百侣曾游，忆往昔峥嵘岁月稠。恰同学少年，风华正茂；书生意气，挥斥方遒。指点江山，激扬文字，粪土当年万户侯。曾记否，到中流击水，浪遏飞舟！ 防空导弹（Anti-aircraft missile）被命名为红旗。单独看起来有点土味，其实它来源于毛泽东的《清平乐 六盘山》，这首词写于1935年10月，长征即将胜利，红军主力即将在陕北会师，红旗猎猎飘扬在高山之巅，刚柔并济。海军版的防空导弹因此直接叫海红旗。 天高云淡，望断南飞雁。不到长城非好汉，屈指行程二万。六盘山上高峰，红旗漫卷西风。今日长缨在手，何时缚住苍龙？ 其中的“长缨”当年也被用作反潜导弹的名字。有趣的是，很多年以后的今天，隔壁日本有艘潜艇正巧叫做“苍龙”。 飞机上搭载的空对空导弹系列被命名为霹雳，代号PL。出处已不可考，正好辛弃疾写了一首《破阵子 为陈同甫赋壮词以寄之》: 醉里挑灯看剑，梦回吹角连营。八百里分麾下炙，五十弦翻塞外声。沙场秋点兵。马作的卢飞快，弓如霹雳弦惊。了却君王天下事，赢得生前身后名。可怜白发生! 至于我们的镇国利器、核弹快递：东风系列弹道导弹，开头已提到其出处。海军核潜艇上用的潜射弹道导弹取名叫巨浪，是在东风系列基础上开发而成。巨浪刚开始叫巨龙，可能觉得画风有点魔幻，后正式名称改叫巨浪，也算是很形象。 巡航导弹被命名为长剑，意义不言自明。长剑-10在2009年建国60周年阅兵式中正式亮相，使中国成为继美、苏之后世界上第三个实现自主研制、生产和装备对地巡航导弹的国家。该型导弹武器系统获2009年中国国家科学技术进步奖特等奖。长剑-10的射程约1500~2500公里，由WS-2400竖立式运输发射车搭载，打击精度高，可低空突防。长剑-20为空军轰炸机挂载型号。正好辛弃疾又写了一首《水调歌头 送杨民瞻》: 日月如磨蚁，万事且浮休。君看檐外江水，滚滚自东流。风雨瓢泉夜半，花草雪楼春到，老子已菟裘。岁晚问无恙，归计橘千头。梦连环，歌弹铗，赋登楼。黄鸡白酒，君去村社一番秋。长剑倚天谁问，夷甫诸人堪笑，西北有神州。此事君自了，千古一扁舟。 飞机篇 中航工业自己内部有给各个战斗机和直升机命名，但这个只是内部叫法，并不对外公布。官方命名并不多，冷战时代我们一般只给代号。歼击机（战斗机）以歼开头加代号，轰炸机以轰开头加代号，直升机以直开头加代号，运输机以运开头加代号等。现代战机逐渐开始有名字。战斗机一般选的是各种龙，比如歼10的名字叫猛龙，歼20是威龙，FC-1（JF-17）叫枭龙。中国最大型运输机运20叫鲲鹏，预警机叫空警200、空警2000。著名的战斗轰炸机：歼轰7又叫飞豹。海军的舰载战斗机：歼15叫飞鲨。无人机当中有著名的翼龙，已销往全国各地。神秘的大型超音速无人机被命名为暗箭。 不过北约给中国飞机的命名一向是充满歧视的味道。例如强5攻击机的北约代号是番摊（Fantan），是一种赌博的方法。歼8战斗机被称作长须鲸。长须鲸的游泳速度很快，但性格温顺，遭到了人类的大量捕杀。飞豹的北约名字是比目鱼，丑且不说，只会一辈子呆在海底扑腾。歼10以后稍微好点，歼10被称作火鸟（Firebird），歼20被称作火牙（Firefang）。 飞机的心脏，发动机一向是巅峰科技的体现。中国的航空发动机一般用山脉命名。比如，涡喷发动机WP-14是中国第一台完全自主研发的航空发动机，名字叫昆仑，用于J-7、J-8等系列飞机。涡扇发动机WS9的名字叫秦岭，用于飞豹。涡扇发动机WS-10的名字叫太行，用于J-10和J-11系列。最新的涡扇发动机WS-15，命名为峨嵋，将装备于第四代主力战机J-20上。 陆战篇 中国装备数量最多的坦克是96式坦克，总计2500+。全名是ZTZ-96主战坦克，其中的ZTZ代表：装甲-坦克-主战。还有火炮的命名也是类似，比如第三代122毫米口径的PLZ-07自行榴弹炮，其中的PLZ代表：火炮-榴弹-自行。自行火炮 跟坦克的主要区别就是火力支援，自身防护比较薄弱，机动性也不如坦克，通常是把火炮架在轮式车辆或战车上。坦克的要求就高的多了，不仅要保证火力强大，还要保证自身装甲防护过硬，机动性强，三者指标都要平衡。首要作战任务不是火力覆盖，而是干掉对方的坦克。不过随着时代的发展，他们俩的差别也在缩小，比如有轮式坦克这种不伦不类的家伙。但是，自行火炮始终属于炮兵，通常是要保护的对象，偶尔可以做做偷袭的工作，而坦克始终属于装甲兵团，那是要在第一线冲锋陷阵的。 根据《中国人民解放军军语》，坦克的正式定义是：由武器系统、防护系统、信息系统和越野机动平台组成，具有强大的直射火力、高度的越野机动性、良好的装甲防护力，主要用于遂行地面突击和两栖突击任务的装甲战斗车辆。乘员3～4人。分为主战坦克和特种坦克。中国的主战坦克第一代为59式，第二代为80式，第三代为96式和99式。1999年10月1日在国庆阅兵式上99式坦克首次公开露面，是世界上顶尖的主战坦克之一。最新改进型为ZTZ-99A。 这里顺便讲讲第一代主战坦克59式的故事。59式很有名，比如某“螳臂当车的歹徒”当年阻拦的就是59式坦克。由于中国陆军规模庞大，短时间内全换装高成本的96式、99式不现实，所以历年来对59式进行了大量的改进，又被成为“59魔改”。但无论怎么改，都是在五对负重轮的不归路上越走越远。其履带中的负重轮为五对共十个，经常成为大家调侃的对象：“远看炮塔吓死人，近看5对负重轮”。最初的59是仿照苏联的T-54A主战坦克研制的，1956年毫无基础的中国购买了T-54A全套图纸和生产线，1958年11月5日第1辆组装车在617厂完工下线，至今中国总共生产了1万余辆这种坦克。T-54系列诞生于1945年，直到今天中东国家还有大量T-54服役，被誉为坦克中的“AK47”，结实耐用。T-54系列坦克也是有史以来产量最大的坦克，其总数据估计高达86,000-100,000辆。59改的型号至少有16种以上，不仅有火力怪物59D式主战坦克，有利用59底盘改装而成的特种工程车辆，也有为了省钱直接在59底盘装上96的火炮系统，最近甚至还出了无人操控的版本。参见此文:坊间传言，日本空自曾在宫古海峡上空拍到了中国新型“59改”侦察机 所以再回头看看，取个好名字还是非常重要的事情。再读一遍开头提到的辛弃疾的名作《青玉案 元夕》吧： 东风夜放花千树，更吹落，星如雨。宝马雕车香满路，风箫声动，玉壶光转，一夜鱼龙舞。 蛾儿雪柳黄金缕，笑语盈盈暗香去。众里寻他千百度，蓦然回首，那人却在，灯火阑珊处。 王国维《人间词话》云：古今之成大事业、大学问者，必经过三种之境界：“昨夜西风凋碧树。独上高楼，望尽天涯路。”（晏殊《蝶恋花》）此第一境也。“衣带渐宽终不悔，为伊消得人憔悴。”（柳永《蝶恋花》）此第二境也。“众里寻他千百度，蓦然回首，那人却在灯火阑珊处。”（辛弃疾《青玉案》）此第三境也。]]></content>
      <categories>
        <category>木雁之间</category>
      </categories>
      <tags>
        <tag>Navy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fundamentals of Computational Fluid Dynamics in OpenFOAM]]></title>
    <url>%2Fblog%2F2018%2F06%2F30%2FOpenFOAM-CFD%2F</url>
    <content type="text"><![CDATA[Introduction OpenFOAM® is the leading free, open source software for computational fluid dynamics (CFD), and other computational science and engineering. Here is a short introduction of the fundamentals of Finite Volume Method (FVM) in OpenFOAM. Two books are recommended: The OpenFOAM Technology Primer The Finite Volume Method in Computational Fluid Dynamics - An Advanced Introduction with OpenFOAM® and Matlab® The scalar transport equation A mathematical model that describes a fluid flow is defined as a system of partial differential equations. In the model, the physical properties, e.g., pressure, velocity or temperature are dependent variables. The transport equation is used to describe the physical processes which change these properties in different way: ∂ϕ∂t+∇⋅(U⃗ϕ)+∇⋅(D∇ϕ)=Sϕ\frac{\partial \phi}{\partial t} + \nabla \cdot (\vec{U} \phi) + \nabla \cdot (D \nabla \phi) = S_{\phi} ​∂t​​∂ϕ​​+∇⋅(​U​⃗​​ϕ)+∇⋅(D∇ϕ)=S​ϕ​​ where ϕ\phiϕ is the scalar property, U⃗\vec{U}​U​⃗​​ is the velocity vector, DDD is the diffusion coefficient. Three terms in the left side represent temporal term, convective term and diffusive term, respectively. The purpose of any numerical method is to obtain an approximation of a solution of the mathematical model by solving a system of algebraic equations. Domain discretization The continuous space/fields in the mathematical model must be discretized into a finite number of volumes (cells). In the domain Ω\OmegaΩ, Each finite volume stores an averaged value of the physical property in its cell centre C. Two major types of meshes are distinguished, i.e., structured and unstructured meshes. Structured mesh has a regular connectivity and unstructured mesh has a irregular connectivity. The structured meshes support direct cell traversal. The cells can be labelled with the indices increasing in the directions of the coordinate axis. The unstructured meshes have no apparent direction in the way the cells are addressed. Their topology is un-ordered. The unstructured meshes allow us to discretize flow domains of very high geometrical complexity and do local refinement conveniently. The quality of a mesh can be evaluated by skewness, Jacobian, smoothness, etc. Skewness can be decided based on equilateral volume for triangles or based on the deviation from normalized equilateral angle for prisms and pyramids. Skewness=Optimalsize−CellsizeOptimalsizeSkewness = \frac{Optimal\; size - Cell\; size}{Optimal\; size} Skewness=​Optimalsize​​Optimalsize−Cellsize​​ Skewness=max{θmax−9090,90−θmin90}Skewness = \max\{\frac{\theta_{max}-90}{90},\frac{90-\theta_{min}}{90}\} Skewness=max{​90​​θ​max​​−90​​,​90​​90−θ​min​​​​} Jacobian, which is short for the Jacobian Matrix Determinate, is a measure of the normals of the element faces relative to each other. Jacobian ratio JJJ is the ratio of maximum determinant of Jacobian to minimum determinant of Jacobian. J=1.0J=1.0J=1.0 is desired. J&gt;0.6J&gt;0.6J&gt;0.6 typically indicate the mesh quality is good. Smoothness means there should not be sudden jumps in the size of the cell to avoid errors at nearby nodes. The change in size should be smooth. Laplacian smoothing is the most commonly used smoothing technique. Element addressing The mesh topology determines the way mesh elements are addressed by numerical methods. Indirect addressing The cells and faces of the mesh are defined as sets of indirected indexes to mesh points. The face is defined by the indices of its points, rather than by its points directly. For example, a hexahedral cell is addressed by faces 1 to faces 6 without storing any point related data directly. Otherwise dealing with the mesh information could lead to multiple copies of the same points and faces in memory. Owner-neighbour addressing Owner-neighbour addressing optimization defines the way that the indices in the mesh faces are ordered by using the direction of normal vector of the face. Two global lists are introduced: the face-owner and ther face-neighbour list. The owner cell of a face is the cell with a lower index in the list of mesh cells. The face area normal vector is directed from the owner into the neighbour cell. Boundary mesh addressing Boundary addressing optimization isolates the boundary faces and stores them at the end of the list of mesh faces. The boundary mesh is defined as a set of patches for different interpretation. All the faces of the boundary mesh are directed outwards from the flow domain. They have only a cell owner and no neighbour. Equation discretization Once the domain is dicretized, approximations are applied on the mathematical model which transfer the differential terms into dicrete differential operators. That a numerical method is consistent means, as the size of the cells is reduced, the discrete mathematical model must approach the exact mathematical model (Ferziger and Peric 2002). Time is also discretized into a sequential finite intervals. Let’s see the discretization of a simple advection equation for a scalar property ϕ\phiϕ and the vector U\mathbf{U}U. ∂ϕ∂t+∇⋅(Uϕ)=0\frac{\partial \phi}{\partial t} + \nabla \cdot (\mathbf{U} \phi) = 0 ​∂t​​∂ϕ​​+∇⋅(Uϕ)=0 First we need to integrate it in time and space: ∫tt+Δt∫Vp(∂ϕ∂t+∇⋅(Uϕ))dxdt=0\int_t^{t+\Delta t} \int_{V_p} (\frac{\partial \phi}{\partial t} + \nabla \cdot (\mathbf{U} \phi)) dx dt = 0 ∫​t​t+Δt​​∫​V​p​​​​(​∂t​​∂ϕ​​+∇⋅(Uϕ))dxdt=0 where VpV_pV​p​​ is the cell volume. The temporal term can be approximated as: ∫tt+Δt∫Vp∂ϕ∂tdxdt≈Vpϕn−ϕoΔt\int_t^{t+\Delta t} \int_{V_p} \frac{\partial \phi}{\partial t} dx dt \approx V_p \frac{\phi^n - \phi^o}{\Delta t} ∫​t​t+Δt​​∫​V​p​​​​​∂t​​∂ϕ​​dxdt≈V​p​​​Δt​​ϕ​n​​−ϕ​o​​​​ where nnn and ooo mark the new time step and the old time step respectively, and Δt\Delta tΔt denotes the time step. The advective term is integrated by applying the Gauss divergence theorem: ∫tt+Δt∫Vp∇⋅(Uϕ)dxdt≈∑ϕfUfS\int_t^{t+\Delta t} \int_{V_p} \nabla \cdot (\mathbf{U} \phi) dx dt \approx \sum \phi_f \mathbf{U}_f \mathbf{S} ∫​t​t+Δt​​∫​V​p​​​​∇⋅(Uϕ)dxdt≈∑ϕ​f​​U​f​​S where fff is a face of a cell, and S\mathbf{S}S is the outward-pointing face area normal vector with the magnitude of the face area. The variations of the face interpolated values in time are neglected. Finanly the discrete form is obtained: Vpϕn−ϕoΔt+∑ϕfUfS=0V_p \frac{\phi^n - \phi^o}{\Delta t} + \sum \phi_f \mathbf{U}_f \mathbf{S} = 0 V​p​​​Δt​​ϕ​n​​−ϕ​o​​​​+∑ϕ​f​​U​f​​S=0 If the explicit temporal discretization is used, the spatial terms will be evaluated in the old time step, as: Vpϕn−ϕoΔt+∑ϕfoUfoS=0V_p \frac{\phi^n - \phi^o}{\Delta t} + \sum \phi_f^o \mathbf{U}_f^o \mathbf{S} = 0 V​p​​​Δt​​ϕ​n​​−ϕ​o​​​​+∑ϕ​f​o​​U​f​o​​S=0 If the implicit temporal discretization is used, the spatial terms will be evaluated in the new time step, as: Vpϕn−ϕoΔt+∑ϕfnUfnS=0V_p \frac{\phi^n - \phi^o}{\Delta t} + \sum \phi_f^n \mathbf{U}_f^n \mathbf{S} = 0 V​p​​​Δt​​ϕ​n​​−ϕ​o​​​​+∑ϕ​f​n​​U​f​n​​S=0 It means the variables from the surrounding cells in the new time step are dependent. Afterwards, a system of algebraic equations is constructed and solved. All face interpolations are based on looping over mesh faces. Cell values are accessed using owner-neighbour addressing of the cells. The face values can be interpolated only once other than twice in the loop, saving the computational time. ∑ϕfUfS=∑fownerϕfUfS−∑fneighbourϕfUfS\sum \phi_f \mathbf{U}_f \mathbf{S} = \sum^{owner}_f \phi_f \mathbf{U}_f \mathbf{S} - \sum^{neighbour}_f \phi_f \mathbf{U}_f \mathbf{S} ∑ϕ​f​​U​f​​S=​f​∑​owner​​ϕ​f​​U​f​​S−​f​∑​neighbour​​ϕ​f​​U​f​​S Face interpolation The values ϕ\phiϕ stored in cell centres CCC are used to interpolate the values in the face centres CfC_fC​f​​. The face centered value ϕf\phi_fϕ​f​​ can be calculated by: ϕf=fxϕP+(1−fx)ϕN\phi_f = f_x \phi_P + (1-f_x)\phi_N ϕ​f​​=f​x​​ϕ​P​​+(1−f​x​​)ϕ​N​​ where fxf_xf​x​​ is a linear coefficient computed by: fx=∣∣fN⃗∣∣∣∣PN⃗∣∣f_x = \frac{||\vec{fN}||}{||\vec{PN}||} f​x​​=​∣∣​PN​⃗​​∣∣​​∣∣​fN​⃗​​∣∣​​ Let’s take the central differencing scheme (CDS) as an example. The cell face values for a uniform grid are computed by linear interpolation as: ϕe=0.5(ϕP+ϕE)\phi_e = 0.5 (\phi_P + \phi_E) ϕ​e​​=0.5(ϕ​P​​+ϕ​E​​) ϕw=0.5(ϕW+ϕP)\phi_w = 0.5 (\phi_W + \phi_P) ϕ​w​​=0.5(ϕ​W​​+ϕ​P​​) The Taylor series truncation error of the central differencing scheme is second order accurate if Pe&lt;2P_e&lt;2P​e​​&lt;2. PeP_eP​e​​ is the Péclet number, the ratio of the rate of advection to the rate of diffusion driven by an appropriate gradient. Boundary conditions There is only one cell next to a boundary face, thus this cell will always be the owner of the boundary face, and the normal area vector of a boundary face will always be directed out of the flow domain. For the Dirichlet boundary conditions, the values of the physical properties are specified and fixed. For the Neumann boundary condition, the values are computed from the internal cell values and the values of the derivative are specified, e.g., zero gradient boundary condition. ∇ϕ(xb)=0\nabla \phi(x_b) = 0 ∇ϕ(x​b​​)=0 According to the Taylor series approximation: ϕP=ϕb+∇ϕ(xb)δx+O(δ2x)≈ϕb+∇ϕ(xb)δx=ϕb\phi_P = \phi_b + \nabla \phi (x_b) \delta x + O(\delta^2 x) \approx \phi_b + \nabla \phi (x_b) \delta x = \phi_b ϕ​P​​=ϕ​b​​+∇ϕ(x​b​​)δx+O(δ​2​​x)≈ϕ​b​​+∇ϕ(x​b​​)δx=ϕ​b​​ In a word, applying the boundary condition is basically either defining a value or a gradient on a certain boundary face. Finally a system of algebraic equations can be generated.]]></content>
      <categories>
        <category>曹衣出水</category>
      </categories>
      <tags>
        <tag>CFD</tag>
        <tag>OpenFOAM</tag>
        <tag>FVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EVA配乐听听看]]></title>
    <url>%2Fblog%2F2018%2F06%2F17%2Fmelody-eva%2F</url>
    <content type="text"><![CDATA[引言 优秀的作品历久弥新。 从1995年10月4日到1996年3月27日，东京电视台首次播放了一部动画，共26话，在日本引起了轰动和争议。尽管神作层出不穷，至今它仍被众多爱好者认为是最伟大的动漫作品，常被视为日本动画历史上的里程碑。这部作品就是《新世纪福音战士》，日语《新世紀エヴァンゲリオン》，英语Neon Genesis Evangelion，简称EVA，讲述了由人操纵的Evangelion与神秘生物使徒作战的故事。如果认为它是俗套的机器人大战外星人的动画就错了。EVA更注重人物的内心世界，还有大量宗教、哲学、心理意象的交织让它变得复杂。它不是超级英雄拯救地球的动画，讲的都是普通人的故事。这里普通人指的是他们都有各种各样的缺点，每个人都能从中看到自己的影子。比如，男主碇真嗣虽然是司令的儿子，权二代，但是（看上去）却非常懦弱、脆弱。日常逃避现实，要么关键时刻认怂，要么躲在女主背后哭泣。女主之一明日香，德日混血，傲娇始祖，虽然是个14岁就大学毕业的天才，却命运坎坷，试管婴儿出生，年幼目睹了患精神病的母亲上吊自杀。自尊心太强，害怕孤独，只能通过事事争第一来寻找自己的存在感。女主之一绫波丽，是司令死去的老婆碇唯的基因制造的克隆人，典型的三无少女。日常靠吃药止痛，冷漠内向，没有喜怒哀乐，作为莉莉斯灵魂的容器，人偶一般的存在。所有人都有自己的秘密，而EVA的故事也是逐渐打开自己心扉的历程。资源及观看指南见文章末尾。 EVA打破了很多传统，也创造了很多神话。之所以称得上神作，一半原因来自其杰出的配乐。 下面就听听看吧。由于EVA充满了各种暴力血腥的战斗场景，所以各种燃爆的BGM必不可少。但在着重描写人物心理变化的情节中，也有很多非燃向的经典配乐，有大量的西方古典音乐元素，比如最经典的，巴赫的G弦上的咏叹调。以下是一些我喜欢的，除第一首以外，排名不分先后。 甘き死よ、來たれ（来吧，甜蜜的死亡） 这首歌的原名是德语，Komm, süsser Tod，日语名《甘き死よ、來たれ》，作词作曲编曲均是鷺巣詩郎，EVA御用音乐人。不过动画里的是英文版Come, Sweet Death（或者Come on Sweet Death）。出处是EVA旧剧场版The End Of Evangelion《Air/真心为你》的最后。随着音乐响起，出现真嗣突然掐住明日香脖子的经典画面。本该是世界末日的情节，却配了这一首略显轻快的歌曲，强烈反差一向是EVA的套路。莫名听起来像Hey Jude，有种开开心心看着全世界崩溃的感觉。废柴男主真嗣君当时的心情是，别人讨厌我，所以大家都去死吧。我也讨厌自己，所以我也去死吧。作为我心目中EVA系列第一神曲，在EVA的大结局中出现，让世界的终结和死亡是如此的彻底。英文歌词写的很凄美，悲伤与绝望简直要溢出字里行间。 I know I know I've let you down I've been a fool to myself I thought that I could live for no one else But now through all the hurt and pain It's time for me to respect The ones you love mean more than anything ... I wish that I could turn back time Cos now the guilt is all mine Can't live without the trust from those you love I know we can't forget the past You can't forget love and pride Because of that it's all killing me inside ... 链接戳我 残酷な天使のテーゼ（残酷天使的行动纲领） 这首歌是1995TV版的OP主题曲，日语名《残酷な天使のテーゼ》，英文版A Cruel Angel’s Thesis。由及川眠子作词，佐藤英敏作曲，大森俊之编曲，高桥样子演唱。这首作品旋律充满激情，节奏高燃，被众多EVA粉奉为无上经典。中国大陆于1999-2001年间由深圳电视台引进EVA，中文译名《新世纪天鹰战士》，原作剧本被大量删节情节、改动台词，主题曲变成了《勇敢的少年》，尽管改得乱七八糟，但它依然是童年美好的回忆，现在看来能通过审核简直是个奇迹。由于原唱声线很像鞠萍姐姐，所以常被疯狂恶搞，后被鞠萍姐姐本人亲自否认。2015年及川眠子发推特称，对EVA并没有太大兴趣，歌词的创（xia）作（xie）过程只用了2个小时。因为歌词比较中二，毕竟最初是给小朋友看的，这里就不贴了。日文版也有众多经典版本，除了高桥洋子，还有林原惠美、宫村优子、三石琴乃、伊藤静等著名声优翻唱，这里贴一首原版的《残酷な天使のテーゼ》和管弦乐版的The Heady Feeling of Freedom。 原版戳我 变奏版戳我 辣耳朵童年怀旧版慎点 Fly me to the moon（带我飞向月亮） 这首歌是1995TV版的ED片尾曲，不过它不是因为EVA才出名的。原作是1954年美国著名作曲家Bart Howard写的一首华尔兹舞曲In other words，后改为Fly me to the moon，被大量改编、翻唱。最著名的是1964年由Quincy Jones编曲，由Frank Sinatra演唱的版本。1969年阿波罗11号带着这首歌的唱片登陆月球，它是第一首在月球上播放的人类歌曲。1999年，这首歌获得美国歌曲创作名人堂授予的Towering Song Award。历经半个世纪依然风靡全球，出现在无数的影视作品里，例如，1987年的电影《华尔街》主题曲，2012年的电影《她们的名字叫红》片尾曲等。当然，最经典的还是EVA的片尾曲。音乐响起，画面中出现月亮和绫波丽的水中倒影，波光粼粼，女神那轻快而又神秘的倒影撩动着你我的心弦，分不清现实与虚幻。 月球在EVA中是很重要的元素。有两个月亮，一个叫白之月，是亚当的卵，使徒们的生命之源，2000年被人类在南极大陆发现。另一个叫黑之月，是莉莉丝的卵，人类的生命之源，四十亿年前黑之月撞击地球（第一次冲击），一部分形成了现在的月球，剩下的埋藏于箱根地下，后来人们在这之上建立了抵御使徒袭击的第三新东京市，并利用黑之月中的莉莉丝开展人类补完计划。在The End Of Evangelion中黑之月被绫波丽带出，是人类灵魂回归的中心。 一个冷知识：TV版每集结尾的Fly me to the moon都不一样，由不同的声优和歌手演唱。 链接戳我 翼をください（给我一双翅膀） 这首是从70年代就开始流行的民谣歌曲，山上路夫作词，井村邦彦作曲，1971年2月5日正式发行。声线清澈，旋律空灵，虽不是亢奋的节奏却依然触动人心，非常好听。1973年以后收录于日本小学教科书中，是学生们喜爱的合唱曲目。它还是1998年世界杯预选赛时，日本国家足球队的声援歌曲。同样也是被翻唱过无数多次，还有Babymetal的现场版、新垣结衣的翻唱版（误）。 「我变成怎么样无所谓，世界变成怎样也无所谓，但是绫波，至少她一个人，我一定要救出来！」 《给我一双翅膀》出现在EVA新剧场版《破》结局高潮部分，初号机暴走，引发第三次冲击，真嗣救出绫波丽，两人紧紧相拥，已经是超越人类而接近神的存在。本插曲由绫波丽的声优林原惠美演唱，可以说是完美。 链接戳我 今日の日はさようなら（道别在今日） 也译作《今天是再见的日子》。作词和作曲都是金子诏一。这首歌也是一首著名的童谣，常用作学生时代的毕业歌曲。简单的歌词里，充满了真挚的情感，忧伤的曲调中，满怀对未来的希望。这是一首简单的送别歌，平凡而又深刻。蜡笔小新第121话风间要走了，小新也唱过这首歌。 为什么这首歌谣能够成为EVA中的经典配乐之一呢？这是因为它出现在EVA新剧场版《破》中最暴力血腥的场景里。明日香驾驶的3号机在试验中被第十三使徒侵蚀，开始进攻NERV总部基地，碇真嗣奉命驾驶初号机将其摧毁。然而碇真嗣下不了手，违抗命令拒不抵抗，险被使徒化的3号机杀死。此时他老爹碇司令启用傀儡系统接管初号机，被接管后的初号机开始变得不受控制，将3号机疯狂地撕扯成碎片，并且啃食。3号机内的明日香生死不明，而此时碇真嗣还坐在初号机里亲眼目睹这一切。现场让人极度不适，鲜血和肠子糊成一团令人作呕。此时的背景音乐正是这首安静温柔的《道别在今日》，这种强烈的反差给人巨大的冲击。孩子们天真纯净的嗓音让人愈发地绝望。强大可怕的傀儡系统表明碇司令似乎不再需要碇真嗣来驾驶EVA了。没有人需要自己，自己心爱的人也无法保护，眼睁睁地看着她被“自己”撕碎，这个世界是如此的丑陋，而自己是如此孤独。“道别”可能象征着真嗣君整个精神的崩溃。TV版也有类似桥段，不过驾驶员是真嗣君的同学铃原东治，新剧场版中驾驶员改成了明日香，很多观众表示难以接受。 对明日的到来 我们满怀憧憬 走在希望的旅途上 就像那只小鸟 高高飞在天空 让我们自由的生活 在今日的时分 我们彼此道别 直到再会的那一天 这一幕是初号机吃完3号机之后，咬碎驾驶员所在的插入栓，意犹未尽地望向傍晚的天空，此时此刻出现一道彩虹，构图十分经典。庵野秀明最擅长的就是如此，用那些美好的意象、温柔的音乐，来反衬血淋淋的现实。 链接戳我 桜流し（樱流/樱花纷飞） 这首歌是EVA新剧场版《Q》的主题曲，出现在结尾。由日裔美籍歌手宇多田光作词作曲。于2012年11月17日对外公开发行。宇多田光是一个很有天赋的人，作词作曲编曲大多数由自己完成。自出道起，连续七张专辑销量破百万。日本最高销量的十张音乐专辑中，有三张为宇多田光的作品。其中1999年发行的第一张专辑First love，是至今为止日本乃至整个亚洲最高销量的专辑。她本人也很热爱动漫，家中收藏的漫画有新世纪福音战士、幽游白书、灌篮高手、攻壳机动队等数百本，也唱过上文提到的fly me to the moon。请宇多田光来为福音战士新剧场版制作主题曲，再合适不过。2010年宇多田光发布公告称：“明年开始，我将暂停‘身为歌手的活动’，而专心在‘身为人的活动’上。从15岁开始，过去12年来，我的生活完全以歌手活动为重心，然而，某一部分的我一直停留在15岁没有成长，但那是对于一个人很重要的部分。今后我要努力去接触，去学习我所缺乏的那一部分。可能2年，可能5年，但一定会回来的，请给我一些时间。” 直到2012年11月16日，《破》上映的前一天，宇多田光公布了暂退以来首次发表的新作《桜流し》，一天之内空降iTunes单曲榜销售冠军，下载次数是第二名的十倍。而这次发表新歌也是她应电影方要求而限定复出，之后仍处于暂退状态（傲娇）。 回到电影的结尾，刚刚进行了一场混乱的战斗，整个世界差点再次毁在真嗣君的手里，好基友渚熏自爆献身，真嗣吓得瑟瑟发抖。明日香救出躲在插入栓里的真嗣，拉着他慢慢走出镜头，绫波丽缓缓跟在后面。十四年过去了，物是人非，万籁俱寂。这首歌与EVA新剧场版《Q》十分搭配，完美地诠释了这部电影想表达的内容，那就是： Everybody finds love in the end 但愿。 链接戳我 魂のルフラン（魂之轮回） 这首歌是EVA旧剧场版《死与新生》的主题曲，发表于1997年2月。及川眠子作词，大森俊之作曲、編曲。整首歌的风格大气磅薄，歌词也很感人。 请 回到我身边 循着你的记忆 去往 温柔与梦的源泉 ... 梦是现实的延续，现实是梦的终结。 链接戳我 THANATOS-IF I CAN’T BE YOURS（塔纳托斯-如果我不能属于你） 这首歌是EVA旧剧场版The End Of Evangelion《Air/真心为你》的主题曲，发表于1997年8月，MASH作词，鷺巣詩郎作曲，在Oricon单曲排行榜上高居第2位。风格与上一首相反，婉转温柔，肝肠寸断。THANATOS是希腊神话中的死神，是个美少年，住在冥界，掌管死亡。EVA中的THANATOS是绫波丽的专属BGM，在TV版绫波丽对抗子宫天使的时候选择自爆而死的时候出现。自己选择同归于尽，生的希望留给碇真嗣。这首歌是在这首BGM的基础上重新填词而成。 Now it's time, I fear to tell I've been holding it back so long But something strange deep inside of me is happening I feel unlike I've ever felt And it's makin' me scared That I may not be what I (think I am) ... 链接戳我 次回予告（下集预告） 这首BGM的资料查不到，如你所见，是新剧场版下集预告采用的BGM，很魔性，听起来回味无穷。 链接戳我 Beautiful world（美丽的世界） 这首歌也是宇多田光的歌，为EVA新剧场版《序》写的主题曲。2007年8月29日发售，累计销售23万张，是2007年销量最高的动画歌曲。后来，EVA新剧场版《破》也采用了这首歌重新编曲作为主题曲，即Beautiful World -PLANiTb Acoustica Mix。 链接戳我 终章 EVA全系列的链接:EVA 新世纪福音战士，评分9.7。顺便给一个观看顺序指南：相比原版，新剧场版画面和音质都有了很大提高，故事虽然有较大改动但也算比较完整，建议从《序》、《破》、《Q》三部曲开始看。然后，真爱党请来看TV版1-24话，再接着看旧剧场版The End Of Evangelion，包括两集：《Air》与《真心为你》，分别对应TV版的25和26话。忽略原25-26话，因为当时经费紧张，最后两话只能用大量意识流及闪瞎眼的画面来凑数，不容易看懂。太长不看党请直接看就旧剧场版《死与新生》，相当于原TV版的一个精编，也加入了一些TV版中没有的新剧情，真爱党同样可以看看。全看完的朋友，我们一起祈祷有生之年可以看到第四部新剧场版《:│▌》，用个休止符当剧名是什么鬼！ 这些看完之后，一定还处于似懂非懂的懵比状态，所以。。。 想了解EVA剧情背景的看这里：EVA全解析：动漫界至高神作到底在讲什么 转自奇法大陆 唱片合集：新世纪福音战士的音乐世界]]></content>
      <categories>
        <category>游鱼出听</category>
      </categories>
      <tags>
        <tag>Melody</tag>
        <tag>EVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（5）支持向量机]]></title>
    <url>%2Fblog%2F2018%2F06%2F09%2Fmachine-learning-5%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. In this note, let’s discuss about the support vector machine (SVM). Reference is added: the course of CMU (http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/ Linear classifier A classification decision is made by the linear classifier based on the value of a linear combination of the characteristics (features), which is typically presented to the machine by a feature vector. Input: x⃗\vec{x}​x​⃗​​ Output: y=f(w⃗⋅x⃗)=f(∑jwjxj)y = f(\vec{w} \cdot \vec{x}) = f(\sum_j w_j x_j)y=f(​w​⃗​​⋅​x​⃗​​)=f(∑​j​​w​j​​x​j​​) fff is such a function, that maps the values above a threshold to the first class and other values to the second class, or give the probability. w⃗\vec{w}​w​⃗​​ is a vector of weights, which can be learned from a set of labeled samples. There are two classes of methods for determining the parameters of w⃗\vec{w}​w​⃗​​: Generative model: a model of the joint probability distribution on P(X,Y)P(X,Y)P(X,Y) Linear discriminant analysis (LDA) Naive Bayes classifier Hidden Markov model discriminative model: a model of the conditional probability of YYY with an observation xxx as P(Y∣X=x)P(Y \mid X=x)P(Y∣X=x) Logistic regression: maximum likelihood estimation of w⃗\vec{w}​w​⃗​​ Perceptron: an algorithm that attempts to fix all errors encountered in the training set SVM: maximizes the margin between the decision hyperplane and the examples in the training set. Neural networks Generative algorithms try to learn P(X,Y)P(X,Y)P(X,Y) which can be transformed into P(Y∣X=x)P(Y \mid X=x)P(Y∣X=x) later to classify the data. Discriminative algorithms try to learn P(Y∣X)P(Y \mid X)P(Y∣X) directly from the data and then classify data, does not care how the data was generated (P(X)P(X)P(X)). Here is the ralation: P(Y∣X)=P(X,Y)/P(X)=P(X∣Y)P(Y)/P(X)P(Y \mid X) = P(X,Y) / P(X) = P(X \mid Y) P(Y) / P(X) P(Y∣X)=P(X,Y)/P(X)=P(X∣Y)P(Y)/P(X) Generative algorithms are suitable for unsupervised learning. Discriminative models usually proceeds in a supervised way. Maximum margin classifier In the case of support vector machines, a data point is viewed as a ppp-dimensional vector (a list of ppp numbers). The points was supposed to separated by a p−1p-1p−1 dimensional hyperplane. Many hyperplanes exists. The best choice is the one that represents the largest margin between the two classes, which is known as the maximum-margin hyperplane. This classifier is also called as a maximum margin classifier. The equation of a hyperplane can be written as: ωTx−b=0\omega^T x - b = 0 ω​T​​x−b=0 Note that ωTx=ω⃗⋅x⃗\omega^T x = \vec{\omega}\cdot \vec{x}ω​T​​x=​ω​⃗​​⋅​x​⃗​​. Two classes can be determined by: f(x)=ωTx−b&gt;0f(x) = \omega^T x - b &gt; 0f(x)=ω​T​​x−b&gt;0 leads to y=1y = 1y=1 f(x)=ωTx−b&lt;0f(x) = \omega^T x - b &lt; 0f(x)=ω​T​​x−b&lt;0 leads to y=−1y = -1y=−1 The problem is how to find the optimum hyperplane with a maximum margin. Functional margin The functional margin is defined as: γ^=yf(x)=y⋅(ωTx−b)\hat{\gamma} = yf(x) = y \cdot (\omega^T x - b) ​γ​^​​=yf(x)=y⋅(ω​T​​x−b) The result would be positive for properly classified points and negative otherwise. Hence, a large functional margin represents a confident and a correct prediction. The functional margin has a problem. For example, the two equations ωTx−b=0\omega^T x - b =0ω​T​​x−b=0 and 2ωTx−2b=02\omega^T x - 2b =02ω​T​​x−2b=0 represent the same hyper plane, but the functional margins differs. Geometrical margin The geometrical margin is defined as: γ~=yγ=y⋅ωTx−b∣∣ω∣∣=γ^∣∣ω∣∣\tilde{\gamma} = y\gamma = y \cdot \frac{\omega^T x - b}{|| \omega ||} = \frac{\hat{\gamma}}{|| \omega ||} ​γ​~​​=yγ=y⋅​∣∣ω∣∣​​ω​T​​x−b​​=​∣∣ω∣∣​​​γ​^​​​​ where ω\omegaω is the normal vector of the hyper plane. Here ∣∣ω∣∣|| \omega ||∣∣ω∣∣ is the Euclidean norm (2-norm), gives the ordinary distance from the origin to the point ω\omegaω. The margin in maximum margin classifier refers to the geometrical margin. Samples on the margin are called the support vectors. If the training data is linearly separable, two parallel hyperplanes can be selected to separate the two classes of data so that the distance between them (called margin) is as large as possible. After normalized or standardized, these hyperplanes are: label 1: ωTx−b=1\omega^T x - b = 1ω​T​​x−b=1 label -1: ωTx−b=−1\omega^T x - b = -1ω​T​​x−b=−1 The geometrical margin between these two hyperplanes is 2/∣∣ω∣∣2/|| \omega ||2/∣∣ω∣∣. The optimization problem is to minimize 12∣∣ω∣∣2\frac{1}{2}|| \omega ||^2​2​​1​​∣∣ω∣∣​2​​ s.t. (subject to) yi(ω⃗⋅xi⃗−b)≥1y_i(\vec{\omega}\cdot \vec{x_i} - b)\geq 1y​i​​(​ω​⃗​​⋅​x​i​​​⃗​​−b)≥1 for each iii in data. This optimization is called the constrained optimization with the ‘s.t.’. It can be solved efficiently by quadratic programming (QPQPQP). The max-margin hyperplane is completely determined by those xi⃗\vec{x_i}​x​i​​​⃗​​ which lie nearest to the hyperplane. These xi⃗\vec{x_i}​x​i​​​⃗​​ are called support vectors. Moving other points a little does not affect the decision boundary. To predict the labels of new points, only the support vectors are necessary. We use 12∣∣ω∣∣2\frac{1}{2}|| \omega ||^2​2​​1​​∣∣ω∣∣​2​​ for matematical purpose when derivating the Lagrangian function to optimize it. Soft-margin for not linearly separable data It could be very complicated or over-fitting to classify all samples correctly in practical. For the soft-margin SVM, errors in classification are allowed, which is more common. The criteria now is to minimize the number and the value of noises. E.g. A large margin causes lots of misclassified samples. A smaller margin can reduce the number of errors, but each error becomes bigger. The optimization problem is to minimize 12∣∣ω∣∣2+C∑iξi\frac{1}{2}|| \omega ||^2 + C \sum\limits_{i} \xi_i​2​​1​​∣∣ω∣∣​2​​+C​i​∑​​ξ​i​​ s.t. yi(ω⃗⋅xi⃗−b)≥1−ξiy_i(\vec{\omega}\cdot \vec{x_i} - b)\geq 1-\xi_iy​i​​(​ω​⃗​​⋅​x​i​​​⃗​​−b)≥1−ξ​i​​ and ξi≥0\xi_i \geq 0ξ​i​​≥0 for each iii in data. ξ\xiξ is pronounced as /ksi/, called “slack” variables, pay linear penalty. ξi=0\xi_i = 0ξ​i​​=0 means the classification of sample iii is correct. The larger xix_ix​i​​ is, the bigger mistake the classification makes. 0&lt;ξi≤10 &lt; \xi_i \leq 10&lt;ξ​i​​≤1 means sample iii is in the margin, resulting to margin violation. ξi&gt;1\xi_i &gt; 1ξ​i​​&gt;1 stands for sample iii is misclassified. CCC is the tradeoff parameter, chosen by cross-validation. A large CCC means we will get a small margin width and reduce the number of errors as much as possible. Contrarily, a small CCC means the margin width can be large even though we get more errors. The product CξiC \xi_iCξ​i​​ is the penalty for misclassifying. If C→∞C \to \inftyC→∞, the soft-margin is recovered to the hard-margin SVM. ξi\xi_iξ​i​​ is defined as: ξi=loss(sgn(ω⃗⋅xi⃗−b),yi)=1−yi(ω⃗⋅xi⃗−b)\xi_i = loss(sgn(\vec{\omega}\cdot \vec{x_i} - b),y_i) = 1-y_i(\vec{\omega}\cdot \vec{x_i} - b) ξ​i​​=loss(sgn(​ω​⃗​​⋅​x​i​​​⃗​​−b),y​i​​)=1−y​i​​(​ω​⃗​​⋅​x​i​​​⃗​​−b) With the regularization and introducing the Hinge loss: max(0,1−yi(ω⃗⋅xi⃗−b))max(0, 1-y_i(\vec{\omega}\cdot \vec{x_i} - b)) max(0,1−y​i​​(​ω​⃗​​⋅​x​i​​​⃗​​−b)) the optimization problem is to minimize: 12∣∣ω∣∣2+λ∑i=1mmax(0,1−yi(ω⃗⋅xi⃗−b))\frac{1}{2}|| \omega ||^2 + \lambda \sum_{i=1}^m max(0, 1-y_i(\vec{\omega}\cdot \vec{x_i} - b)) ​2​​1​​∣∣ω∣∣​2​​+λ​i=1​∑​m​​max(0,1−y​i​​(​ω​⃗​​⋅​x​i​​​⃗​​−b)) It belongs to the quadratic programming (QPQPQP) with a convex function f(x)f(x)f(x). A convex function is the line segment between any two points on the graph lies above or on it, e.g., f(x)=x2f(x)=x^2f(x)=x​2​​. For a twice differentiable function of a single variable, if the second derivative is always greater than or equal to zero for its entire domain then the function is convex. The locally optimal point of a convex problem is globally optimal. Lagrangian Duality The standard form convex optimization problem can be expressed as: minimize f(x)=12∣∣ω∣∣2f(x)=\frac{1}{2}|| \omega ||^2f(x)=​2​​1​​∣∣ω∣∣​2​​, subject to gi(x)≤0,i=1,...,kg_i(x) \leq 0, i=1,...,kg​i​​(x)≤0,i=1,...,k and hi(ω)=0,i=1,...,lh_i(\omega) = 0, i=1,...,lh​i​​(ω)=0,i=1,...,l. The generalized Lagrange function is introduced as: L(ω,α,β)=f(ω)+∑i=1kαigi(ω)+∑i=1lβihi(ω)\mathscr{L}(\omega, \alpha, \beta) = f(\omega) + \sum_{i=1}^k \alpha_i g_i(\omega) + \sum_{i=1}^l \beta_i h_i(\omega) L(ω,α,β)=f(ω)+​i=1​∑​k​​α​i​​g​i​​(ω)+​i=1​∑​l​​β​i​​h​i​​(ω) where αi\alpha_iα​i​​ and βi\beta_iβ​i​​ are the Lagrangian multipliers. αi&gt;0\alpha_i&gt;0α​i​​&gt;0 must be satisfied. There is a lemma: if ω\omegaω satisfies the primal constraint sss, maxα,β;αi≥0L(ω,α,β)=f(ω)max_{\alpha,\beta; \alpha_i \geq 0} \mathscr{L}(\omega, \alpha, \beta) = f(\omega) max​α,β;α​i​​≥0​​L(ω,α,β)=f(ω) Otherwise, the maximum of L\mathscr{L}L can be +∞+\infty+∞, which is related to the values of αi,βi\alpha_i,\beta_iα​i​​,β​i​​. The primal optimization problem minωf(x)min_\omega f(x)min​ω​​f(x) is equivalent to: p∗=minω maxα,β;αi≥0L(ω,α,β)p* = min_\omega \ max_{\alpha,\beta; \alpha_i \geq 0} \mathscr{L}(\omega, \alpha, \beta) p∗=min​ω​​ max​α,β;α​i​​≥0​​L(ω,α,β) The dual problem is: d∗=maxα,β;αi≥0 minωL(ω,α,β)d^* = max_{\alpha,\beta; \alpha_i \geq 0} \ min_\omega \mathscr{L}(\omega, \alpha, \beta) d​∗​​=max​α,β;α​i​​≥0​​ min​ω​​L(ω,α,β) If both of the optimization solution of the primal problem d∗d^*d​∗​​ and that of the dual problem p∗p^*p​∗​​ exsit, we have the theorem of weak duality: d∗≤p∗d^* \leq p^* d​∗​​≤p​∗​​ Proof: minωL(ω,α,β)≤L(ω,α,β)≤maxα,β;αi≥0L(ω,α,β)min_\omega \mathscr{L}(\omega, \alpha, \beta) \leq \mathscr{L}(\omega, \alpha, \beta) \leq max_{\alpha,\beta; \alpha_i \geq 0} \mathscr{L}(\omega, \alpha, \beta) min​ω​​L(ω,α,β)≤L(ω,α,β)≤max​α,β;α​i​​≥0​​L(ω,α,β) maxα,β;αi≥0 minωL(ω,α,β)≤minω maxα,β;αi≥0L(ω,α,β)max_{\alpha,\beta; \alpha_i \geq 0} \ min_\omega \mathscr{L}(\omega, \alpha, \beta) \leq min_\omega \ max_{\alpha,\beta; \alpha_i \geq 0} \mathscr{L}(\omega, \alpha, \beta) max​α,β;α​i​​≥0​​ min​ω​​L(ω,α,β)≤min​ω​​ max​α,β;α​i​​≥0​​L(ω,α,β) If the primal optimization problem is solved through the dual optimization problem, we need make sure that $ d^* = p^* $, which is called the strong duality. If there exist a saddle point of L∗(ω∗,α∗,β∗)\mathscr{L^*}(\omega^*, \alpha^*, \beta^*)L​∗​​(ω​∗​​,α​∗​​,β​∗​​), d∗=p∗=L∗(ω∗,α∗,β∗)d^* = p^* = \mathscr{L^*}(\omega^*, \alpha^*, \beta^*) d​∗​​=p​∗​​=L​∗​​(ω​∗​​,α​∗​​,β​∗​​) The saddle point satisfies the Karush-Kuhn-Tucker (KKT) conditions: ∂∂ωL(ω,α,β)=0\frac{\partial}{\partial \omega} \mathscr{L}(\omega, \alpha, \beta)=0 ​∂ω​​∂​​L(ω,α,β)=0 ∂∂αiL(ω,α,β)=0, i=1,...,k\frac{\partial}{\partial \alpha_i} \mathscr{L}(\omega, \alpha, \beta)=0,\ i=1,...,k ​∂α​i​​​​∂​​L(ω,α,β)=0, i=1,...,k ∂∂βiL(ω,α,β)=0, i=1,...,l\frac{\partial}{\partial \beta_i} \mathscr{L}(\omega, \alpha, \beta)=0,\ i=1,...,l ​∂β​i​​​​∂​​L(ω,α,β)=0, i=1,...,l αigi(ω)=0, i=1,...,k\alpha_i g_i(\omega) = 0,\ i=1,...,k α​i​​g​i​​(ω)=0, i=1,...,k gi(ω)≤0, i=1,...,kg_i(\omega) \leq 0,\ i=1,...,k g​i​​(ω)≤0, i=1,...,k αi≥0, i=1,...,k\alpha_i \geq 0,\ i=1,...,k α​i​​≥0, i=1,...,k hj(ω)=0, j=1,...,lh_j(\omega) = 0,\ j=1,...,l h​j​​(ω)=0, j=1,...,l Note that, gi(ω)=0g_i(\omega) = 0g​i​​(ω)=0 can be derived if αi&gt;0\alpha_i &gt; 0α​i​​&gt;0 is satisfied. The theorem is described as: If ω∗,α∗andβ∗\omega*, \alpha^* and \beta^*ω∗,α​∗​​andβ​∗​​ satisfy the KKT condition, it is also a solution to the primal and the dual problems. Recall the optimization problem in the section Geometrical margin, as: minω,b12∣∣ω∣∣2min_{\omega,b} \frac{1}{2}|| \omega ||^2 min​ω,b​​​2​​1​​∣∣ω∣∣​2​​ s.t. 1−yi(ω⃗⋅xi⃗−b)≤0,i=1,...,m1 - y_i(\vec{\omega}\cdot \vec{x_i} - b) \leq 0, i=1,...,m1−y​i​​(​ω​⃗​​⋅​x​i​​​⃗​​−b)≤0,i=1,...,m, note that there are mmm conditons. The Lagrangian function is: L(ω,b,α)=12ω⃗Tω⃗−∑i=1mαi[yi(ω⃗⋅xi⃗−b)−1]\mathscr{L}(\omega,b, \alpha) = \frac{1}{2} \vec{\omega}^T \vec{\omega} - \sum_{i=1}^m \alpha_i [y_i(\vec{\omega}\cdot \vec{x_i} - b) - 1] L(ω,b,α)=​2​​1​​​ω​⃗​​​T​​​ω​⃗​​−​i=1​∑​m​​α​i​​[y​i​​(​ω​⃗​​⋅​x​i​​​⃗​​−b)−1] Thus, the primal optimization problem is equivalent to: p∗=minω,b maxα,β;αi≥0L(ω,b,α)p* = min_{\omega,b} \ max_{\alpha,\beta; \alpha_i \geq 0} \mathscr{L}(\omega,b, \alpha) p∗=min​ω,b​​ max​α,β;α​i​​≥0​​L(ω,b,α) The corresponding dual problem is: d∗=maxα,β;αi≥0 minω,bL(ω,b,α)d* = max_{\alpha,\beta; \alpha_i \geq 0}\ min_{\omega,b} \mathscr{L}(\omega,b, \alpha) d∗=max​α,β;α​i​​≥0​​ min​ω,b​​L(ω,b,α) The minω,bL(ω,b,α)min_{\omega,b} \mathscr{L}(\omega,b, \alpha)min​ω,b​​L(ω,b,α) can be solved by letting ∇ωL=0\nabla_\omega \mathscr{L} =0∇​ω​​L=0 and ∇bL=0\nabla_b \mathscr{L} =0∇​b​​L=0. ∇ωL=ω⃗−∑i=1mαiyixi⃗=0\nabla_\omega \mathscr{L} = \vec{\omega} - \sum_{i=1}^m \alpha_i y_i \vec{x_i} = 0 ∇​ω​​L=​ω​⃗​​−​i=1​∑​m​​α​i​​y​i​​​x​i​​​⃗​​=0 ∇bL=∑i=1mαiyi=0\nabla_b \mathscr{L} = \sum_{i=1}^m \alpha_i y_i = 0 ∇​b​​L=​i=1​∑​m​​α​i​​y​i​​=0 Thus, ω⃗=∑i=1mαiyixi⃗\vec{\omega} = \sum_{i=1}^m \alpha_i y_i \vec{x_i} ​ω​⃗​​=​i=1​∑​m​​α​i​​y​i​​​x​i​​​⃗​​ ω⃗Tω⃗=∑i,j=1mαiαjyiyj(xi⃗Txj⃗)\vec{\omega}^T \vec{\omega} = \sum_{i,j=1}^m \alpha_i \alpha_j y_i y_j (\vec{x_i}^T \vec{x_j}) ​ω​⃗​​​T​​​ω​⃗​​=​i,j=1​∑​m​​α​i​​α​j​​y​i​​y​j​​(​x​i​​​⃗​​​T​​​x​j​​​⃗​​) ∑i=1mαi[yi(ω⃗⋅xi⃗−b)]=∑i=1mαiyiω⃗⋅xi⃗−0=∑i=1mαiyiω⃗Txi⃗=∑i,j=1mαiαjyiyj(xi⃗Txj⃗)=ω⃗Tω⃗\sum_{i=1}^m \alpha_i [y_i(\vec{\omega}\cdot \vec{x_i} - b)] = \sum_{i=1}^m \alpha_i y_i \vec{\omega}\cdot \vec{x_i} - 0 = \sum_{i=1}^m \alpha_i y_i \vec{\omega}^T \vec{x_i} = \sum_{i,j=1}^m \alpha_i \alpha_j y_i y_j (\vec{x_i}^T \vec{x_j}) = \vec{\omega}^T \vec{\omega} ​i=1​∑​m​​α​i​​[y​i​​(​ω​⃗​​⋅​x​i​​​⃗​​−b)]=​i=1​∑​m​​α​i​​y​i​​​ω​⃗​​⋅​x​i​​​⃗​​−0=​i=1​∑​m​​α​i​​y​i​​​ω​⃗​​​T​​​x​i​​​⃗​​=​i,j=1​∑​m​​α​i​​α​j​​y​i​​y​j​​(​x​i​​​⃗​​​T​​​x​j​​​⃗​​)=​ω​⃗​​​T​​​ω​⃗​​ L(ω,b,α)=∑i=1mαi−12∑i,j=1mαiαjyiyj(xi⃗Txj⃗)\mathscr{L}(\omega,b, \alpha) = \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i,j=1}^m \alpha_i \alpha_j y_i y_j (\vec{x_i}^T \vec{x_j}) L(ω,b,α)=​i=1​∑​m​​α​i​​−​2​​1​​​i,j=1​∑​m​​α​i​​α​j​​y​i​​y​j​​(​x​i​​​⃗​​​T​​​x​j​​​⃗​​) The training data points whose the weight αi\alpha_iα​i​​ are nonzero are called the support vectors. The optimal ω⃗\vec{\omega}​ω​⃗​​ is a linear combination of a small number of data points. For a new data z⃗\vec{z}​z​⃗​​, we can compute: ζ=ω⃗⋅z⃗−b=∑i=1mαiyi(xi⃗Tz⃗)−b\zeta = \vec{\omega} \cdot \vec{z} - b = \sum_{i=1}^m \alpha_i y_i (\vec{x_i}^T \vec{z}) - b ζ=​ω​⃗​​⋅​z​⃗​​−b=​i=1​∑​m​​α​i​​y​i​​(​x​i​​​⃗​​​T​​​z​⃗​​)−b where z⃗\vec{z}​z​⃗​​ is classified as class 1 if ζ&gt;0\zeta&gt;0ζ&gt;0 and as class 2 otherwise. It means we make decisions by comparing each new example with only the support vectors. The kernel method The SVM optimization problem is rewritten as finding the maximum of: L(ω,b,α)=∑i=1mαi−12∑i,j=1mαiαjyiyj(xi⃗Txj⃗)\mathscr{L}(\omega,b, \alpha) = \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i,j=1}^m \alpha_i \alpha_j y_i y_j (\vec{x_i}^T \vec{x_j}) L(ω,b,α)=​i=1​∑​m​​α​i​​−​2​​1​​​i,j=1​∑​m​​α​i​​α​j​​y​i​​y​j​​(​x​i​​​⃗​​​T​​​x​j​​​⃗​​) s.t. 0≤αi≤C,i=1,...,m0\leq \alpha_i \leq C, i=1,...,m0≤α​i​​≤C,i=1,...,m and ∑i=1mαiyi=0\sum\limits_{i=1}^m \alpha_i y_i=0​i=1​∑​m​​α​i​​y​i​​=0. Note that there are m+1m+1m+1 conditions. The data points only appear as inner product, not their explicit coordinates. Typically in the original features, the data is not linearly-separable. We can replace x⃗\vec{x}​x​⃗​​ by ϕ(x⃗)\phi(\vec{x})ϕ(​x​⃗​​) for some nonlinear ϕ\phiϕ, and the decision boundary is a nonlinear surface ω⃗⋅ϕ(x⃗)−b=0\vec{\omega} \cdot \phi(\vec{x}) - b = 0​ω​⃗​​⋅ϕ(​x​⃗​​)−b=0. For example, let ϕ=xy\phi=xyϕ=xy, the features are expanded into three dimensional point as (x,y,xy) from two dimensional point (x,y). Define the kernel function KKK by: K(xi⃗,xj⃗)=ϕ(xi⃗)Tϕ(xj⃗)K(\vec{x_i},\vec{x_j}) = \phi(\vec{x_i})^T \phi(\vec{x_j}) K(​x​i​​​⃗​​,​x​j​​​⃗​​)=ϕ(​x​i​​​⃗​​)​T​​ϕ(​x​j​​​⃗​​) The expanded SVM optimization problem is written as finding the maximum of: L(ω,b,α)=∑i=1mαi−12∑i,j=1mαiαjyiyjϕ(xi⃗)Tϕ(xj⃗)\mathscr{L}(\omega,b, \alpha) = \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i,j=1}^m \alpha_i \alpha_j y_i y_j \phi(\vec{x_i})^T \phi(\vec{x_j}) L(ω,b,α)=​i=1​∑​m​​α​i​​−​2​​1​​​i,j=1​∑​m​​α​i​​α​j​​y​i​​y​j​​ϕ(​x​i​​​⃗​​)​T​​ϕ(​x​j​​​⃗​​) s.t. 0≤αi≤C,i=1,...,m0\leq \alpha_i \leq C, i=1,...,m0≤α​i​​≤C,i=1,...,m and ∑i=1mαiyi=0\sum\limits_{i=1}^m \alpha_i y_i=0​i=1​∑​m​​α​i​​y​i​​=0. For a new data z⃗\vec{z}​z​⃗​​, let’s see: ζ=ω⃗⋅z⃗−b=∑i=1mαiyiϕ(xi⃗)Tϕ(z⃗)−b\zeta = \vec{\omega} \cdot \vec{z} - b = \sum_{i=1}^m \alpha_i y_i \phi(\vec{x_i})^T \phi(\vec{z}) - b ζ=​ω​⃗​​⋅​z​⃗​​−b=​i=1​∑​m​​α​i​​y​i​​ϕ(​x​i​​​⃗​​)​T​​ϕ(​z​⃗​​)−b where z⃗\vec{z}​z​⃗​​ is classified as class 1 if ζ&gt;0\zeta&gt;0ζ&gt;0 and as class 2 otherwise. The kernel trick The problem is, the feature expansion yield lots of features because it is high dimensional. The feature space is typically infinite-dimensional. For example, the polynomial of degree kkk on nnn original features yields O(nk)O(n^k)O(n​k​​) expanded features. Thus the feature expansion could be inefficient and sometimes overfitting. By using the kernel function mapping, we can operate in high-dimensional and implicit feature space only computing the inner products \phi(\vec{x_i})^T \phi(\vec{x_j}). This operation is often computationally cheaper than the explicit computation of ϕ\phiϕ. This approach is called the kernel trick, to make optimization efficient when there are lots of features. Linear kernel: K(x⃗,x⃗′)=x⃗Tx⃗′K(\vec{x},\vec{x}&#x27;) = \vec{x}^T \vec{x}&#x27;K(​x​⃗​​,​x​⃗​​​′​​)=​x​⃗​​​T​​​x​⃗​​​′​​ Polynomial kernel: K(x⃗,x⃗′)=(1+x⃗Tx⃗′)pK(\vec{x},\vec{x}&#x27;) = (1 + \vec{x}^T \vec{x}&#x27;)^pK(​x​⃗​​,​x​⃗​​​′​​)=(1+​x​⃗​​​T​​​x​⃗​​​′​​)​p​​ Radial basis kernel (RBK): K(x⃗,x⃗′)=exp(−12∣∣x⃗−x⃗′∣∣2)K(\vec{x},\vec{x}&#x27;) = exp(-\frac{1}{2} ||\vec{x} - \vec{x}&#x27;||^2)K(​x​⃗​​,​x​⃗​​​′​​)=exp(−​2​​1​​∣∣​x​⃗​​−​x​⃗​​​′​​∣∣​2​​) The kernel matrix If we have the feature mapping ϕ\phiϕ for x1,x2,...,xmx_1,x_2,...,x_mx​1​​,x​2​​,...,x​m​​, the kernel KKK is a x×xx\times xx×x matrix as: Ki,j=ϕ(xi⃗)Tϕ(xj⃗){K_{i,j}} = \phi(\vec{x_i})^T \phi(\vec{x_j}) K​i,j​​=ϕ(​x​i​​​⃗​​)​T​​ϕ(​x​j​​​⃗​​) where KKK is a symmetric positive-semidefinite matrix (or non-negative definite: all of whose eigenvalues are nonnegative). Ki,j=Kj,iK_{i,j}=K_{j,i} K​i,j​​=K​j,i​​ The necessary and sufficient condition of kernel, Mercer’s theorem: KKK is a continuous symmetric non-negative definite kernel. The eigenvalues λi\lambda_iλ​i​​ is nonnegative. This KKK is also called the Mercer kernel. yTKy≤0,∀yy^T K y \leq 0, \forall y y​T​​Ky≤0,∀y Sequential minimal optimization (SMO) algorithm Coordinate descent / ascend Coordinate descent is used to deal with the unconstrained optimization problems. To find the minimum of a multivariable function, coordinate descent (ascend) is an optimization algorithm that successively minimizes (maximizes) along one coordinate directions while fixing all other coordinates. It may get stuck for a non-smooth multivariable function. SMO The expanded SVM optimization problem above is a constrained one. Training a support vector machine requires the solution of a very large quadratic programming (QP) optimization problem. Sequential minimal optimization breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. (John C. Platt, 1998) The constraint involving the Lagrange multipliers αi\alpha_iα​i​​, the smallest possible problem involves two such multipliers, α1\alpha_1α​1​​ and α2\alpha_2α​2​​. The SMO algorithm is: Select one pair αi\alpha_iα​i​​ and αj\alpha_jα​j​​ and optimize the pair (αi,αj\alpha_i, \alpha_jα​i​​,α​j​​) while holding other multipliers fixed. Repeat till convergence. Solved when all the multipliers satisfy the KKT conditions. Heuristics are used to choose the pair of multipliers so as to accelerate the rate of convergence, since there are n(n−1)2\frac{n(n-1)}{2}​2​​n(n−1)​​ choices for (αi,αj\alpha_i, \alpha_jα​i​​,α​j​​). α1y1+α2y2=ξ\alpha_1 y_1 + \alpha_2 y_2 = \xi α​1​​y​1​​+α​2​​y​2​​=ξ 0≤α1≤C, 0≤α2≤C0 \leq \alpha_1 \leq C,\ 0 \leq \alpha_2 \leq C 0≤α​1​​≤C, 0≤α​2​​≤C where ξ\xiξ is the negative of the sum over the rest of terms in the equality constraint, which is fixed in each iteration. The SMO algorithm can be considered a special case of the Osuna algorithm, where the size of the optimization is two and both Lagrange multipliers are replaced at every step with new multipliers that are chosen via good heuristics.]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
        <tag>Support Vector Machine</tag>
        <tag>Hyperplane</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Adobe Premiere -- Introduction to video editing]]></title>
    <url>%2Fblog%2F2018%2F06%2F07%2Fintroduction-to-premiere%2F</url>
    <content type="text"><![CDATA[本文介绍如何使用Adobe公司旗下的Premiere来剪辑视频。本文只介绍一些基本操作，仅供入门。其他多种多样的玩法以后有时间再慢慢摸索。 Premiere简介 首先简单介绍一下Premiere这个软件。以下摘自中文维基百科：Adobe Premiere Pro是由Adobe公司开发的非线性编辑的视频编辑软件。Premiere是Creative Suite套装的一部分，可用于图像设计、视频编辑与网页开发。Premiere Pro支持许多不同插件以加强其功能、增加额外的视频/声音效果及支持更多的文件格式。Premiere Pro能支持非常高的清晰度，可最高支持10,240x8,192的显示屏清晰度，以及高至32位的色深，可使用RGB和YUV颜色模型。在声音方面，能支持VST声音插件，以及5.1声道环绕立体声。Premiere Pro的插件能够导入与导出至QuickTime或DirectShow的格式。Premiere Pro也支持很多视频与声音格式，在导出和导入视频时，也提供很多编码解码器。通过使用Cineform Neo line的插件，就可以支持3D编辑功能，也可以在2D的显示屏上看见3D物料。Photoshop文件（psd档）可直接导入至Premiere Pro中。 注意， 当前版本仅支持64位操作系统。使用Premiere的另一个原因是其对wmv格式的视频文件支持很友好。如果出现无法打开的视频素材，请使用“格式工厂”进行转换。PS. 格式工厂是目前windows上我用过的最好的格式转换软件。 非线性编辑指的是对视频或音频的素材通过电脑设备或其他数字随机存取的方式剪辑，意味着能够对视频片段中的任意一帧进行操作。在最初电影剪接的过程中，电影底片必须被剪断。数字视频技术出现后，产生了非线性剪接手段。与过去需要两台以上的录像机，从不同的磁带合成到一盘磁带的线性编辑方式相比，能立即重新排列、替换、增加、删除、修改映像数据，以达到快速编辑的目标，并且理论上素材质量不会损失。 视频加速 我在工作中一个需求是对视频加速。原始视频文件是经过慢放处理的，市场一共接近三分钟，而我希望在演讲PPT时在10秒内放完，那我就需要加速至1800%。那么打开Premiere，进行如下操作。 新建项目。 导入视频文件。双击左下区域内的位置，选择所有要导入的素材即可。 新建轨道。右击图中1箭头所示的按钮即可新建轨道等。或者直接将左下区域内出现的视频文件拖至此按钮上。然后右下区域就会出现对应的文件和轨道，按行排列。上部为视频轨道，下部为音频轨道。左上区域可以对单个轨道进行编辑，右上区域可以预览叠加后的效果。拖动右下区域的红色竖线可预览不同时刻。 调整速度。在轨道上视频素材对应位置处右击可以看到很多操作，编辑“剪辑速度/持续时间”，改变数值可使视频加速播放或减速播放。此时可以看到对应轨道上的长度变化。 视频剪辑 将视频导入轨道之后，便可以对时间线进行删减。使用快捷键C，此时时间线会从你cut的地方断开，视频会变成两段，重新拖动便可改变顺序，或提取出你需要的部分重新进行拼接。使用快捷键D，可以删掉当前所在的片段。不使用波纹编辑的话，视频删掉的那部分会变成空白。使用波纹编辑，删掉的部分后续片段会自动衔接至上一片段。 合二为一 我的另一个需求是对两个视频一左一右进行对比，同时播放。于是不仅要调整播放速度使之每个时刻保持一致，还要调整每个视频的位置和比例。 效果控件。左上区域顶部有一排菜单，调整视频位置和比例需要用到其中的效果控件。 单击“运动”。然后转到右上区域，点击要调整的视频，可以看到视频四周出现了调整位置大小的控制点，拖动即可编辑。预览一下，看看是不是实现了同步播放。 调整背景。默认合成的视频背景色是黑色，我们可以调整为白色。有很多方法，这里介绍其中一个。在之前新建轨道的左下角那个按钮上点击，新建彩色蒙版。颜色改为白色。将该片段拖至所有视频轨道下方即可。 输出 输出编辑好的视频时，可以调整输出格式、位置等等。也可以添加滤镜、更改视频解码器等等。这里也可以选取输出的时间片段。拖动橘色的时间线，便可更改视频的起始位置。最后点击导出即可。 以后有其他需求再更新本文。]]></content>
      <categories>
        <category>吴带当风</category>
      </categories>
      <tags>
        <tag>Post Processing</tag>
        <tag>Video Editing</tag>
        <tag>Premiere</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[企业的故事]]></title>
    <url>%2Fblog%2F2018%2F05%2F12%2FCV-Enterprise%2F</url>
    <content type="text"><![CDATA[引言 企业（Enterprise）号可能是历史上最传奇的一个名字了，除了英法，仅美国共有多达9艘军舰和1架航天飞机以“Enterprise”命名，甚至星际迷航里的联邦星舰NCC-1701A也是。初次印象可能有些奇怪，为什么一个毫无特色的名词这么流行呢？其实，这是翻译的问题。本文就先从Enterprise的含义说起。 来看Webster’s Dictionary韦氏词典上Enterprise的解释： a project or undertaking that is especially difficult, complicated, or risky a unit of economic organization or activity, especially: a business organization readiness to engage in daring or difficult action 可以看到Enterprise有三层意思：第一，可以表示艰难、复杂、冒险的计划或项目。第二是常见的企业的意思。第三，表示对艰巨的行动做好准备、充满信心、雄心勃勃，意译也就是进取、事业心、探险精神的意思。所以命名为Enterprise的本意是“进取”。例如，开头提到的航天飞机Space Shuttle OV-101也叫Enterprise，一般翻译成进取号。不过因为约定俗成,大家都统一叫企业了。 限于篇幅，本文主要介绍美国的第七艘和第八艘企业：分别是二战期间的航空母舰CV-6和现代航空母舰CVN-65。其中，CV-6在二战中表现极为突出，被美国人亲切地称为&quot;大E&quot;（The Big E）和&quot;幸运E&quot;（Lucky E）。由于日本曾多次单方面宣称企业被击沉，后来却又总是在战斗中遇见，所以也被日本人称为“灰色幽灵”（The Grey Ghost）。CVN-65则是全世界第一艘核动力航空母舰，上世纪六十年代的产物，至今仍远远领先于绝大多数国家。 企业CV-6 1933年，美国总统富兰克林·罗斯福发布了罗斯福新政，以工代赈。国会向海军提供高达两亿三千八百万美元的拨款，用于更新舰艇装备，本来只计划建造小型航空母舰的美国海军开始兴建新的大型航空母舰。编号为CV-6的航空母舰，作为约克城级的第二艘，于1936年10月3日下水，地点是在著名的航母摇篮：纽波特纽斯造船厂。1938年5月12日这艘当时还默默无闻的大船正式服役，直到二战胜利后的1947年2月17日正式退役。在她的一生之中，企业一共航行了442,475公里，击沉敌舰71艘，击破敌舰192艘，击毁敌机911架。整个太平洋战争中，企业一共获得20枚战斗之星，为美国海军舰艇之最。 时间回到1941年12月7日，日本帝国海军旗下六艘大型航空母舰组成的舰队，成功偷袭了美国珍珠港海军基地。在持续1个小时50分钟的轰炸后，美国海军共损失了8艘战列舰、3艘巡洋舰、3艘驱逐舰，188架战机，2402人死亡，1282人受伤。然而匪夷所思的是，美国在太平洋战区的三艘主力航空母舰（列克星敦、萨拉托加和企业）都不在家，幸免于难。企业号原定在前一日回珍珠港，但因风浪而有所延误。珍珠港袭击发生时，美国海军一共只有7艘航空母舰，除了珍珠港的3艘以外，2艘在大西洋（胡蜂和突击者），2艘在本土的诺福克（约克城和大黄蜂）。而彼时日本海军已建成服役10艘航空母舰：赤城，加贺、苍龙、飞龙、翔鹤、瑞鹤、凤翔、龙骧、祥凤和瑞凤。珍珠港事件后，抗日小分队约克城、胡蜂和大黄蜂开赴太平洋支援。值得一提的是，珍珠港事件使美国高层充分意识到，拥有舰载飞机的航空母舰在现代海战中具有极其可怕的破坏力，从此海军的建设重心开始向航空母舰倾斜。而日本则沉浸在珍珠港的胜利中不思进取，其高层仍旧认为战列舰才是大洋上决战的核心力量，航母只是用来偷袭、护航等等，无意中忽视了航空母舰的地位，直到1944年才幡然悔悟，然而为时已晚。有一个小插曲，那天企业的一架SBD轰炸机正飞向珍珠港福特岛海军基地，被友军当成了前来偷袭的日本飞机而击落，两名飞行员牺牲。 1942年2月，企业与约克城分别空袭马绍尔群岛及吉尔伯特群岛。企业炸沉了一艘运输船，并炸伤了几艘轻巡洋舰。2月24日企业空袭威克岛，战果甚微。 1942年4月18日，杜立特空袭东京。陆军的B-25轰炸机如果从珍珠港起飞前往东京，那航程肯定不足，怎么办呢？一位空军上将杜立特提出了一个疯狂的想法，那就是用航空母舰把这些大飞机运到日本附近海域，然后再从航母上起飞。可是航母甲板相对于陆地机场来说要短的多，于是B-25经过了魔改，起飞距离从3000英尺缩短到了800英尺，终于能从大黄蜂上起飞。此时大黄蜂的舰载机都被B-25所代替，所以毫无自卫能力，企业承担起舰队的护航任务。空袭行动意外地被日本渔船发现，为了避免全军覆没，B-25轰炸机只好提前起飞，大黄蜂和企业也迅速撤离战场。那起飞的轰炸机回不来了怎么办呢？当时他们被要求飞往中国迫降，其中很多飞行员被中国农民救助生还。杜立特空袭东京的成功可以说是奇迹。1993年杜立特去世，葬礼上，所有尚可飞行的B-25全部升空以示悼念。4月25日，企业返回珍珠港修整。 1942年5月8日，珊瑚海海战爆发，战况十分惨烈，双方各有损失。美国方面，“列太太”：列克星敦(Lady Lex)被击沉，约克城受损严重。日本方面，轻型航母祥凤被击沉，主力航母翔鹤也受重创，而另一艘主力航母瑞鹤的舰载机损耗严重。这对后续爆发的中途岛海战产生了消极影响。珍珠港之后太平洋战场所有战役中，企业只缺席了这一次。 1942年6月4日，中途岛海战爆发。日本海军由于翔鹤瑞鹤的缺席，只有四艘主力航母与美军进行对决。而由于日本的密码被美军破译，使得日本偷袭中途岛的计划被美军掌握的一清二楚。结果就变成了偷袭不成反被歼，美国以损失一艘大型航母的代价（约克城被击沉），全灭日本的四艘主力航母，其中，企业功不可没。那天上午，本应平均分配各个攻击编队的人数，但因企业指挥失误，导致超过27架轰炸机去猛揍了加贺，并很快送它到了太平洋海底，而只有3架轰炸机及时转向前往轰炸赤城，只有一枚炸弹击中。神奇的是该炸弹引爆了赤城机库的弹药，引发后续的连环爆炸。下午，约克城的侦察机找到了飞龙，企业及时起飞轰炸机编队，将飞龙击沉。 1942年5月，日军占领瓜达尔卡纳尔岛。8月7日，美军陆战队登陆图拉吉以及瓜岛部分地区，包括亨德森机场，开启瓜岛争夺战的序章。美军的三艘航空母舰胡蜂、萨拉托加与企业均参加了战斗，为陆战队提供空中支援。由于登陆顺利，美军第61特遣舰队司令弗莱彻在9日决定将三航母后撤以补充燃油。这项争议性的命令使美军陆战队失去了制空权，并间接导致萨沃岛海战中美军遭遇惨败。 1942年8月24日，东所罗门海战爆发，双方各有损失。美国海军萨拉托加击沉日本海军龙骧。企业号遭到数十架日本飞机集火，飞行甲板被贯穿，引发大火。企业严重受损，只好回到珍珠港大修。8月31日，萨拉托加也受到鱼雷攻击受损严重，同样回到珍珠港修理。 1942年9月15日，胡蜂号航母被日本海军潜艇伊19击沉。10月18日，尼米兹任命哈尔西为南太平洋战区总司令。10月23日，企业返回南太平洋战区。 1942年10月26日，圣克鲁斯群岛海战爆发。大黄蜂的轰炸机重创了翔鹤，而自己也被重创，不得已自沉。企业也受损严重，回到努美阿紧急维修。10月26日的早晨，远方的大黄蜂缓缓下沉，只剩一艘受损的企业孤军奋战。于是他们在企业的飞行甲板上拼出了那个著名的标语：企业vs日本（Enterprise vs Japan），在这样绝望的处境里鼓舞了所有美国人。日本方面，虽然没有航母沉沒，但是致命的是大量有经验的飞行员被消耗。至此，参加了珍珠港事件的765名日本精英飞行员中，至少有409人死亡。翔鹤被迫撤出一线战斗一直维修到了1943年。瑞鹤也因缺乏机组人员被迫返回日本本土。由于时间紧迫，11月11日，企业离开努美阿，再次前往瓜岛，此时抢修人员则继续在舰内赶工。11月14日，企业击沉了日本海军重巡洋舰衣笠和七艘运输船。瓜岛战役结束后，1943年5月8日，企业返抵珍珠港。5月27日企业因其优秀的表现，荣获美国总统集体嘉奖。此时，美国海军的新式航空母舰埃塞克斯级如下饺子般陆续服役。 时间回到1944年6月15日，美军在塞班岛登陆。随后6月19日爆发的菲律宾海海战，是历史上最大的航空母舰对决。美国海军第五舰队此时已拥有15艘航空母舰，而日本海军实力虽早已不如当年，已是困兽之斗，但东拼西凑也能找出9艘航空母舰。企业隶属于第五舰队第三支队，仅该支队就拥有企业、新列克星敦、普林斯顿、圣贾辛托四艘大型航空母舰。由于实力悬殊，日本大败，损失了3艘航空母舰和378架飞机。有趣的是，由于美军的飞机和弹药技术上均大幅领先日军飞机，导致空战变成了单方面的大屠杀，这一波128架来袭的日军飞机一共损毁97架。著名的马里亚纳射火鸡大赛（The Great Marianas Turkey Shoot）由此而来。神奇的是，夜间返航时美军飞机燃油耗尽争先恐后降落导致混乱，仅降落作业就损失了80架，而与日军作战而损失的飞机只有20架。日本的主力航空母舰翔鹤、大凤、飞鹰被击沉，仅存的瑞鹤被企业和约克城联合攻击而严重受损，一度下令弃船，后瑞鹤侥幸被拖回并修复。日本首相东条英机在此期间倒台。日本彻底丧失了西太平洋制海权，“绝对国防圈”遭突破，美国陆军的大型轰炸机B-29进驻马里亚纳，并使用凝固汽油弹对日本本土进行空袭。 1944年10月20日，麦克阿瑟宣布重返菲律宾，期间企业提供了有效的空中支援。此时，莱特湾海战已悄然爆发。共计21艘航空母舰、21艘战列舰、170艘驱逐舰与近2,000架军机参与了此次战斗，为人类历史上最大规模的海战。日本海军孤注一掷，然而被彻底摧毁。无望之下，这也是日本第一次发动神风特攻队自杀攻击。在恩加尼奥角附近，日本海军四艘航空母舰组成了诱饵编队，成功地钓到了大鱼：包括企业在内的美国海军主力航空母舰舰队，而日本最后的主力航空母舰瑞鹤被迫牺牲，被击沉。另一边，萨马岛附近，美国海军用于对地支援的第七舰队只有几艘护航航母和驱逐舰，遭遇了日本海军的主力战列舰编队。于是金凯德不断地给哈尔西发电报求援。连坐镇珍珠港的总司令尼米兹也给哈尔西发了一份简短的电报：“第34特混舰队在哪里？”，但负责电报加密的军官，随意添加了一句“全世界都想知道”，于是电文就变成了“全世界都想知道，第34特混舰队在哪里？”。太平洋上的译码人员误以为是正文未加删减，导致哈尔西十分生气，又耽误了很长时间。那边日本的主力舰队正开心地炮轰美军护航航母，尽管美军驱逐舰拼死骚扰以争取时间，但美军还是损失惨重。正在美国舰队苦苦挣扎的时候，日本司令栗田认为美军主力很快就要赶来，为了保存实力，于是日本主力开始向北撤退。金刚号在返航日本途中，在台湾海峡附近海域被美军潜艇击沉。至此，日本海军绝大部分已经消耗殆尽，仅剩一艘受伤的战列舰大和。 1945年2月19日，美国登陆硫磺岛，航空母舰萨拉托加前往掩护登陆部队。萨拉托加运气总不好，21日遭日本神风特攻队重创，被迫撤出。企业接替她的位置，加入护航航空母舰的编队。从2月23日到3月2日，企业上的飞机昼夜不间断地起降、飞行、执行任务，创下美国航母持续最长的飞行作业纪录。 时间回到1945年3月18日，冲绳战役期间，企业遭到日本轰炸机袭击，炸弹因投弹高度过低而没有引爆，后却被友军的高射炮击中而受损。由于维修而错过了最后围歼战舰大和的坊之岬海战。5月14日，企业遭遇了大批次神风自杀式攻击，舰艏飞行甲板严重扭曲，升降台彻底损毁，飞机无法启起飞，虽不至于沉没，但企业因此失去了作战能力，不得不返回美国普吉湾海军基地大修，直到战争结束。 1945年9月2日，东京湾，在美国最新的密苏里号战列舰上，日本向同盟国签字投降，中国代表徐永昌参加了签字仪式。密苏里州是美国时任总统杜鲁门的家乡。本来企业更有资格代表美国，但此时她仍在船坞中维修，遗憾地错过了东京湾的受降仪式。“在这庄严的仪式之后，我们将告别充满血腥屠杀的旧世界，迎来一个十分美好的世界，一个维护人类尊严的世界，一个致力于追求自由、宽容和正义的世界，这是我最热忱地希望，也是全人类的希望！”道格拉斯·麦克阿瑟说到。 战争结束了，输掉的自然已经灭亡，胜利者也是伤痕累累。太平洋上风云千樯，最后都永远地与这个古老的星球融为一体。 企业CVN-65 第七艘企业光荣退役以后，舰名由下一代航空母舰继承，也就是著名的CVN-65企业号核动力航空母舰。她的头衔可不少： 世界上第一艘核动力航空母舰 美国海军第一代核动力航空母舰 唯一一艘装备8座核反应堆的航空母舰 唯一一艘装备4片方向舵的航空母舰 当时世界上最大、最长的航空母舰 企业级核动力航母仅建造了一艘CVN-65 美国海军服役最长的航空母舰，一共服役了51年 同样诞生于美国航母的摇篮：纽波特纽斯造船厂。1958年2月4日开工，1960年9月24日下水。1961年11月25日企业CVN-65正式服役，12月编入美国大西洋舰队。1964年8月至10月，核动力航母企业、核动力巡洋舰长滩、核动力驱逐舰班布里奇组成“全核舰队”，，65天全程无补给航行了3万海里，绕地球一周，在那个年代堪称外星科技。当然，现在也没几个国家可以做到。 作为世界上第一艘核动力航空母舰，造价自然不菲，高达4.5亿美元（1958年）。满载排水量几乎是CV-6排水量的4倍。下表列出了两艘企业的数据对比。以下称CV-6为老企业，CVN-65为新企业。 技术数据 老企业CV-6 新企业CVN-65 标准排水量 19,900吨 75,700吨 满载排水量 25,600吨 94,000吨 船长 246.7米 342.3米 船宽 33.2米 40.5米 吃水 8.5米 11.9米 舰载机 97架 99架 升降台 3座 4座 弹射器 3座 4座 动力 9座Babcock&amp;Wilco锅炉 8座A2W核反应堆 主机 4座1000kw蒸汽轮机+2座200kw柴油轮机 16座2500kw发电轮机+4座1000kw柴油轮机 功率 120,000轴马力 280,000轴马力 极限航速 32.5节 33节 续航距离 12,000海里（15节） 无限 除了变大变强了以外，新企业又有哪些黑科技呢？首先，最重要的就是飞机弹射装置。以前，需要航母跑到极限航速，转向逆风，飞机从甲板末尾开始加速，才有可能起飞。有了弹射器的话，在飞机起飞的时候，航空母舰不需要再加速到极限，也不需要迎合风向，加速距离也大大减小，所以甲板上能摆更多的飞机。航空母舰的效率大大提升。进入喷气式飞机时代，弹射器更是大型航母发射重型舰载机的必备装备。 这张企业的老照片里，左边两个长条下面就是弹射器的轨道。因为弹射器的技术是绝密，即使是美国盟友也得不到技术转让，比如法国只能进口整个弹射器设备，英国干脆放弃了弹射器改为滑跃起飞。现代弹射器的技术非常复杂，即使是当年的苏联也没有研制成功。只有美国有这种逆天的黑科技，为了防止泄密，安装时都必须使用大棚覆盖。以前，活塞式飞机对弹射器要求很低，即使没有弹射装置也可以起飞。后来英国人发明了液压弹射器，但是技术并不成熟。但是喷气式飞机重量大大增加，对起飞速度要求很高。从越南战争起，美军航母上开始广泛使用蒸气弹射器。新企业上还装有内燃弹射器，不过性能并不令人满意，蒸汽弹射器还是主流。蒸汽弹射器的原理其实很简单，与小时候玩的弹弓类似。但为什么世界上绝大多数国家都造不出来呢？蒸汽弹射器的核心难点是气缸的制造和安装精度，还要频繁承受高压高温蒸汽压力，对缸体材料、制造工艺和维护保障的要求极高。其他国家不使用蒸汽弹射器的另一个原因是其能量转换效率极低，仅有6%，所以需要强大的动力才能保证蒸汽弹射器的运转。美国著名的C13型蒸汽弹射器，每次弹射就要消耗625公斤的蒸汽和1吨左右的缓冲淡水。如果航母每分钟弹射1架，那么连续弹射8架飞机之后，动力系统中蒸汽就会损失20%，动力输出随之损失32%，航速就要下降8节[2]。这就体现了核动力航空母舰的优势所在。美国最新的航空母舰上，蒸汽弹射器也已经被更为先进的电磁弹射器所取代，其便于维护，能量易于调节便于发射不同的飞机，功率损耗低寿命高，作战效率又会有一次飞跃。 第二个新技术是斜角甲板。对比老企业的甲板和新企业的甲板可以看到，老企业的甲板是直挺挺的，这种叫做全通甲板，易于建造。那它缺点也显而易见，所有飞机都大大咧咧摆在上面，起飞和降落不能同时进行，影响效率。有个英国人叫Dennis Campbell发明了斜角甲板，指一条和中线甲板呈角度的甲板跑道，其后部甲板更宽。这个角度一般是9度，在船长一定的情况下，采用斜角甲板能够拥有更长的跑道，并与船首的起飞跑道互不干扰。该设计可以保证同时进行飞机的起飞和降落，降落失败的飞机也可以再次加速复飞而不影响其他区域。斜角甲板技术表面上看让航空母舰变得更胖更丑了，但是却极大地提高了舰载机运转的效率，可以说是天才的发明。 原计划美国要建造6艘新企业级航空母舰，但因为当时核动力技术不成熟，动力系统成本及其昂贵导致它的造价远远超过预期。美国也没钱大手大脚，被迫改为建造常规动力的小鹰级，直到后来的尼米兹级核动力航空母舰的出现，动力系统日趋成熟使得成本下降，才得以大规模投入建造。由于单个反应堆的功率不高，只有35000轴马力，所以新企业上的A2W核反应堆多达8座，才能达到30节以上的高航速。后来技术成熟后，尼米兹号只需要2座A4W核反应堆。A代表航空母舰（Aircraft carrier），2和4分别代表第2代和第4代，W代表制造商西屋公司（Westinghouse）。核动力推进带来的是几乎无限的续航能力，秒天秒地。另一个冷知识是，新企业上没有烟囱，飞机起降不再会被浓烟迷住双眼。也不需要那些庞大的锅炉，节省下来的空间使航空燃油和航空炸弹的装载量大大增加，船员居住条件也更宽敞舒适。 说完了新企业的船体，再说说新企业的核心战斗力，舰载机。新企业常规搭载有20架F-14战斗机、60架F/A-18大战斗攻击机、4架EA-6B电子战飞机、4架E-2C预警机、8架S-3A/B反潜机。每一种飞机都可以单独写一篇介绍了。其中，F-14雄猫与企业一起参与了著名的电影“Top Gun” (中文译名壮志凌云）的拍摄，该电影由汤姆克鲁斯主演，当时是美国海军的征兵宣传片，俘获了一众猫迷的心。F-14最著名的是其AN/AWG-9长程雷达系统，和专为此雷达设计的AIM-54不死鸟空空导弹，能够同时追踪24个90km内的目标，并同时对其中6个目标进行攻击或栏截。这是宙斯盾系统出现之前，当时美军海军所拥有唯一的多目标同时作战系统。不死鸟导弹是全世界第一种主动雷达制导的空空导弹，有效射程高达184km。由于不死鸟导弹重量大、对雷达系统要求高，其他飞机无法使用。直到十七年之后的1991年，AIM-120先进中程空对空导弹服役后，美军才有其他战机有类似的多目标同时攻击能力。F-14第二著名的是其可变的机翼，机翼后掠角度可以由20°至68°之间变动，最大变动速度为每秒7°，由计算机自动控制。这项设计使机翼在任何高度与速度下都能达到最佳的升阻比，使得F-14有惊人的高速及转向性能。翼梁由钛合金制成，强度极高。在美国海军服役32年后，由于成本高昂以及维护复杂，F-14于2006年9月22日正式退役，由F/A-18E/F超级大黄蜂所取代。F/A-18也经常出现在电影中，比如变形金刚等大片里都有露脸。它是专门针对航空母舰起降而开发的对空、对地全天候多功能舰载机，虽然性能上不一定比得过传奇的F-14，但其胜任多用途、维护简单、成本低等特点，成为了航空母舰的最优选择。下图也是企业CVN-65最著名的系列照片之一，甲板上摆出了爱因斯坦著名的质能方程，彰显出其作为核动力航空母舰的骄傲。E既代表能量Energy，也代表企业Enterprise。甲板上密密麻麻的飞机中，大块头是著名的F-14D，机翼向上折叠的是/A-18系列。 至于新企业经历了各种大大小小的事件与冲突，比如著名的古巴导弹危机等等，由于牵扯到太多政治的恶臭，本文不再详述。 终章 如果只能选一个词来形容生命的伟大，那一定是enterprise。终有一天，地球上浩瀚的大洋也不再无垠，我们的征途将会是遥远的星河，地球只是故乡。正如《星际迷航》里联邦星舰NCC-1701A企业号所言：To boldly go where no man has gone before. 彩蛋 二战早期海战的主力还是战列舰，航空母舰只是个无足轻重的小角色，其肥大粗笨的船体和甲板，与型线流畅优美的战列舰比起来，可以说毫无颜值。因此航空母舰时常受到其他海军的嘲笑和不屑，被笑作“顶着舱盖的浴缸”。航母上的军官和水兵被嘲讽是“平顶水手”和“棕靴海军”。西奥多·梅森过去曾是战列舰BB-44加利福尼亚的船员，参加过护送企业CV-6驶往西海岸的行动。他退役后在回忆录中这样写道：“看着（企业）在汹涌波涛中上下起伏，我不禁觉得它就像是一个身形庞大、弯腰驼背的老头，被一大群保镖围着，那时候要是有人告诉我企业将会成为战争中最功勋卓著的战舰，我会毫不犹豫地嘲笑他是一个大傻瓜，依我之见，要把这艘船开到西海岸恐怕都会有麻烦。” 参考资料 [1] Enterprise vs Japan [2] 花费28年投32亿美元，让弹射器做到45秒弹射一架飞机]]></content>
      <categories>
        <category>故事</category>
      </categories>
      <tags>
        <tag>Navy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（4）贝耶斯分类]]></title>
    <url>%2Fblog%2F2018%2F05%2F02%2Fmachine-learning-4%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. Bayesian estimation If you flip a coin, what’s the probability it will fall with the head up? To solve this problem, you may flip the coin many times. You will get the data: X={Xi}i=1n,Xi∈H,TX=\{X_i\}^n_{i=1}, X_i \in {H,T} X={X​i​​}​i=1​n​​,X​i​​∈H,T where HHH means Head and TTT means Tail. The probabilityshould be: P(Head)=θ,P(Tail)=1−θP(Head) = \theta, P(Tail) = 1 - \theta P(Head)=θ,P(Tail)=1−θ If you flip the coin 100 times, with 57 heads and 43 tails, the probability of P(Head)P (Head)P(Head) should be 57/100. Because the flips are i.i.d., which means: Independent events Identically distributed according to Bernoulli distribution (n=1n=1n=1 means BErnoulli distribution) Maximum likelihood estimation The aim of the maximum likelihood estimation is to choose θ\thetaθ that maximizes the probability of observed data. MLE of probability of head can be calculated based on i.i.d. as below: θ^MLE=argmaxθP(X∣θ)\hat{\theta}_{MLE} = \arg \max \limits_{\theta} P(X \mid \theta) ​θ​^​​​MLE​​=arg​θ​max​​P(X∣θ) =argmaxθ∏i=1nP(Xi∣θ)= \arg \max \limits_{\theta} \prod_{i=1}^n P(X_i \mid \theta) =arg​θ​max​​​i=1​∏​n​​P(X​i​​∣θ) =argmaxθ∏i:Xi=HαHθ∏i:Xi=TαT(1−θ)= \arg \max \limits_{\theta} \prod_{i:X_i=H}^{\alpha_H} \theta \prod_{i:X_i=T}^{\alpha_T} (1-\theta) =arg​θ​max​​​i:X​i​​=H​∏​α​H​​​​θ​i:X​i​​=T​∏​α​T​​​​(1−θ) =argmaxθθαH(1−θ)αT= \arg \max \limits_{\theta} \theta^{\alpha_H} (1-\theta)^{\alpha_T} =arg​θ​max​​θ​α​H​​​​(1−θ)​α​T​​​​ Let J(θ)=θαH(1−θ)αTJ(\theta)=\theta^{\alpha_H}(1-\theta)^{\alpha_T}J(θ)=θ​α​H​​​​(1−θ)​α​T​​​​. It is easy to find the θ\thetaθ that maximizes the probability of observed data: ∂J(θ)∂θ∣θ=θ^MLE=0\frac{\partial J(\theta)}{\partial \theta} \Big\vert_{\theta=\hat{\theta}_{MLE}} = 0 ​∂θ​​∂J(θ)​​​∣​∣​∣​​​θ=​θ​^​​​MLE​​​​=0 Thus the MLE of probability of head is: θ^MLE=αHαH+αT\hat{\theta}_{MLE} = \frac{\alpha_H}{\alpha_H+\alpha_T} ​θ​^​​​MLE​​=​α​H​​+α​T​​​​α​H​​​​ Prior and posterior If you flip a coin 5 times, it is possible that 5 heads are obtained. In this case, the probability of head will be 1.0 if you use the maximum likelihood estimation. However, it is crazy to consider that the result of fliping a coin is always head. Thus, we need to use prior probability to adjust the probability calculation. A prior probability distribution of an uncertain quantity is the probability distribution that would express one’s beliefs about this quantity before some evidence is taken into account. A prior can be determined from past information such as previous experiments, or according to some principle such as Jeffreys prior. A posterior probability distribution of an unknow quantity is the probability distribution treated as the conditional probability that is assigned after the relevant evidence or background is taken into account. In contrasts with the likelihood function P(X∣θ)P(X \mid \theta)P(X∣θ), the posterior probability is given as: P(θ∣X)=P(X∣θ)P(θ)P(X)P(\theta \mid X) = \frac{P(X \mid \theta)P(\theta)}{P(X)} P(θ∣X)=​P(X)​​P(X∣θ)P(θ)​​ where P(θ)P(\theta)P(θ) is probability density function (pdf) of the prior probability. Conjugate prior In Bayesian probability theory, if the posterior distributions P(θ∣X)P(\theta \mid X)P(θ∣X) are in the same probability distribution family as the prior probability distribution P(θ)P(\theta)P(θ), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function. When a family of conjugate priors exists, choosing a prior from that family simplifies posterior form. An example of the conjugate prior: In the problem of coin flipping, Likelihood is ~ binomial distribution: P(X∣θ)=(nαH)θαH(1−θ)αTP(X \mid \theta) = \left( \begin{matrix} n \\ \alpha_H \end{matrix} \right) \theta^{\alpha_H} (1-\theta)^{\alpha_T} P(X∣θ)=(​n​α​H​​​​)θ​α​H​​​​(1−θ)​α​T​​​​ =n!αH!(n−αH)!θαH(1−θ)αT= \frac{n!}{\alpha_H! (n-\alpha_H)!} \theta^{\alpha_H} (1-\theta)^{\alpha_T} =​α​H​​!(n−α​H​​)!​​n!​​θ​α​H​​​​(1−θ)​α​T​​​​ It means in nnn independent flipping trial, the probability of occurring αH\alpha_Hα​H​​ heads belongs to binomial distribution, with the probability of head is θ\thetaθ for every flipping. Prior is assumed as Beta distribution: P(θ)=θβH−1(1−θ)βT−1B(βH,βT)∼Beta(βH,βT)P(\theta) = \frac{\theta^{\beta_H-1} (1-\theta)^{\beta_T-1}}{B(\beta_H,\beta_T)} \sim Beta(\beta_H,\beta_T) P(θ)=​B(β​H​​,β​T​​)​​θ​β​H​​−1​​(1−θ)​β​T​​−1​​​​∼Beta(β​H​​,β​T​​) where B(βH,βT)=Γ(βH)Γ(βT)Γ(βH+βT)B(\beta_H,\beta_T)=\frac{\Gamma(\beta_H) \Gamma(\beta_T)}{\Gamma(\beta_H+\beta_T)}B(β​H​​,β​T​​)=​Γ(β​H​​+β​T​​)​​Γ(β​H​​)Γ(β​T​​)​​, Γ(z)\Gamma(z)Γ(z) is the gamma function as an extension of the factorial function, Γ(z)=∫0∞xz−1e−xdx\Gamma(z) = \int_0^{\infty} x^{z-1}e^{-x} dxΓ(z)=∫​0​∞​​x​z−1​​e​−x​​dx, and Γ(n)=(n−1)!\Gamma(n) = (n-1)!Γ(n)=(n−1)!, if nnn is a positive integer. Posterior is also Beta distribution: P(θ∣X)∼Beta(βH+αH,βT+αT)P(\theta \mid X) \sim Beta(\beta_H + \alpha_H,\beta_T + \alpha_T) P(θ∣X)∼Beta(β​H​​+α​H​​,β​T​​+α​T​​) For binomial distribution, the conjugate prior is Beta distribution. The pdf of Beta distribution got more concentrated as values of βH\beta_Hβ​H​​ and βT\beta_Tβ​T​​ increased. As more samples are obtained, βH+αH&gt;&gt;βH\beta_H + \alpha_H &gt;&gt; \beta_Hβ​H​​+α​H​​&gt;&gt;β​H​​, the effect of prior will be “washed out”. Another example of the conjugate prior: In the problem of rolling a dice, there are k=6k=6k=6 outcomes. Likelihood is ~ multinomial (θ=θ1,θ2,...,θk\theta={\theta_1,\theta_2,...,\theta_k}θ=θ​1​​,θ​2​​,...,θ​k​​) P(X∣θ)=n!α1!α2!...αk!∏i=1kθiαiP(X \mid \theta) = \frac{n!}{\alpha_1!\alpha_2!...\alpha_k!} \prod_{i=1}^k \theta_i^{\alpha_i} P(X∣θ)=​α​1​​!α​2​​!...α​k​​!​​n!​​​i=1​∏​k​​θ​i​α​i​​​​ The multinomial distribution is a generalization of the binomial distribution. In nnn independent rolling test, the probability of occurring α=α1,α2,...,αk\alpha={\alpha_1,\alpha_2,...,\alpha_k}α=α​1​​,α​2​​,...,α​k​​ times of the dice number α=X1,X2,...,Xk\alpha={X_1,X_2,...,X_k}α=X​1​​,X​2​​,...,X​k​​ belongs to multinomial distribution, with the probability of XkX_kX​k​​ is θk\theta_kθ​k​​ for every rolling. The sum of θk\theta_kθ​k​​ is 1 and the sum of αk\alpha_kα​k​​ is nnn. Prior is assumed as Dirichlet distribution: P(θ)=∏i=1kθiβi−1B(β1,β2,...,βk)∼Dir(β1,β2,...,βk)P(\theta) = \frac{\prod_{i=1}^k \theta_i^{\beta_i -1}}{B(\beta_1,\beta_2,...,\beta_k)} \sim Dir(\beta_1,\beta_2,...,\beta_k) P(θ)=​B(β​1​​,β​2​​,...,β​k​​)​​∏​i=1​k​​θ​i​β​i​​−1​​​​∼Dir(β​1​​,β​2​​,...,β​k​​) Posterior is also Dirichlet distribution: P(θ∣X)∼Dir(β1+α1,β2+α2,...,βk+αk)P(\theta \mid X) \sim Dir(\beta_1+\alpha_1,\beta_2+\alpha_2,...,\beta_k+\alpha_k) P(θ∣X)∼Dir(β​1​​+α​1​​,β​2​​+α​2​​,...,β​k​​+α​k​​) For multinomial, the conjugate prior is Dirichlet distribution. MLE compared with MAP: maximum a posterior estimation MLE: to choose θ\thetaθ that maximizes the likelihood probability (the probability of observed data) P(X \mid \theta). MAP: to choose θ\thetaθ that maximizes the posterior probability P(\theta \mid X). θ^MAP=argmaxθP(θ∣X)\hat{\theta}_{MAP} = \arg \max \limits_{\theta} P(\theta \mid X) ​θ​^​​​MAP​​=arg​θ​max​​P(θ∣X) =argmaxθP(X∣θ)P(θ)= \arg \max \limits_{\theta} P(X \mid \theta)P(\theta) =arg​θ​max​​P(X∣θ)P(θ) In the problem of flipping trial, the maximum a posterior estimation of probability of head is Beta distribution: P(θ∣X)∼Beta(βH+αH,βT+αT)P(\theta \mid X) \sim Beta(\beta_H+\alpha_H,\beta_T+\alpha_T) P(θ∣X)∼Beta(β​H​​+α​H​​,β​T​​+α​T​​) MLE vs MAP: θ^MLE=αHαH+αT\hat{\theta}_{MLE} = \frac{\alpha_H}{\alpha_H+\alpha_T} ​θ​^​​​MLE​​=​α​H​​+α​T​​​​α​H​​​​ θ^MAP=αH+βH−1βH+αH+βT+αT−2\hat{\theta}_{MAP} = \frac{\alpha_H + \beta_H - 1}{\beta_H+\alpha_H+\beta_T+\alpha_T-2} ​θ​^​​​MAP​​=​β​H​​+α​H​​+β​T​​+α​T​​−2​​α​H​​+β​H​​−1​​ For small sample size, the prior is important. For infinite nnn, prior will be useless. Therefore, when βH=βT=1\beta_H=\beta_T=1β​H​​=β​T​​=1, MAP will be same as MLE. We use Gaussian distribution for a continuous variable xxx which is i.i.d. The central limit theorem (CLT) establishes that, in most situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution The pdf is N(μ,σ2)N(\mu,\sigma^2)N(μ,σ​2​​): P(x∣μ,σ)=1σ2πe−(x−μ)22σ2P(x \mid \mu,\sigma) = \frac{1}{\sigma \sqrt{2\pi}} e^{\frac{-(x-\mu)^2}{2\sigma^2}} P(x∣μ,σ)=​σ√​2π​​​​​1​​e​​2σ​2​​​​−(x−μ)​2​​​​​​ where μ\muμ is the mean and σ2\sigma^2σ​2​​ is the variance. MLE probability for Gaussian distribution: θ^MLE=argmaxθP(X∣θ)\hat{\theta}_{MLE} = \arg \max \limits_{\theta} P(X \mid \theta) ​θ​^​​​MLE​​=arg​θ​max​​P(X∣θ) =argmaxθ∏i=1nP(Xi∣θ)= \arg \max \limits_{\theta} \prod_{i=1}^n P(X_i \mid \theta) =arg​θ​max​​​i=1​∏​n​​P(X​i​​∣θ) MLE for Gaussian mean and variance: μMLE=1n∑i=1nxi\mu_{MLE} = \frac{1}{n} \sum_{i=1}^n x_i μ​MLE​​=​n​​1​​​i=1​∑​n​​x​i​​ σMLE2=1n∑i=1n(xi−μ^)2\sigma_{MLE}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \hat{\mu})^2 σ​MLE​2​​=​n​​1​​​i=1​∑​n​​(x​i​​−​μ​^​​)​2​​ Here we use average value to estimate μ\muμ, which is not a true parameter. The estimation of σ\sigmaσ is under-estimated thus MLE for the variance of a Gaussian is biased. Because the average of x1,x2,...,xn{x_1,x_2,...,x_n}x​1​​,x​2​​,...,x​n​​ is used as μ^\hat{\mu}​μ​^​​, the true mean is not equal to the average of samples. If we use the average of samples to estimate the average of total, there will be a bias. For a unbiased variance estimator: σunbiased2=1n−1∑i=1n(xi−μ^)2\sigma_{unbiased}^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \hat{\mu})^2 σ​unbiased​2​​=​n−1​​1​​​i=1​∑​n​​(x​i​​−​μ​^​​)​2​​ Because only n−1n-1n−1 xix_ix​i​​ is independent with the average of samples. The nthn^{th}n​th​​ xix_ix​i​​ can be obtained by the average and other n−1n-1n−1 values, which includes no additional information. If we use different samples, the mean is also different. The prior for mean is also Gaussian distribution: P(μ∣η,λ)=1λ2πe−(μ−η)22λ2P(\mu \mid \eta,\lambda) = \frac{1}{\lambda\sqrt{2\pi}}e^{\frac{-(\mu-\eta)^2}{2\lambda^2}} P(μ∣η,λ)=​λ√​2π​​​​​1​​e​​2λ​2​​​​−(μ−η)​2​​​​​​ The conjugate prior for mean is Gaussian distribution and for variance is Wishart distribution. Bayes classifier Bayes Optimal Classifier Bayes optimal classifier is a kind of ensemble learning. In Bayesian learning, the primary question is: What is the most probable hypothesis give data? If we have: X: Features Y: Target classes How to decide which class yyy is for a new sample with feature xxx? (x∈Xx\in Xx∈X and y∈Yy\in Yy∈Y) The Bayes rule is: P(Y=y∣X=x)=P(X=x∣Y=y)P(Y=y)P(X=x)P(Y=y \mid X=x) = \frac{P(X=x \mid Y=y)P(Y=y)}{P(X=x)} P(Y=y∣X=x)=​P(X=x)​​P(X=x∣Y=y)P(Y=y)​​ According to the Bayes decision rule, we need to minimize the conditional risk of making a wrong classification of a sample. Becasue R(y∣x)=1−P(y∣x)R(y \mid x)=1-P(y \mid x)R(y∣x)=1−P(y∣x), the Bayes optimal classifier can be written as: h∗(x)=argmaxY=yP(Y=y∣X=x)h^*(x) = \arg \max \limits_{Y=y} P(Y=y \mid X=x) h​∗​​(x)=arg​Y=y​max​​P(Y=y∣X=x) As the denominator P(X=x)P(X=x)P(X=x) is fixed for call cases, the maximum can be derived as: h∗(x)=argmaxY=yP(X=x∣Y=y)P(Y=y)h^*(x) = \arg \max \limits_{Y=y} P(X=x \mid Y=y)P(Y=y) h​∗​​(x)=arg​Y=y​max​​P(X=x∣Y=y)P(Y=y) P(X=x∣Y=y)P(X=x \mid Y=y)P(X=x∣Y=y) is the class conditional density (likelihood) P(Y=y)P(Y=y)P(Y=y) is the class prior A binary classification and the optimal Bayes decision boundary is shown as follows. Conditional independence Given Z, X is conditionally independent of Y means: P(X,Y∣Z)=P(X∣Z)P(Y∣Z)P(X,Y \mid Z) = P(X \mid Z)P(Y \mid Z) P(X,Y∣Z)=P(X∣Z)P(Y∣Z) e.g. If the alarm in your house is active (Z), your neighbor Mary will call the police (X) or another neighbor John will call the police (Y). It is assumed that they do not communicate for each other. In this case, Mary’s calling and John’s calling are conditionally independent. It also means whether Mary’s calling the police is irrelated to John’s calling: P(X∣Y,Z)=P(X∣Z)P(X \mid Y,Z) = P(X \mid Z) P(X∣Y,Z)=P(X∣Z) Note: Independence ⇎\nLeftrightarrow⇎ Conditionally Independence X is conditionally independent of Y with a given Z, does not mean that, X is independent of Y. e.g. It is unknown whether Mary’s calling is independent of John’s calling. They can call the police for other cases other than the alarm in your house. On the contrary, X is independent of Y, does not result to, X is conditionally independent of Y with a given Z. e.g. X: rolling a dice, the number is 3. Y: rolling a dice, the number is 2. Z: rolling a dice two times, the sum of the two numbers is 5. In this case, X is not conditionally independent of Y with the given Z. Naive Bayes (NB) Naive Bayes Optimal Classifier assumes that the data is conditionally independent on the given class, which makes the computation more feasible. P(X1,...,Xd∣Y)=∏i=1dP(Xi∣Y)P(X_1,...,X_d \mid Y) = \prod_{i=1}^d P(X_i \mid Y) P(X​1​​,...,X​d​​∣Y)=​i=1​∏​d​​P(X​i​​∣Y) where ddd is the number of features. For each discrete feature, the likelihood is P(Xi∣Y)P(X_i \mid Y)P(X​i​​∣Y). If the class prior is $P(Y), the decision rule of Naive Bayes can be: hNB∗(x)=argmaxyP(x1,x2,...,xd∣y)P(y)h^*_{NB}(x) = \arg \max \limits_{y} P(x_1,x_2,...,x_d \mid y)P(y) h​NB​∗​​(x)=arg​y​max​​P(x​1​​,x​2​​,...,x​d​​∣y)P(y) =argmaxy∏i=1dP(xi∣y)P(y)= \arg \max \limits_{y} \prod_{i=1}^d P(x_i \mid y)P(y) =arg​y​max​​​i=1​∏​d​​P(x​i​​∣y)P(y) Actually, the features are not conditionally independent, then P(X1,...,Xd∣Y)≠∏i=1dP(Xi∣Y)P(X_1,...,X_d \mid Y) \neq \prod_{i=1}^d P(X_i \mid Y) P(X​1​​,...,X​d​​∣Y)≠​i=1​∏​d​​P(X​i​​∣Y) Nonetheless, Naive Bayes sometimes still performs well even when the assumption of conditionally independence is violated. If the training data is insufficient, e.g., there is no sample for a feature X1X_1X​1​​ in the class YYY, the likelihood will be zero. No matter what the values X2,...,XdX_2,...,X_dX​2​​,...,X​d​​ take, the probability keeps zero as: P(X1,...,Xd∣Y)=0P(X_1,...,X_d \mid Y) = 0 P(X​1​​,...,X​d​​∣Y)=0 If the features are continuous, we use Gaussian Naive Bayes (GNB) instead. Let Xi=xX_i = xX​i​​=x and Y=ykY=y_kY=y​k​​. The probability is: P(x∣yk)=1σik2πe−(x−μik)22σik2P(x \mid y_k) = \frac{1}{\sigma_{ik}\sqrt{2\pi}} e^{\frac{-(x-\mu_{ik})^2}{2\sigma_{ik}^2}} P(x∣y​k​​)=​σ​ik​​√​2π​​​​​1​​e​​2σ​ik​2​​​​−(x−μ​ik​​)​2​​​​​​ The subscript indicates the mean and variance for each class kkk and each feature iii are different. Reference [1] Some common probability distributions [2] Why use n−1n-1n−1 for unbiased variance estimation [3] Example of NB: Sex classification based on features including height, weight, and foot size]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
        <tag>Bayesian Learning</tag>
        <tag>Maximum Likelihood Estimation</tag>
        <tag>Bayes Optimal Classifier</tag>
        <tag>Naive Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉菲的故事]]></title>
    <url>%2Fblog%2F2018%2F04%2F18%2FDD-Laffey%2F</url>
    <content type="text"><![CDATA[引言 如果你喜欢葡萄酒，一定对拉菲很熟悉。这个法语中的拉菲Lafite，因为赌神的那句“来一瓶82年的拉菲”的著名台词梗火起来。回到1982年夏季的波尔多，气候炎热却不干燥，葡萄成熟较早，等到收成时均是干燥的晴天。在这种气候下收获的葡萄，品质十分完美。Robert M. Parker给出1982年的拉菲古堡干红100分的满分评价，果香浓郁，单宁柔和。2014年纽约春季拍卖会上，一箱82年的拉菲拍出了41,650美元的天价。 不过今天不讲葡萄酒，本文讲的是二战期间美国海军的故事。 拉菲，英文名Laffey。美国海军历史上有两艘船都叫拉菲。 初代拉菲 第一艘拉菲，是美国海军Benson级驱逐舰七番舰，舷号DD-459。以一名美国水手巴特利特·拉菲（1841–1901）的名字命名，文末有他的故事简介。 1940年6月14日，法国巴黎沦陷。 1940年7月19日，美国众议院以316：0全票通过通过了两洋舰队法案，以应对德国海军的威胁。 1941年1月13日，拉菲号驱逐舰开工建造，地点是旧金山的伯利恒钢铁厂。 1941年10月31日，举行下水仪式。 伯利恒钢铁Bethlehem Steel Corporation的子公司，伯利恒船舶工业公司，曾是美国最大的造船企业，总部位于宾夕法尼亚州的Bethlehem。这家成立于1857年的老牌企业，曾是美国第二大钢铁生产公司，于2003年倒闭，被出售给美国国际钢铁集团。后者被印度的米塔尔钢铁公司并购。下图是拉菲下水时的旧照，屁股上的459和舰名Laffey清晰可见，注意此时烟囱和火炮都尚未安装。密密麻麻的钢铁森林与薄雾海色，可一窥当年的工业时代。 1941年12月7日，日本偷袭珍珠港，太平洋战争正式爆发。 1942年3月31日，拉菲服役，舰长为威廉·E·汉克。海试后启程奔赴太平洋战场。 1942年6月4日，中途岛海战爆发，美国海军大获全胜。 1942年8月7日，瓜达尔卡纳尔岛战役爆发。 1942年8月28日，拉菲抵达瓦努阿图的埃法特岛，执行反潜作战任务。 1942年9月15日，第18特遣舰队航行至马基拉岛东南，胡蜂号航空母舰被日本伊19潜艇击沉，拉菲参与救援并安全撤离。 1942年10月11日，埃斯佩兰斯海角海战爆发。拉菲击破日军旗舰青叶，并配合美军旗舰旧金山击沉日军驱逐舰初雪。 1942年11月13日，瓜达尔卡纳尔岛海战第五次战役（日方称第三次所罗门海战）中，拉菲重创比叡，后寡不敌众沉没。 拉菲不到9个月的一生就像流星般短暂，共获得三枚战斗之星。在那个疯狂的年代，你我都是前仆后继，朝生暮死。 所罗门的战神 1942年11月11日，拉菲号加入第67特遣舰队水面打击群，执行护送运输船队的任务。13日，美军前往萨沃岛海域，拉菲担任舰队前卫。当天夜里，一轮新月无力地挂在海空，能见度极低，随后爆发了激烈而混乱的海战，双方船只难分敌我，纠缠在一起，事后蒙森号上的一名军官称之为“在酒吧里把灯打掉后的斗殴”。 拉菲率先攻击了彼时正拿着探照灯乱晃的日军晓号驱逐舰，导致其动力丧失，迅速被拉菲、奥班农、亚特兰大、旧金山、波特兰和海伦娜乱炮击沉。美军亚特兰大号巡洋舰被探照灯照亮，遭到了日军的集中火力攻击而大破，失去了动力随波逐流。 混乱中，拉菲突然发现左舷有个开着探照灯的巨大黑影向自己高速冲过来，正是日军旗舰，金刚级战列舰二番舰比叡，满载排水量32,000吨的怪物，而拉菲只有区区1,800吨。由于日军指挥不得其法，舰队阵型大乱，本应被护航的旗舰比叡反而冲到了最前面。拉菲立即规避并发射五枚鱼雷，其中两枚击中，不过因为引信过近未引爆。拉菲与比叡最近时仅相距10～20英尺，也就是差不多3到6米的距离，以至于比叡号的舰炮由于俯角不足竟然无法瞄准拉菲。拉菲抓住骑脸的机会，对准比叡的舰桥倾泻了所有的炮弹，连机枪也用上来进行炮击，摧毁了比叡的舰桥和指挥所，日军首席参谋铃木正金中佐获得成就：当场阵亡。比叡舰长西田正雄大佐和第11舰队司令阿部弘毅中将均身受重伤。比叡号既然无法射击驱逐舰，转而向仅仅2,300米远处的旧金山号射击。卡拉汉海军少将阵亡。 血洗比叡舰桥后，拉菲准备撤离。不过此时已经来不及，日军后续舰船已经跟上。拉菲右边是比叡的小妹雾岛：金刚级战列舰四番舰，左前方是日本驱逐舰小分队：村雨、五月雨、朝云，远处有巡洋舰长良虎视眈眈。混战中，拉菲首先摧毁了村雨的锅炉，逼村雨退出了战斗。可惜寡不敌众，拉菲受到集火，2～4号主炮炮塔相继被摧毁，但仍坚持用仅剩的1号炮塔英勇还击。距离渐渐拉开，比叡的主炮一轮齐射，一枚14英寸炮弹命中拉菲舰桥，拉菲的各仓室也纷纷中弹被毁。1时52分许，一枚（可能来自驱逐舰照月的）九三式鱼雷击中拉菲号左舷舰尾，剧烈的爆炸使其断成两截，随后弹药库发生殉爆，汉克舰长丧生，拉菲在烈火熊熊中沉没海底。拉菲号全舰247人中59人阵亡、116人受伤，幸存者乘救生挺登陆瓜岛获救。 这场混乱的夜战中，美军损失惨重。旗舰亚特兰大号轻巡洋舰被击沉，诺曼·斯科特少将战死，一共损失了两艘巡洋舰和四艘驱逐舰。日方损失了一艘重巡洋舰和两艘驱逐舰。不过美军的战略意图得以实现，挫败了日军当晚炮轰亨德森机场的企图，为瓜岛登陆部队争取了宝贵的一天时间。此后，由于拉菲无畏的进攻使得比叡号丧失了射击能力，以及一众日军高级参谋被消灭，日军只好放弃作战任务，撤出战区。13日上午4时20分，驱逐舰雪风到达战场…如果不清楚这意味这什么，请看上文雪风的故事。后面战事本文不再详述。日本舰队的结局是：13日天亮以后，在美军不断空袭中，比叡这艘天皇御昭舰被迫自沉。15日凌晨，雾岛号在与华盛顿号和南达科他号的交火中严重受损而沉没。可以说，拉菲在此战役中功不可没。 拉菲的英雄事迹极大地振奋了美国海军和民众的士气。各大报刊争相报道以鼓舞民心，南太平洋战区总司令哈尔西上将也致信嘉奖。威廉·E·汉克少校因其非凡勇气，被追授海军十字勋章加佩金星章。瓜达尔卡纳尔海战胜利后，罗斯福将美国总统集体嘉奖授予拉菲号的全体船员。一起欣赏一下嘉奖令全文： 美利坚合众国总统荣幸地将总统集体嘉奖授予 美国军舰拉菲号（DD-459） 以表彰下文所述事迹：兹嘉奖拉菲号驱逐舰在1942年9月 15日至11月13日于南太平洋战场中对抗日军的杰出表现。 她不顾附近仍有敌潜艇出没，毅然对落水船员伸出援手； 在埃斯佩兰斯海角海战中崭露头角后，她又成功击退敌机 空袭；在萨沃岛的决战中，尽管已遍体鳞伤且燃起烈火， 拉菲号仍对进犯之敌予以迎头痛击。在敌人仓皇撤退后， 不幸伤重沉没。她为我们树立了一个英勇不屈的光辉榜样。 为了纪念拉菲号和她的汉克舰长在瓜达尔卡纳尔海战中的无畏表现，1943年新建的两艘艾伦·萨姆纳级驱逐舰分别获名拉菲号 (DD-724)和汉克号 (DD-702)。DD-724就是下文要讲的二代拉菲的故事。 二代拉菲 二代拉菲终于有了一张合格的彩色照片。她坚持到了二战的胜利。现在还在美国南卡罗来纳州Mount Pleasant的爱国者地海军海事博物馆Patriots Point Naval &amp; Maritime Museum。背后的大屁股是USS Yorktown，约克城号航空母舰（CV-10）。这是第二艘名为约克城的航空母舰。为纪念在中途岛海战中沉没的约克城号航空母舰（CV-5），中途岛海战四个月后，美国把一艘正在建造的埃塞克斯级航母更名为约克城，这艘新约克城见证了二战的胜利。初代约克城和初代拉菲均在二战中英勇牺牲，在未来漫长的岁月里，二代拉菲将继续为二代约克城护航。 1943年6月28日，二代拉菲号驱逐舰开工建造，地点是缅因州的巴斯钢铁厂。 1943年11月21日，举行下水仪式。建造时间不到五个月。 1944年2月8日，正式服役。 巴斯钢铁厂简称BIW，于1844年由托马斯·W·海德建立，1995年被通用动力公司收购。巴斯钢铁被誉为美国海面战舰的摇篮，包括二战中著名的著名的弗莱彻级驱逐舰，治亚号战列舰。二战后，美军现役主力巡洋舰提康德罗加级，主力驱逐舰阿利·伯克级，以及下一代的主力：相当科幻的朱姆沃尔特级驱逐舰DDG-1000，都出生于此。 1944年6月6日，二代拉菲来到了英吉利海峡参加了著名的诺曼底登陆，炮击了德军岸防部队。行动中她的运气不错，仅挨了一发炮弹还是哑弹。 1945年4月12日，罗斯福总统去世。 1945年4月16日，二代拉菲遭神风特攻队自杀式飞机袭击。 二代拉菲是一艘幸运舰，有个小名叫The Ship That Would Not Die。当年一共有22架自杀飞机围攻，炸毁了舰上的火炮，并引起了弹药库的爆炸，舵机也遭损坏，舰尾进水下沉，死亡32人，71人负伤。二代拉菲的反击干掉了9架。据说当时的通讯官法兰克曼森少尉询问贝克顿舰长是否弃船, 贝克顿骂回去，“去你妈的，只要还有一门炮能开火老子就绝不会弃船（No! I’ll never abandon ship as long as a single gun will fire）”。但他几乎没有听见旁边有人轻声抗议，“除非我还能找到一个人来开火（And if I can find one man to fire it）”。最终由于美军出色的损管，二代拉菲奇迹般地没有沉没，经过修复，1945年10月继续服役。 1952年1月中旬，二代拉菲起航前往韩国，支援朝鲜战争，参与了长达861的元山封锁（Blockade of Wonsan）。 1975年3月9日，二代拉菲终于可以休息了，正式退役除籍。 二代拉菲在二战中获得了五枚战斗之星。不可思议的是，二代拉菲同样获得了总统集体嘉奖，创造了历史。 彩蛋 洛阳和汉阳 上文提到的初代拉菲属于美国海军Benson级驱逐舰，在1940-1943年期间美国一共建造了32艘Benson级驱逐舰。首舰Benson号，本森or班森，舷号DD-421，战斗到了二战的胜利。1946年退役并转为后备舰艇。五番舰Hilary P. Jones号，即希拉里·P·琼斯号，舷号DD-427。在一八五号公共法案下作为共同防卫援助方案的一部分，1954年这两艘驱逐舰被移交（借）给中华民国海军，并分别改名为洛阳号和汉阳号驱逐舰。1958年8月洛阳舰担任蒋介石座舰，巡视过金门及马祖。汉阳舰在服役过程表现优异，参与各项演训屡获第一，也经常担任总统座舰与高阶长官专送任务。 巴特利特·拉菲的故事 Bartlett Laffey参加了1861-1865年间的南北战争。那个时候，内河的船还是以船尾明轮当作推进，拉菲正是在一艘这样的炮艇USS Marmora上服役。1864年3月5日，南军对密西西比州的亚祖城发动猛烈袭击，马莫拉号炮艇在亚祖河口进行防御。战役当中，水手拉菲带了一队士兵登上河岸支援战斗，包括一门M1841式榴弹炮。敌军猛烈的火力摧毁了运输用的炮架，也破坏了它的装填装置。此时拉菲守岗位毫不退缩，守护了这门榴弹炮，并坚持反击，直到击退了南方联军的进攻。拉菲及其他两名水手因其勇敢的表现而被授予荣誉勋章（Medal of Honor）。荣誉勋章是由美国政府颁发的最高军事荣衔，授予那些“在战斗中冒生命危险，在义务之外表现出英勇无畏”的军人。历史也已证明，两艘拉菲都继承了他英勇无畏的精神，不辱其名。 所罗门的疯狗 瓜岛战役期间，日本守军资源奇缺，而运输船速度又慢又脆弱，所以日本海军常在夜间趁美军无法有效使用轰炸机的时候，以高速驱逐舰向瓜岛输送兵力及物资。日军自嘲这些专跑夜路的行动是老鼠输送，而美方称之为东京快车。日军总共派出350船次的驱逐舰，总共运输了约2万人以上。夕立是日本海军白露级驱逐舰四番舰，成功执行了多次惊心动魄的东京快车行动。夕立以大胆出名，曾经在1942年9月4日，夕立仅带领两艘驱逐舰突袭了美军舰队，击沉两艘美军运输舰，并炮击了亨德森机场。联合舰队参谋长宇垣缠在著作中将吉川舰长形容为“伟勋之士”，村雨的船员在回忆录中更是将夕立形容为“非常识”的“无法模仿”的船。 时间来到1942年11月12日的第三次所罗门海战，按照日本的说法，开战时夕立和春雨疯狂突进，导致整个美军舰队大乱。春雨发射鱼雷脱离战场之后，夕立独自冲进美军舰队，和美军船只混在一起。两舷火力全开，造成美军巨大伤亡，多艘船只燃起大火。由于寡不敌众，夕立的火力逐渐被打掉。疯狂的吉川舰长甚至命令船员“张帆”，用床单拼凑成的风帆向美军撞去。事后的夕立被日本人赞誉为阿修罗，所罗门的噩梦等，其战功被特记在第四水雷战队的战功详报功绩栏中。然而美军并没有记录到如此神勇的夕立，只记录到没头没脑的夕立冲过来，与日本舰队分隔开来，立即被美军炮火攻击。夕立升起表示投降的白旗，但随后却又再次向美军方开火，愤怒的波特兰立刻将其击沉。所以获得外号所罗门的疯狗。历史学家更倾向于美军的说法，认为是日本的造神运动以掩盖此次海战的失利和激励军队的士气。至于真相，只有永恒沉默的铁底湾才知道吧。 所罗门的鬼神 不像夕立的战果有水分，另一艘日本驱逐舰绫波号，则是实打实的表现神勇，战功卓著。绫波，熟悉的Ayanami，是吹雪型特型驱逐舰的第11艘，1930年4月30日服役。同样在第三次所罗门海战期间，在队友脱离编队前往支援其他舰队之后，绫波独自面对了美军2艘战列舰、4艘驱逐舰。结果是绫波击沉2艘驱逐舰，大破1艘驱逐舰，并导致一艘战列舰（南达科他）的电力中断。因此，在日军中绫波得到了黑豹、鬼神的称呼。随后绫波也被美军舰队集火攻击，发生爆炸。因为弹药库随时会殉爆，舰长下达全员退舰的命令，船员全部跳进海中，在漂流期间合唱军歌。大部分生还者被赶来的浦波救起。后因弹药库鱼雷爆炸后绫波逐渐沉入海底。 拉菲以及所罗门海战的故事就讲到这里了。古今多少事，都付笑谈中。 参考资料 [1] Wikipedia - USS Marmora [2] Wikipedia - Bartlett Laffey [3] Wikipedia - 拉菲号驱逐舰 [4] 碧蓝航线:拉菲 [5] 战舰少女:拉菲(DD-724) [6] 初代拉菲的美国总统集体嘉奖全文 [7] 二代拉菲的美国总统集体嘉奖全文 [8] 二代拉菲的推特主页]]></content>
      <categories>
        <category>故事</category>
      </categories>
      <tags>
        <tag>Navy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Geometry and performance of a propeller]]></title>
    <url>%2Fblog%2F2018%2F04%2F06%2Fpropeller-concepts%2F</url>
    <content type="text"><![CDATA[Introduction A propeller in propulsion system can transmit power by converting rotational motion into thrust, based on Bernoulli’s principle. The most common propeller is called the screw propeller, with fixed helical blades rotating around a shaft. The first screw propeller in the world was invented in 1835, by John Ericsson and Francis Pettit Smith. Propellers can be used in marine ships, aircrafts, etc. The basic concepts and priciples in marine propulsion are introduced in this blog. Geometry Hub: The solid center disk that mates with the propeller shaft and to which the blades are attached. A smaller hub can lead to a larger thrust, however there is a tradeoff between size and strength. Blade: Twisted fins or foils that protrude from the propeller hub. The shape and the speed at which they are driven dictates the torque a given propeller can deliver. Higher diameter equates to higher efficiency for low speed vehicles (&lt;35kn). To obtain higher torque, the rpm (revolution per minute) should be reduced and the diameter should be increased. In high speed vessels, larger diameters lead to high drag. ZZZ is used to represent the number of blades. Here are basic nomenclatures to describe a blade section: Leading edge: the point at the front of the airfoil that has the maximum curvature. Trailing edge: the point of maximum curvature at the rear of the airfoil. Meanline (camber): half distance along a section between the upper and lower surfaces. Quite often the meanline distribution is tabulated forms such as a NACA a=0.8 meanline, where a=0.8 means the meanline can create constant lift of 80% of the chord, then the lift drops linearly to zero at 100%. Chord ©: the nose-tail line, connecting the leading edge and trailing edge. Camber height (f): the distance between nose-tail line and meanline normal to chord. Thickness (t): the section thickness along a line normal to the meanline based on American convention. Thickness measured normal to the chord line is based on British convention. Angle of attack (AOA): the angle between a reference line on a body (often the chord line of an airfoil) and the vector representing the relative motion between the body and the fluid through which it is moving. The chord line of the root or the zero lift axis is often chosen as the reference line. Note that top pressure on the blade section is lower than the bottom pressure. The difference of the pressure leads to the lift force on the blade, thus thrust is generated. Face: the pressure face, high-pressure side, faces backwards and pushes the water. Back: the suction face, low-pressure side, faces upstream and towards the front of the vessel. Leading edge: the side cuts through the fluid. Trailing edge: the edge of the downstreams. Pitch: the axial distance advanced during one complete rotation of screw is called nominal pitch. The distance the ship is propelled forward in one propeller rotation is actually less than the pitch. The trace of the tip points on the blade is a helix. Pitch ratio is the ratio of pitch to diameter. Slip: The difference between the nominal pitch and the actual distance in one retation (s=pnt−VAts = pnt - V_Ats=pnt−V​A​​t). Propeller section: A circular arc section cut through the blade at some radius. We can expand it to 2-D foil section. Midchord line: the line produced from the midpoint of section nose tail line of each section along a blade. Rake: Axial distance from the midchord point at the hub section and the section of interest. skew angle: the angle between a radial line going through the hub section midchord point and a radial line through the midchord point of the section of interest AND projected. Performance Speed of advance: the propeller advances through the water at a speed of advance VAV_AV​A​​, which delivers a thrust TTT. When the speed of advance is zero, the efficiency is also zero, but the propeller still delivers thrust and absorbs power. Thrust power: PT=TVAP_T = T V_A P​T​​=TV​A​​ Effective power: PE=RVP_E = R V P​E​​=RV Wake: In general the water around the stern has acquired a forward motion in the same direction as the ship. This forward-moving water is called wake. The difference between the ship speed V and the speed of advance is the wake speed. The wake fraction is defined as: w=V−VAVw = \frac{V-V_A}{V} w=​V​​V−V​A​​​​ Wake is due to three principal causes: The frictional drag of the hull causes a following current towards the stern. The streamline flow past the hull causes an increased pressure around the stern. In this region the relative velocity of the water past the hull will be less than the ship’s speed. The ship forms a wave pattern on the surface of the water, and the water particles in the crests have a forward velocity due to the orbital motion, while in the troughs the orbital velocity is sternward. This wake will be positive or negative according to whether there is a crest or a trough of the wave in the vicinity of the propeller. Thrust deduction: The propeller close to the hull can induce a low pressure on the hull which increases its drag. The trust must be higher to overcome the additional drag. The thrust deduction coefficient is defined as: t=T−RRt = \frac{T-R}{R} t=​R​​T−R​​ where R is the total ship resistance and T is the propeller thrust. Here are the non-dimensional characterization of the propeller performance. Advance coefficient: J=VAnDJ = \frac{V_A}{nD} J=​nD​​V​A​​​​ Thrust coefficient: KT=T/(ρn2D4)K_T = T/(\rho n^2D^4) K​T​​=T/(ρn​2​​D​4​​) Torque coefficient: KQ=Q/(ρn2D5)K_Q = Q/(\rho n^2D^5) K​Q​​=Q/(ρn​2​​D​5​​) Propeller efficiency: η0=TVA2πnQ\eta_0 = \frac{T V_A}{2\pi nQ} η​0​​=​2πnQ​​TV​A​​​​ Propulsive efficiency: ηt=RV2πnQ≈1−t1−wη0\eta_t = \frac{R V}{2\pi nQ} \approx \frac{1-t}{1-w} \eta_0 η​t​​=​2πnQ​​RV​​≈​1−w​​1−t​​η​0​​ Because of the wake, the propulsive efficiency for a propeller can be greater than 1.0. It means the propeller can reduce the ship resistance by taking advantage of its wake. When the local absolute pressure is less than local vapor pressure, cavitation occurs, generally on the suction side. The water can boil in a lower pressure (than atomstpheric pressure) condition even though the temperature is lower than 100∘C100^{\circ} C100​∘​​C. Cavitation number based on inflow velocity: σv=P−Pvap0.5ρAVA2\sigma_v = \frac{P - P_{vap}}{0.5\rho A V_A^2} σ​v​​=​0.5ρAV​A​2​​​​P−P​vap​​​​ Cavitation number based on propeller tip velocity: σND=P−PvapρN2D2\sigma_{ND} = \frac{P - P_{vap}}{\rho N^2 D^2} σ​ND​​=​ρN​2​​D​2​​​​P−P​vap​​​​ Cavitation causes a great deal of noise, damage to components, vibrations, and a loss of efficiency. The pitting caused by the collapse of cavities produces great wear on components. Highly localized collapses of cavities can erode metals over time. Reference [1] Wikipedia - Propeller [2] Wikipedia - Cavitation [3] MIT 2.016 Hydrodynamics [4] An introduction to propeller cavitation [5] Principles of Naval Architecture Volume II: Resistance, Propulsion and Vibration]]></content>
      <categories>
        <category>曹衣出水</category>
      </categories>
      <tags>
        <tag>Marine Hydrodynamics</tag>
        <tag>Propulsion</tag>
        <tag>Principle of Naval Architecture</tag>
        <tag>Propeller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[雪风的故事]]></title>
    <url>%2Fblog%2F2018%2F03%2F25%2FDD-Yukikaze%2F</url>
    <content type="text"><![CDATA[引言 雪风是旧日本帝国在二战期间建造的一艘驱逐舰。她的一生充满传奇，作为战败国的一艘普通的驱逐舰，历经太平洋上大大小小超过十六次的海战而没有沉没，几无受损，被称作“奇跡の駆逐艦”、“呉の雪風 佐世保の時雨”、“不沈の航迹”等。关于雪风的文献很多：《激動の昭和 : 世界奇跡の駆逐艦 : 雪風》、《强运駆逐舰栄光の生涯》、《駆逐艦雪風―誇り高き不沈艦の生涯》等。 本文要讲的雪风的故事，从1938年开始，到1971年结束。 雪风的诅咒 雪风的诞生 1938年8月2日，第8艘阳炎型驱逐舰开工建造，地点是佐世保海军工厂。 1939年3月24日，举行下水及命名仪式。 1940年1月20日，雪风竣工并服役，母港为吴港。 佐世保海军工厂是日本重要的海军造船厂，赤城加贺两艘王牌航母在这里接受改装。明治22年（1889）佐世保镇守府设立，同时设有佐世保造船部。明治36年（1903）改为佐世保海军工厂。昭和36年（1961）更名为佐世保重工业株式会社佐世保造船所。佐世保港位于长崎县，其母港位于广岛县。是不是很耳熟？这两个县都是当时日本的重工业基地，战争末期遭原子弹轰炸。 雪风的诅咒有二，一是自己永不沉没，二是队友非沉即破。海军间流传雪风会将周围舰船的运气都吸光。也就是从那个遥远的昭和15年1月开始，雪风开始了它精彩而又祥瑞的一生。 雪风和它的朋友们 1941年12月12日，雪风初登场，为菲律宾的登陆作战提供火力支援。 1941年12月24日，雪风前往拉蒙湾，支援登陆作战。此战雪风轻微受损，接受维修舰明石的维修。 1942年1月4日，旗舰妙高号重巡洋舰带着第五战队的小学生们在达沃休息，昼ご饭を食べる，遭到美军B-17轰炸机轰炸。32枚600磅的航空炸弹从25000英尺的高空落下，本不指望能打中什么，谁能想到雪风当时正在旁边补充油料…这些瞎投的炸弹当中有一枚刚好钻进了妙高身体爆炸，妙高严重受损，三座前炮塔损毁，35名水兵和军官死亡。雪风自己由于指挥得当躲避所有的炸弹。 妙高：MMP... 雪风：？ 1942年3月3日，泗水北方海域，雪风击沉了美军白桦号潜艇。 白桦：出名了出名了！ 雪风：First Blood! 1942年6月4日，中途岛海战爆发。雪风作为运输船队的护卫并预备从事防空战。中途岛海战详情参考此链接。日本海军四艘主力航母、248架飞机、1艘重巡洋舰沉没，3000人以上战死。雪风及时撤退完好无损。 赤城：露落露消我太阁， 加贺：浪花之梦梦还多。 雪风：？ 1942年8月8日，第一次所罗门海战（萨沃岛海战）爆发，也是瓜达尔卡纳尔岛战役的开端。雪风执行运输与护送任务。 1942年10月26日，平静的海色下暗流汹涌。圣克鲁斯群岛战役爆发，雪风执行护卫任务，舰队旗舰翔鹤大破，舰体严重受损，退出一线战场长达一年。 翔鹤：MMP... 雪风：？ 1942年11月12日夜晚，第三次所罗门海战（瓜达尔卡纳尔岛海战）爆发，海况恶劣，能见度极低，雪风遭到友军误射。舰队旗舰比叡（金刚级战列舰）开着九座探照灯就冲进美军舰队并被围殴。雪风的九妹驱逐舰天津风大破，比叡严重受损并自沉，司令官阿部弘毅转乘雪风。14日同型舰雾岛被击沉。自此四艘金刚舰同时沉了俩。12月28日，天皇批准日军从瓜岛撤退。 比叡：四十九年一睡梦， 雾岛：荣花一期酒一盅。 天津风：MMP... 雪风：？ 1943年2月1日开始执行ケ号作战（瓜达尔卡纳尔岛撤退）。每次海战均有战舰沉没或严重受损，例如，驱逐舰卷云沉没，舞风沉没，卷风大破，矶风大破。雪风毫发无损。 卷云：时之有限花吹散， 舞风：此心归于春山风。 卷风：MMP... 矶风：MMP... 雪风：？ 1943年3月2日，俾斯麦海海战爆发，日本海军被澳大利亚空军和美国空军袭击并重创，运输船队包括旭盛丸、建武丸、爱洋丸、神爱丸、太明丸、帝洋丸、大井川丸、野岛全军覆没。驱逐舰白雪、荒潮、朝潮、时津风被击沉，3000至5000名人员战死，损失物资2500吨。其中，被击沉的时津风也是雪风的姐妹舰，雪风依然毫发无损，顺便捞起了时津风的船员。 各种丸：人生五十年， 时津风：如梦亦如幻。 白雪： 有生斯有死， 二潮： 壮士何所憾。 雪风：？ 1943年7月6日至9日，改造后的雪风做了第八舰队（外南洋部队）的旗舰，驱逐舰做舰队旗舰可能是前无古人后无来者。 1943年7月12日，科隆班加拉岛海战爆发。夜战中雪风作战神勇，在克死旗舰神通（以及战队司令伊崎俊二）后击破了美军3艘巡洋舰。雪风也不幸被击中，然而炮弹竟然未爆炸。此战役以美军作战失败告终，但日军同样损失惨重，所谓的最精锐战队華の二水戦也如昙花一现。 神通：仅与金刚寺菩萨种的青松作一别。 雪风：？ 1943年7月20日，参与向科隆班加拉岛的输送作战。满月的夜间，船队受到美军轰炸，驱逐舰夕暮、清波被击沉，重巡洋舰熊野损毁严重。有流言认为夕暮成为了雪风的替死鬼。 夕暮：生于天地之清澈， 清波：归于本愿之清澄。 熊野：MMP... 雪风：？ 1944年1月10日起，雪风和姐妹舰天津风以及轻型航母千岁执行护卫运输船队任务。1月16日天津风被美军潜艇击中断成两截，司令战死，船头包括舰桥部分沉没，船尾在海上漂了一周被拖回越南，安装新船头后得以苟活。 1944年2月，由于时津风被击沉，初风被妙高撞沉，天津风也在大修，第十六驱逐队只剩雪风一艘了。雪风此时被编入第十七驱逐队，然而并不受大家欢迎，因为有流言称“雪風が十六駆で僚艦を全部食い尽くした”。 1944年5月14日，驱逐舰“电”被美军潜艇击中，雪风前往救援，电此时已完全沉没。5月18日，雪风触礁导致螺旋桨损坏。6月9日，谷风被美军潜艇击沉。第十七驱逐队从突袭珍珠港开始至此都没有战损，然而雪风加入之后，谷风成为第一艘被击沉的队员。此后雪风倍遭排挤。 谷风：吾心如那吹散云雾见明月的秋之晚风。 雪风：？ 1944年6月19日，史上最大的航空母舰对决，马里亚纳海战（菲律宾海海战）爆发。由于力量对比悬殊，当天的空战被美军戏称为“马里亚纳射火鸡大赛”。雪风没有受损，反而使用探照灯击落3架美军飞机。五航战的骄傲，翔鹤被击沉，傍晚时分大凤也追随翔鹤而去。 翔鹤：梅雨如露亦如泪， 大凤：杜鹃载吾名至云。 美军飞机：瞎了MMP... 雪风：令人窒息的操作 1944年10月20日，史上最大的海战，莱特湾海战爆发，双方舰船总吨位达到了206万吨。宇宙第一战列舰大和的姐妹舰武藏被击沉。雪风曾护航过的扶桑和山城先后被击沉。第二舰队司令栗田健男的旗舰爱宕号重巡洋舰被击沉。重巡洋舰摩耶被击沉。妙高、高雄、长门、金刚和榛名受重创，其中金刚与浦风在返航日本途中被美军潜艇击沉。战列舰大和接替旗舰位置。10月25日，“幸运の空母”瑞鹤也走到了终点，被美军飞机击沉。除了这些大船，还有12艘驱逐舰被击沉。雪风本计划救援重巡洋舰筑摩，结果大和发错了命令让雪风归队，让野分去救援。驱逐舰野分执行完救援任务后被美军击沉。 瑞鹤：迩来忧患集一身， 武藏：铁胄身躯今始破。 野分：我比窦娥还冤... 雪风：？ 1944年11月24日，雪风护卫长门返回横须贺港，然后执行新航母信浓的护航任务。信浓原本是大和级战列舰的三号舰，后被改为航空母舰。11月29日，信浓海试，刚出港口就被美军潜艇Archerfish发现并跟踪。护航的雪风误判其为渔船，导致信浓被美军潜艇击沉。雪风继续执行打捞工作，甲板上坐满信浓的船员安全返航。 信浓：吾身如筑摩江芦间点点灯火随之消逝。 雪风：？ 1945年4月6日，日本海军已经消耗殆尽，宇宙第一战列舰大和领衔的冲绳水上特攻作战开始，雪风和时雨为旗舰大和护航，结局可想而知。由于资源奇缺，大和出航的燃料来自于其他船舰油罐底部的剩油。4月7日，大和被400架飞机围殴后沉没。雪风的姐妹舰滨风被击沉。雪风也被击中，神奇的是炸弹没爆炸，鱼雷穿底而过。几乎无损的雪风又顺便捞起了大和的幸存者，再顺手送沉了严重损毁已不能行动的驱逐舰矶风。第十七驱逐队也被雪风吃掉只剩雪风一艘了。 大和：春樱秋枫留不住， 滨风：人去关卡亦成空。 雪风：？ 1945年8月15日，日本宣布投降，全体联合舰队的82艘驱逐舰只剩雪风。在整个太平洋战争中，雪风只有不到10名船员死亡，2 人失踪，四任舰长均得以善终。8月18日，雪风护送潜水母舰长鲸回到舞鹤港，途中意外触发水雷。屈服于雪风殿下的淫威，这颗水雷在雪风完全通过后才爆炸。8月26日雪风改为第一预备舰，解除武装后，9月15日改为特别输送舰并引渡给美军。 雪风的尾声 1945年10月5日，不沉战舰雪风退役。阳炎型姐妹舰一共18艘，只有雪风活到了战后。然而雪风的传奇还没结束。 1946年12月30日，雪风成为赔偿舰引渡至联合国。雪风的乘组员们直到最后时刻仍细心地整备。联合国方面感叹曰“战败国的军舰仍不理后事，细心地整备及保养舰只，真是令人惊叹”。 1947年7月3日雪风到达上海高昌庙码头，7月6日移交至中华民国。 1948年5月1日，雪风正式改名为“丹阳”舰，舷号12，成为中华民国海军旗舰。次日，常凯申就听到了刘邓大军渡过黄河开始千里跃进大别山的消息…此后内战开始攻守逆转，国民党方面美援断绝，丹阳被拖到台湾基隆。 1954年6月23日，苏联陶普斯号油轮在台湾海峡被丹阳舰拦截并押送返回高雄，获得大量航空汽油。1955年10月20日陶普斯被改名为会稽号运油舰，编入中华民国海军。主要任务为每个月往返基隆、高雄，给空军机场运送飞机燃油。1953年10月4日，中国和波兰合资成立的中波轮船股份公司所属的的布拉卡号油轮被丹阳舰拦截，羁押于高雄。1954年3月18日，中波轮船股份公司所属的哥德瓦尔特号远洋货轮在台湾东南海域遭丹阳舰炮击，被俘获后羁押于基隆。 1965年12月16日，由于船体机件老化等问题，丹阳舰降旗停役，1966年11月16日正式退役，1971年12月31日完成拆解。1971年10月25日联合国大会通过2758号决议，中华人民共和国政府依据此决议取得原由中华民国政府在联合国拥有的中国席位与代表权，而中华民国政府被驱逐出联合国… 有文章称雪风拆解的当天，委座遇意外车祸一病不起。经考据，委座这场意外车祸发生在1969年9月16日下午，距离雪风拆解还早。不过自从雪风拆解以后，委座身体健康日渐恶化…于1975年4月5号去世。 终章 丹阳舰除役后，日本人成立了雪风永久保存期成会，致力于促成“最後之日本海軍艦艇”归还日本。但在最后关头由于台风的关系而浸水严重受损，导致归还失败。 1971年12月8日，中华民国政府将雪风的舵轮与锚送还日本。舵轮收藏于江田岛的旧海军兵学校，而锚则在庭园中展示。1972年，得到了雪风的日本金融危机爆发，田中角荣内阁因被指金权政治而倒台…而在送走雪风以后，台湾的经济开始起飞… 结束了这传奇的一生后，雪风的两只螺旋桨仍留在台湾，一只被被台湾海军官校收藏，另一只现存于台湾成功岭军史公园。舰钟则安放在台湾左营军史馆。 蒋公：我有一句MMP一定要讲... TG：雪风如此多娇，引无数英雄竞折腰。 日本：祥瑞御免，家宅平安。 雪风：我们的战士，神圣的信仰，永远都不会磨灭，她照耀着我们每一个人，阳炎型8番舰の雪风向您报道。 冷静分析 雪风由于其不沉的奇迹，深受海军高层的重视。然而其他舰船上的水兵们却不这么认为。人人都把雪风看作是扫把星，会把友军的好运全部都吸干。雪风所在的第十六驱逐队消耗殆尽，自己毫发无损。所以最痛恨雪风的应该是她的亲姐妹。从开战至末期都完好无缺的第十七驱逐队，在雪风加入后又逐一沉没，战后只剩孤独的雪风。不过奇迹也好，诅咒也罢，很多时候都被过分解读了。除了雪风的运气好之外，其乘员素质高超，作战英勇，比一般驱逐舰都要优秀，这才是使雪风不沉奇迹的保障。战争总有伤亡和牺牲，雪风并非猥琐自保坑队友，而且出勤率极高，别的舰船被击沉了，又关雪风什么事呢？后期日美国力已不可同日而语，联合舰队的毁灭也是必然，雪风要为此背锅那也是很冤了。 很多时候人们会忽略了雪风的历任舰长和船员的高素质。1941年8月联合舰队最后一次划船赛中，雪风的轮机部门夺得了第一名。而且由于伤亡较少，出勤率高，船员很少更新，所谓好的越好，死的大都是新兵和菜鸟。经历的战役越多，老兵们的经验值也越来越高，就停靠码头这一基本功，技术差的舰长半天靠不上去，雪风每次都是一次成功。所以别的军舰很难躲避的攻击，雪风能够很简单地规避。另外，日本研发的新式海军装备，都先给雪风当试验品，雪风也就成为了日本海军第一艘装备雷达和声纳的驱逐舰。雪风的传奇舰长寺内正道曾是驱逐舰“电”的舰长，在各种伤亡惨重的战役中往往能全身而退。寺内正道曾自信的说，只要我在的船就不会沉，他的信条就是雪风上不能死人。寺内正道准确控制航速，油耗和燃油分配能够处于最佳状态。莱特湾海战中，回航期间驱逐舰的油箱都跑干了，需要冒着巨大危险停下来靠其他巡洋舰加油，只有雪风没有发生断油的状况。冲绳海战中，寺内正道直接站在椅子上，头伸出天井目视观测美军密集的空袭。此次作战大和特攻舰队全军覆没，而寺内正道“左脚踢航海长表示左舵，右脚踢航海长表示右舵”，带领雪风在有着“鉄の暴風”称号的冲绳海战中杀出了一条生路。丹阳舰时代，寺内正道也是雪风永久保存期成会的成员之一，并且参与了迎回雪风船锚的仪式。 无论如何，雪风是一艘优秀的驱逐舰，也遇到了优秀的舰长和船员，可惜投错了胎，认了日本军国主义作爹。最后来一张雪风的彩照，前主炮已经被拆除。 彩蛋 修正 很多文章包括中文维基百科均提到雪风号“于12月24日参与拉莫湾的登陆支援”，一查发现拉莫湾居然在南极，还在想雪风这是圣诞节开了时空传送到了南极？再一想不对啊，日本跑到南极登陆做什么，又没有发现第一使徒。这个拉莫湾一定还是在菲律宾海域。终于查到是“拉蒙湾”，即Lamon Bay，位于菲律宾吕宋岛东部。 名字含义 雪风是第八艘阳炎级驱逐舰，她的名字意思是混杂雪花的强风。阳炎（陽炎）也是一种气象现象。英语里叫Heat haze or heat shimmer。汉语里叫热霾，也就是天气炎热时空气中产生的雾，热浪滚滚，有点像（但不是）海市蜃楼的感觉。其他姐妹舰名字的含义： 天津风指的是吹过高天原的风（天つ风）。 时津风指的是顺应季节刮的风。 矶风指的是吹向海滩的风。 浦风即是海风。 滨风为沙滩上的风。 谷风是白天从山谷向山顶吹拂的风。 山风是夜晚从山顶向山谷吹拂的风。 海战精英——神通 神通号轻巡洋舰长期担任最精锐的第二水雷战队的旗舰。1943年7月的科隆班加拉岛海战中，面对绝对优势的盟军，神通开着探照灯就冲啊冲，冲入敌阵掩护旗下驱逐舰作战。神通以一敌三，承受了超过2000发炮弹，迅速被还原为零件状态。后人云：如融熔的铁水在燃烧。最终，烈火熊熊的神通在受到鱼雷攻击之后断为两截，后半截迅速沉没，在海面挣扎的前半截居然还在坚持炮击直到最后一刻。神通一战成名，被美国军史学家萨缪埃尔·莫里森评价为整个战争中作战最勇猛的日本军舰。 著名海战 瓜达尔卡纳尔群岛位于南太平洋，热带岛屿的风光如画，人际罕至，植物茂盛，昆虫肥大，疟疾横行，空气中充满腐烂的恶臭。发生在这里的瓜岛战役，是日本从战略优势走向劣势的转折点，也是美军在太平洋战略反攻的开始。1942年8月7日至1943年2月9日期间共进行了六次较大规模的海战，31000名日本士兵和7100名盟军士兵战死，38艘日本舰船和29艘盟军舰船长眠海底。瓜岛海战期间萨沃湾沉没的日美双方舰船如上图，由于太多飞机和船只沉没于此，后来人们也称此海域为铁底湾：Iron bottom sound。海底的钢铁经常会干扰途径船只罗盘。《细细的红线》这部詹姆斯·琼斯1961写的小说描述了这些血腥的战斗，并被改编成电影，1999年获德国国际电影节金熊奖，以及第71届奥斯卡最佳影片、最佳导演奖提名。这六次海战双方叫法不同，总结如下： 日军称呼 日文原文 盟军称呼 英语原文 第一次所罗门海战 第一次ソロモン海戦 萨沃岛海战 Battle of Savo Island 第二次所罗门海战 第二次ソロモン海戦 东所罗门海战 Battle of the Eastern Solomons 萨沃岛近海海战 サボ島沖海戦 埃斯佩兰斯海角海战 Battle of Cape Esperance 南太平洋海战 南太平洋海戦 圣克鲁斯群岛战役 Battle of the Santa Cruz Islands 第三次所罗门海战 第三次ソロモン海戦 瓜达尔卡纳尔海战 Naval Battle of Guadalcanal 伦加海夜战 ルンガ沖夜戦 塔萨法隆格海战 Battle of Tassafaronga 莱特湾海战是人类历史上最大的海战，也是世界上最后一次航空母舰+战列舰的对决，美日双方合计42艘主力舰加175艘驱逐舰参与了战斗，也是神风特攻队自杀式攻击的首秀，从1944年10月20日持续至10月26日。此时日军已到穷途末路，这175艘驱逐舰中，美军141艘，日本只有34艘。正规航空母舰日军只剩1艘，而美军有9艘，外加26艘轻型航母和护航航母。作战飞机美国有约1500架，日本只有200来架。图为1944年10月20日的道格拉斯麦克阿瑟登上莱特岛。 进击的跳弹 有趣的是，1943年3月俾斯麦海战中盟军空军B-25轰炸机使用了跳弹攻击。跳弹（？）指的是轰炸机低空高速投掷航空炸弹，打水漂般地击中对方舰船。这种战术最先由英国人发明，使用一种叫跳弹的炸弹成功地轰炸了德国鲁尔水坝。一般的航空炸弹为了减小阻力均被设计成流线型，试验发现流线型根本跳不起来啊，于是跳弹就被设计成了圆柱体，高速投下后可蹦蹦跳跳的漂出去几千米距离。轰炸机就可以提前逃走避免进入日军防空炮射程，挽救了无数飞行员的生命。太平洋上最活跃的B-24远程轰炸机，二战中产量高达18482架。作为参考，今天美国空军所有类型的飞机总量大约是5000架，天朝空军大约1700架。 本文资料主要来自于维基百科、萌娘百科和日本战国时代名人辞世诗。在此一并致谢。]]></content>
      <categories>
        <category>故事</category>
      </categories>
      <tags>
        <tag>Navy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（3）常见分类]]></title>
    <url>%2Fblog%2F2018%2F03%2F15%2Fmachine-learning-3%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. Supervised Learning In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal), like SVM, regression, decision trees, naive Bayes, etc. Classification means the output takes discrete values. Three common classification methods are Logistic Regression, Fisher Linear Discriminant Analysis and Nearest Neighbor Classification. Logistic Regression In logistic regression, the dependent variable is categorical. Therefore, logistic regression is a classification model (with discrete labels). Logistic regression can be binomial, ordinal or multinomial. The binary logistic model is used to estimate the probability of a binary response based on one or more predictor (or independent) variables (features). The output for a dependent variable can have only two values, e.g., “0” and “1”. The conditional distribution y∣xy \mid xy∣x is a Bernoulli distribution rather than a Gaussian distribution. Fig. 1. The standard logistic function The model function of LR model is called the logistic function: g(z)=11+e−zg(z) = \frac{1}{1+e^{-z}} g(z)=​1+e​−z​​​​1​​ where zzz is defined as: z=θTx=θ0+θ1x1+...+θnxnz = \theta^T x = \theta_0 + \theta_1 x_1 + ... + \theta_n x_n z=θ​T​​x=θ​0​​+θ​1​​x​1​​+...+θ​n​​x​n​​ We can obtain the generalized linear model function parameterized by θ\thetaθ: hθ(x)=11+e−θTxh_{\theta}(x) = \frac{1}{1+e^{-\theta^T x}} h​θ​​(x)=​1+e​−θ​T​​x​​​​1​​ The probability of the dependent varible is: p(y=1∣x;θ)=hθ(x)p(y=1 \mid x;\theta) = h_{\theta}(x) p(y=1∣x;θ)=h​θ​​(x) p(y=0∣x;θ)=1−hθ(x)p(y=0 \mid x;\theta) = 1 - h_{\theta}(x) p(y=0∣x;θ)=1−h​θ​​(x) p(y∣x;θ)=(hθ(x))y(1−hθ(x))1−yp(y \mid x;\theta) = (h_{\theta}(x))^y (1 - h_{\theta}(x))^{1-y} p(y∣x;θ)=(h​θ​​(x))​y​​(1−h​θ​​(x))​1−y​​ Maximum likehood estimation(MLE) is used to find the parameter values that maximize the likelihood function with the given observations. Define the likehood: L(θ)=p(y⃗∣X;θ)=(hθ(x))y(1−hθ(x))1−yL(\theta) = p(\vec{y} \mid X;\theta) = (h_{\theta}(x))^y (1 - h_{\theta}(x))^{1-y} L(θ)=p(​y​⃗​​∣X;θ)=(h​θ​​(x))​y​​(1−h​θ​​(x))​1−y​​ =∏i=1m((hθ(x(i)))y(i)(1−hθ(x(i)))1−y(i)= \prod_{i=1}^m ((h_{\theta}(x^{(i)}))^{y^{(i)}} (1 - h_{\theta}(x^{(i)}))^{1-y^{(i)}} =​i=1​∏​m​​((h​θ​​(x​(i)​​))​y​(i)​​​​(1−h​θ​​(x​(i)​​))​1−y​(i)​​​​ In practice, it is often convenient to use the log-likelihood (natural logarithm): logL(θ)=∑i=1my(i)loghθ(x(i))+(1−y(i))log(1−hθ(x(i)))\log L(\theta) = \sum_{i=1}^m y^{(i)}\log h_{\theta}(x^{(i)}) + (1-y^{(i)})\log(1 - h_{\theta}(x^{(i)})) logL(θ)=​i=1​∑​m​​y​(i)​​logh​θ​​(x​(i)​​)+(1−y​(i)​​)log(1−h​θ​​(x​(i)​​)) A log-likelihood can be interpreted as a measure of “encoding length” - the number of bits you expect to spend to encode this information, which is called cross-entropy. The problem is to find the independent variable at which the function values are maximized: argmaxθ∑i=1my(i)loghθ(x(i))+(1−y(i))log(1−hθ(x(i))){argmax}_{\theta} \sum_{i=1}^m y^{(i)}\log h_{\theta}(x^{(i)}) + (1-y^{(i)})\log(1 - h_{\theta}(x^{(i)})) argmax​θ​​​i=1​∑​m​​y​(i)​​logh​θ​​(x​(i)​​)+(1−y​(i)​​)log(1−h​θ​​(x​(i)​​)) By derivation, we can find the θ\thetaθ. there are some optimization methods: Gradient ascent (to find a local maximum) Newton method Conjugate gradient etc. Gradient ascent Apply a gradient ascent algorithm for every jjj: θj:=θj+α∂∂θjlogL(θ)\theta_j:= \theta_j + \alpha \frac{\partial}{\partial \theta_j} \log L(\theta) θ​j​​:=θ​j​​+α​∂θ​j​​​​∂​​logL(θ) where +++ means the direction is the same as the gradient. From the definition of g(z)g(z)g(z), we have: ∂g(z)∂z=g(z)(1−g(z))\frac{\partial g(z)}{\partial z} = g(z) (1 - g(z)) ​∂z​​∂g(z)​​=g(z)(1−g(z)) ∂hθ(x(i))∂θj=hθ(x(i))(1−hθ(x(i)))xj\frac{\partial h_{\theta}(x^{(i)})}{\partial \theta_j} = h_{\theta}(x^{(i)}) (1 - h_{\theta}(x^{(i)})) x_j ​∂θ​j​​​​∂h​θ​​(x​(i)​​)​​=h​θ​​(x​(i)​​)(1−h​θ​​(x​(i)​​))x​j​​ ∂logL(θ)∂θj=∑i=1m(y(i)−hθ(x(i)))xj\frac{\partial \log L(\theta)}{\partial \theta_j} = \sum_{i=1}^m (y^{(i)} - h_{\theta}(x^{(i)}))x_j ​∂θ​j​​​​∂logL(θ)​​=​i=1​∑​m​​(y​(i)​​−h​θ​​(x​(i)​​))x​j​​ Thus, batch gradient ascent method: θj:=θj+α∑i=1m(y(i)−hθ(x(i)))xj\theta_j:= \theta_j + \alpha \sum_{i=1}^m (y^{(i)} - h_{\theta}(x^{(i)})) x_j θ​j​​:=θ​j​​+α​i=1​∑​m​​(y​(i)​​−h​θ​​(x​(i)​​))x​j​​ stochastic gradient ascent method: θj:=θj+α(y(i)−hθ(x(i)))xj\theta_j:= \theta_j + \alpha (y^{(i)} - h_{\theta}(x^{(i)})) x_j θ​j​​:=θ​j​​+α(y​(i)​​−h​θ​​(x​(i)​​))x​j​​ where iii is a randomly chosen sample. Then we can repeat choosing the sample and update the gradient and weights. Newton’s method (Newton-Raphson method) In numerical analysis, Newton’s method is a iterative method for finding successively better approximations to the solution xxx of a real-valued function in the form of f(x)=0f(x) = 0f(x)=0 based on Taylor expansion. The process is repeated as: θk+1:=θk−f(θk)f′(θk)\theta^{k+1}:=\theta^k - \frac{f(\theta^k)}{f&#x27;(\theta^k)} θ​k+1​​:=θ​k​​−​f​′​​(θ​k​​)​​f(θ​k​​)​​ Now we need to solve ∇logL(θ)=0\nabla \log L(\theta) = 0∇logL(θ)=0, the process can be written as: θk+1:=θk−H−1∇θlogL(θ)\theta^{k+1}:=\theta^k - H^{-1} \nabla_{\theta} \log L(\theta) θ​k+1​​:=θ​k​​−H​−1​​∇​θ​​logL(θ) where HHH is the Hessian matrix with n+1n+1n+1 by n+1n+1n+1, whose entries are given by: Hij=∂2logL(θ)∂θi∂θjH_{ij} = \frac{\partial^2 \log L(\theta)}{\partial \theta_i \partial \theta_j} H​ij​​=​∂θ​i​​∂θ​j​​​​∂​2​​logL(θ)​​ The problem is: (why 1/m?) argminθJ(θ)=1m∑i=1m−y(i)loghθ(x(i))−(1−y(i))log(1−hθ(x(i)))argmin_{\theta} J(\theta) = \frac{1}{m} \sum_{i=1}^m -y^{(i)}\log h_{\theta}(x^{(i)}) - (1-y^{(i)})\log(1 - h_{\theta}(x^{(i)})) argmin​θ​​J(θ)=​m​​1​​​i=1​∑​m​​−y​(i)​​logh​θ​​(x​(i)​​)−(1−y​(i)​​)log(1−h​θ​​(x​(i)​​)) The Newton’s method is: θ(t+1)=θ(t)−H−1∇J(θ(t))\theta^{(t+1)} = \theta^{(t)} - H^{-1} \nabla J(\theta^{(t)}) θ​(t+1)​​=θ​(t)​​−H​−1​​∇J(θ​(t)​​) where the gradient of the cost function is: ∇J(θ)=1m∑i=1m(hθ(x(i))−y(i))xj\nabla J(\theta) = \frac{1}{m} \sum_{i=1}^m (h_{\theta} (x^{(i)}) - y^{(i)} )x_j ∇J(θ)=​m​​1​​​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)x​j​​ and the Hessian Matrix is: H=1m∑i=1mhθ(x(i))(1−hθ(x(i)))x(i)(x(i))TH = \frac{1}{m} \sum_{i=1}^m h_{\theta}(x^{(i)})( 1 - h_{\theta}(x^{(i)}) )x^{(i)}(x^{(i)})^T H=​m​​1​​​i=1​∑​m​​h​θ​​(x​(i)​​)(1−h​θ​​(x​(i)​​))x​(i)​​(x​(i)​​)​T​​ (H is a matrix, where is jjj?) Logistic regression has a linear decision boundary (hyperplane). Linear discriminant analysis (LDA) Linear discriminant analysis (LDA), which is also known as Fisher’s linear discriminant, is a simple way for dimensionality reduction in linear classification. By LDA, we can find a linear combination of features that classifies objects in two or more groups. Compared with logistic regression, LDA assumes that the independent variables are normally distributed (Gaussian distribution). Logistic regression is more preferable in applications when the assumption is not valid. Compared with analysis of variance (ANOVA), ANOVA uses categorical independent variables and a continuous dependent variable while LDA adopts continuous independent variables and a categorical dependent variable. Compared with principle component analysis (PCA), PCA is an unsupervised learning method, which does not take into account any difference in class, only for dimensionality reduction. LDA attemps to model the difference between the classes of data using labeled data. Compared with factor analysis, factor analysis builds the feature combinations based on differences. LDA deals with similarities. LDA is a dependence technique. A dependence method is one in which a variable of set of variables is identifies as the dependent variable to be predicted or explained by other, independent variables. Dependence techniques include multiple regression analysis, discriminant analysis, and conjoint analysis. An interdependence method is one in which no single variable or group of variables is defined as being independent or dependent. The goal of interdependence methods is data reduction, or grouping things together. Cluster analysis, factor analysis, and multidimensional scaling are the most commonly used interdependence methods. Consider the nnn dimensional input vector xxx and project it down to one dimension: y=wTxy = w^Tx y=w​T​​x where www is the weight vector for linear transformation. Thus we can place a threshold on yyy and then classify yyy as C1/C2C_1/C_2C​1​​/C​2​​. The dimensionality reduction leads to a considerable loss of information, and classes may become strongly overlapping in the low dimensional space. The goal is to find a projection that maximizes the class separation by adjusting www. Therefore, we need to maxmimize the between-class scatter (use difference of mean values) and minimize the within-class scatter (use covariance matrix). Fig. 1. Comparison of the good projection and the bad projection in LDA Algorithm of LDA Two-class cases The purpose of LDA considers maximizing the “Rayleigh quotient”: J(w)=wTSBwwTSWwJ(w) = \frac{w^T S_B w}{w^T S_W w} J(w)=​w​T​​S​W​​w​​w​T​​S​B​​w​​ where SBS_BS​B​​ is the between class scatter matrix and SWS_WS​W​​ is the within class scatter matrix. the means of every class is: μ1=1N1∑x∈c1x\mu_1 = \frac{1}{N_1} \sum_{x\in c_1} \mathbf{x} μ​1​​=​N​1​​​​1​​​x∈c​1​​​∑​​x μ2=1N2∑x∈c2x\mu_2 = \frac{1}{N_2} \sum_{x\in c_2} \mathbf{x} μ​2​​=​N​2​​​​1​​​x∈c​2​​​∑​​x μ=1N1+N2∑x∈allx\mu = \frac{1}{N_1+N_2} \sum_{x\in all} \mathbf{x} μ=​N​1​​+N​2​​​​1​​​x∈all​∑​​x where xk(k=1,..,m)x_k (k=1,..,m)x​k​​(k=1,..,m) is a vector in Xm×nX_{m\times n}X​m×n​​. The scatter matrix for every class is defined as S1S_1S​1​​ and S2S_2S​2​​: S1=∑x∈c1(x−μ1)(x−μi)TS_1 = \sum_{x\in c_1} (x - \mu_1) (x - \mu_i)^T S​1​​=​x∈c​1​​​∑​​(x−μ​1​​)(x−μ​i​​)​T​​ S2=∑x∈c2(x−μ2)(x−μ2)TS_2 = \sum_{x\in c_2} (x - \mu_2) (x - \mu_2)^T S​2​​=​x∈c​2​​​∑​​(x−μ​2​​)(x−μ​2​​)​T​​ SW=S1+S2S_W = S_1 + S_2 S​W​​=S​1​​+S​2​​ SB=N1(μ1−μ)(μ1−μ)T+N2(μ2−μ)(μ2−μ)T=(μ2−μ1)(μ2−μ1)TS_B = N_1(\mu_1 - \mu) (\mu_1 - \mu)^T + N_2(\mu_2 - \mu) (\mu_2 - \mu)^T = (\mu_2 - \mu_1) (\mu_2 - \mu_1)^T S​B​​=N​1​​(μ​1​​−μ)(μ​1​​−μ)​T​​+N​2​​(μ​2​​−μ)(μ​2​​−μ)​T​​=(μ​2​​−μ​1​​)(μ​2​​−μ​1​​)​T​​ The solution to maximize the J(w)J(w)J(w) can be obtained by derivation and Lagrange multiplier. Let ∣∣wTSww∣∣=1||w^T S_w w||=1∣∣w​T​​S​w​​w∣∣=1 to find one solution for www. ∇wc(w)=∇w(wTSBw−λ(wTSww−1))=0\nabla_w c(w) = \nabla_w (w^T S_B w - \lambda (w^T S_w w - 1)) = 0 ∇​w​​c(w)=∇​w​​(w​T​​S​B​​w−λ(w​T​​S​w​​w−1))=0 The Fisher linear discrimination can be obtained as: Sw−1SBw=λwS_w^{-1} S_B w = \lambda w S​w​−1​​S​B​​w=λw Thus www is the eigenvector of the matrix Sw−1SBS_w^{-1} S_BS​w​−1​​S​B​​. Solution is based on solving a generalized eigenvalue problem. In two-class case, www is calculated as: w=Sw−1(μ2−μ1)w = S_w^{-1} (\mu_2 - \mu_1) w=S​w​−1​​(μ​2​​−μ​1​​) Multi-class cases The purpose is to maximize the “Rayleigh quotient”: J(w)=∣ATSBA∣∣ATSWA∣J(w) = \frac{\mid A^T S_B A \mid}{\mid A^T S_W A \mid} J(w)=​∣A​T​​S​W​​A∣​​∣A​T​​S​B​​A∣​​ where AAA is the projection matrix. μj=1Nj∑x∈cjx\mu_j = \frac{1}{N_j} \sum_{x\in c_j} \mathbf{x} μ​j​​=​N​j​​​​1​​​x∈c​j​​​∑​​x μ=1∑j=1CNj∑x∈allx\mu = \frac{1}{\sum_{j=1}^C N_j} \sum_{x\in all} \mathbf{x} μ=​∑​j=1​C​​N​j​​​​1​​​x∈all​∑​​x where cjc_jc​j​​ means the group of class jjj. CCC is the number of classes. NjN_jN​j​​ is the sample number in the class cjc_jc​j​​. The scatter matrix is generalized as: Sj=∑x∈cj(x−μj)(x−μj)TS_j = \sum_{x\in c_j} (x - \mu_j) (x - \mu_j)^T S​j​​=​x∈c​j​​​∑​​(x−μ​j​​)(x−μ​j​​)​T​​ SW=∑j=1CSj=∑j=1C∑xi∈c=j(xi−μc=j)(xi−μc=j)TS_W = \sum_{j=1}^C S_j = \sum_{j=1}^C \sum_{x_i\in c=j} (x_i - \mu_{c=j}) (x_i - \mu_{c=j})^T S​W​​=​j=1​∑​C​​S​j​​=​j=1​∑​C​​​x​i​​∈c=j​∑​​(x​i​​−μ​c=j​​)(x​i​​−μ​c=j​​)​T​​ SB=∑j=1CNj(μj−μ)(μj−μ)TS_B = \sum_{j=1}^C N_j(\mu_j - \mu) (\mu_j - \mu)^T S​B​​=​j=1​∑​C​​N​j​​(μ​j​​−μ)(μ​j​​−μ)​T​​ where NjN_jN​j​​ is used as the weight. Determinants are the product of all eigenvalues of the matrix. We also have: Sw−1SBAk=λAkS_w^{-1} S_B A_k = \lambda A_k S​w​−1​​S​B​​A​k​​=λA​k​​ To solve AAA, we need to use the eigenvalues of Sw−1SBS_w^{-1} S_BS​w​−1​​S​B​​ and obtain the matrix AAA by KKK eigenvectors. KKK is the number of base vectors, namely the dimension of projection matrix AAA as A=[A1∣A2∣...AK]A=[A_1\mid A_2 \mid ... A_K]A=[A​1​​∣A​2​​∣...A​K​​]. The maximum of KKK is C−1C-1C−1. The classification will be better for the eigenvectors with respect to the larger eigenvalues. Multi-class classification can be transferred to binary classification. The techniques can be categorized into One vs Rest and One vs One. One vs Rest OVR (or one-vs.-all, OvA, one-against-all, OAA) involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. The base classifiers are required to produce a real-valued confidence score for its decision, rather than just a class label, e.g., a new label vector zzz where zi=1z_i = 1z​i​​=1 if yi=ky_i = ky​i​​=k and zi=0z_i = 0z​i​​=0 otherwise. We can use C−1C - 1C−1 binary classifiers. Each binary classifier solves the problem of separating samples in a particular class from samples not in that class. This popular strategy suffers from several problems. Firstly, the scale of the confidence values may differ between the binary classifiers. Second, even if the class distribution is balanced in the training set, the binary classification learners see unbalanced distributions because typically the set of negatives they see is much larger than the set of positives (?). Benefits: Small storing cost and short test time. Shortcomings: Long training time and imbalanced Samples. One vs One OVO involves using C(C−1)/2C(C-1)/2C(C−1)/2 binary classifiers for every possible pairs of classes and learning to distinguish these two classes. At prediction time, a voting scheme is applied: all C(C−1)/2C(C-1)/2C(C−1)/2 classifiers are applied to an sample. The class that got the highest number of vote amongst the discriminant functions gets predicted by the combined classifier. Sometimes it suffers from ambiguities in that some regions of its input space may receive the same number of votes. Benefits: Short training time. Shortcomings: Too many classifiers, large storing cost and long test time. Nearest Neighbor Classification The basic idea of the nearest neighbor classification is: a new sample is classified by calculating the distance to the nearest training case; the sign of that point then determines the classification of the sample. The k-NN classifier extends this idea by taking the k nearest neighbors and assigning the sign of the majority. It is common to select k small and odd to break ties (typically 1, 3 or 5). Larger k values help reduce the effects of noisy points within the training data set, and the choice of k is often performed through cross-validation. KNN classifier is non-parametric, which means it does not make any assumptions on the underlying data distribution. The model structure is determined from the data. KNN is a good choice for a classification study when there is little or no prior knowledge about the distribution data. There is no explicit training phase for KNN. KNN makes predictions just-in-time by calculating the similarity between an input sample and each training instance. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors.]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
        <tag>Classification</tag>
        <tag>Logistic Regression</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（2）线性回归]]></title>
    <url>%2Fblog%2F2018%2F03%2F06%2Fmachine-learning-2%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. The Learning Problem Input variables / features : x(i)x^{(i)}x​(i)​​ Target variable : y(i)y^{(i)}y​(i)​​ A training example: (x(i)x^{(i)}x​(i)​​,y(i)y^{(i)}y​(i)​​) A training set: {(x(i)x^{(i)}x​(i)​​,y(i)y^{(i)}y​(i)​​); i=1,2,...,mi=1,2,...,mi=1,2,...,m} The space of input values: XXX The space of output values: YYY The goal is addressed as: Given a training set, learn a function h:X−&gt;Yh: X-&gt;Yh:X−&gt;Y so that h(x)h(x)h(x) is a good predictor for the corresponding value of yyy. If there are nnn features, like x=[x1,x2,...,xn]Tx=[x_1,x_2,...,x_n]^Tx=[x​1​​,x​2​​,...,x​n​​]​T​​, the training set will be Xm×nX_{m\times n}X​m×n​​ and Ym×1Y_{m\times 1}Y​m×1​​. Note that mmm is the number of samples and nnn is the number of features of each sample. Linear Regression Linear regression is a very simple approach for supervised learning. We use linear regression to predict a quantitative response YYY from the predictor variable XXX. The relationship between XXX and YYY is assumed to be linear. A linear function h(x)h(x)h(x) can be used to map from XXX to YYY: h(x)=∑i=0nθixi=θTϕ(x)h(x) = \sum_{i=0}^n \theta_i x_i = \theta ^T \phi(x) h(x)=​i=0​∑​n​​θ​i​​x​i​​=θ​T​​ϕ(x) The intercept term is: x0=1x_0 = 1x​0​​=1. Note that here iii is not the number of samples. multiple linear regression Multiple linear regression is a generalization of linear regression by considering more than one independent variable (XXX: n&gt;1n&gt;1n&gt;1). multivariate linear regression (General linear model) The multivariate linear regression is a generalization of multiple linear regression model to the case of more than one dependent variable (YYY will be a matrix). Linear basis function models In polynomial curve fitting, the feature extraction is represented as the polynomial basis functions: ϕj(x)=xj\phi_j(x) = x^jϕ​j​​(x)=x​j​​, jjj is the polynomial order (1≤j≤M1\leq j\leq M1≤j≤M). A small change in xxx affects all basis functions. Thus using a polynomial basis function is global. Consider the sigmoidal basis functions: ϕj(x)=σ(x−μjs)\phi_j(x) = \sigma (\frac{x - \mu_j}{s})ϕ​j​​(x)=σ(​s​​x−μ​j​​​​), where σ(x)=ex1+ex\sigma(x) = \frac{e^x}{1+e^x}σ(x)=​1+e​x​​​​e​x​​​​. A small change in xxx only affects nearby basis functions (near μj\mu_jμ​j​​). The Least Mean Square (LMS) method The cost function is defined as: J(θ)=12∑i=1m(hθ(x(i))−y(i))2J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 J(θ)=​2​​1​​​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)​2​​ The problem is how to get a minimum J(θ)J(\theta)J(θ). Gradient descent Apply a gradient descent algorithm for every jjj: θj:=θj−α∂∂θjJ(θ)\theta_j:= \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) θ​j​​:=θ​j​​−α​∂θ​j​​​​∂​​J(θ) where jjj is the number of basis function (1≤j≤M1\leq j\leq M1≤j≤M). Note that descent means the direction is the opposite of the gradient (−∇J- \nabla J−∇J). For a single sample, the LMS update rule (or Widro-Hoff learning rule) can be obtained as: θj:=θj−α(hθ(x(i))−y(i))xj(i)\theta_j:= \theta_j - \alpha (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} θ​j​​:=θ​j​​−α(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ Batch gradient descent For many samples, we use the batch gradient descent as: θj:=θj−α∑i=1m(hθ(x(i))−y(i))xj(i)\theta_j :=\theta_j - \alpha \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)} θ​j​​:=θ​j​​−α​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ in the iteration: Repeat until convergence { ... (for every j) } The iteration can be written as: θjk+1=θjk−α∑i=1m(hθ(x(i))−y(i))xj(i)\theta_j^{k+1}=\theta_j^k - \alpha \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)} θ​j​k+1​​=θ​j​k​​−α​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ where iii is the number of samples (1≤i≤m1\leq i\leq m1≤i≤m), kkk is the iteration step. Note that xj(i)x_j^{(i)}x​j​(i)​​ is the feature extraction, x(i)x^{(i)}x​(i)​​ is the sample. until ∣θjk+1−θjk∣&lt;ϵ|\theta_j^{k+1} - \theta_j^{k}|&lt;\epsilon∣θ​j​k+1​​−θ​j​k​​∣&lt;ϵ, then θj\theta_jθ​j​​ reaches convergence. For every iteration, all samples will be traversed. If a new sample is added, the iteration has to start from the first sample again. Stochastic gradient descent When the training set is large, instead, we use Stochastic gradient descent as: θjk+1=θjk−α(hθ(x(i))−y(i))xj(i)\theta_j^{k+1} = \theta_j^k - \alpha (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} θ​j​k+1​​=θ​j​k​​−α(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ in Loop { for i = 1,m { ... (for every j) } } until ∣θjk+1−θjk∣&lt;ϵ|\theta_j^{k+1} - \theta_j^{k}|&lt;\epsilon∣θ​j​k+1​​−θ​j​k​​∣&lt;ϵ, then θj\theta_jθ​j​​ reaches convergence. For each θ\thetaθ, only one sample is used for iteration until the convergence. If a new sample is added, the iteration will continue only using the new sample. Finally, the new θ\thetaθ and hθh_{\theta}h​θ​​ are obtained. Feature scaling is a method used to standardize the range of independent variables or features of data (data normalization). Feature scaling can be used to improve the iteration rate in gradient descent, e.g., rescaling and standardiztion. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. Thus each feature contributes approximately proportionately to the final values. The normal equations To avoid interations, we can use the normal equations. Consider Xm×nX_{m\times n}X​m×n​​ and Ym×1Y_{m\times 1}Y​m×1​​ in matrix, the derivative of J(θ)J(\theta)J(θ) equals zero: ∇θJ(θ)=XTXθ−XTy⃗=0\nabla_\theta J(\theta) = X^T X\theta - X^T \vec{y} = 0 ∇​θ​​J(θ)=X​T​​Xθ−X​T​​​y​⃗​​=0 Finally, θ=(XTX)−1XTy⃗\theta = (X^T X)^{-1}X^T\vec{y} θ=(X​T​​X)​−1​​X​T​​​y​⃗​​ In most situations of practical interest, the number of data points mmm is larger than the dimensionality nnn of the input space, and the matrix XXX is of full column rank. A matrix is full column rank when each of the columns of the matrix are linearly independent. Then XTXX^TXX​T​​X is necessarily invertible and therefore positive definite. The minimum J(θ)J(\theta)J(θ) can be obtained at the critical point when ∇θJ(θ)=0\nabla_\theta J(\theta)=0∇​θ​​J(θ)=0. If m≤nm \leq nm≤n or XXX is not of full column rank, XTXX^TXX​T​​X is not invertible. Regularization is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting. E.g. In Quadratic ridge regression, the cost function is rewritten as: J(θ)=12∑i=1m(hθ(x(i))−y(i))2+λ2∑j=1n∣θj∣2J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2} \sum_{j=1}^n |\theta_j|^2 J(θ)=​2​​1​​​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)​2​​+​2​​λ​​​j=1​∑​n​​∣θ​j​​∣​2​​ According to ∇θJ(θ)=0\nabla_\theta J(\theta)=0∇​θ​​J(θ)=0, θ^\hat{\theta}​θ​^​​ can be calculated as: θ^=(XTX+λI)−1XTy⃗\hat{\theta} = (X^TX+\lambda I)^{-1}X^T\vec{y} ​θ​^​​=(X​T​​X+λI)​−1​​X​T​​​y​⃗​​ Direct methods: Solve the normal equations by Gaussian elimination or QR decomposition Benefit: in a single step or very few steps Shortcoming: not feasible when data are streaming in real time or of very large amount Iterative methods: use the stochastic or gradient descent Benefit: converging fast, more attactive in large practical problems Shortcoming: the learning rate α\alphaα should be carefully chosen. For probalilistic interpretation of Least Mean Square, by the independence assumption, LMS is equivalent to maximum likehood estimation (MLE) of θ\thetaθ. Locally weighted linear regression (LWR) The problem is how to get a minimum J(θ)J(\theta)J(θ) given as: J(θ)=∑i=1mw(i)(hθ(x(i))−y(i))2J(\theta) = \sum_{i=1}^m w^{(i)}(h_{\theta}(x^{(i)}) - y^{(i)})^2 J(θ)=​i=1​∑​m​​w​(i)​​(h​θ​​(x​(i)​​)−y​(i)​​)​2​​ where w(i)=exp(−(x(i)−x)22τ2)w^{(i)} = exp(-\frac{(x^{(i)} - x)^2}{2\tau^2}) w​(i)​​=exp(−​2τ​2​​​​(x​(i)​​−x)​2​​​​) where xxx is the query point for which we’d like to know its yyy. Essentially higher weights will be put on the training examples close to xxx. Parametric and Nonparametric A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at a parametric model, it won’t change its mind about how many parameters it needs. Nonparametric methods are good when you have a lot of data and no prior knowledge, and when you don’t want to worry too much about choosing just the right features. (Artificial Intelligence: A Modern Approach) Benefits of parametric models: Simpler: These methods are easier to understand and interpret results. Speed: Parametric models are very fast to learn from data. Less Data: They do not require as much training data and can work well even if the fit to the data is not perfect. Limitations of parametric models: Constrained: By choosing a functional form these methods are highly constrained to the specified form, because a parametric model has a fixed and finite number of parameters (θ\thetaθ). Limited Complexity: The methods are more suited to simpler problems. Poor Fit: In practice the methods are unlikely to match the underlying mapping function. Benefits of nonparametric models: Flexibility: Capable of fitting a large number of functional forms. Power: No assumptions (or weak assumptions) about the underlying function. Performance: Can result in higher performance models for prediction. Limitations of non-parametric models: More data: Require a lot more training data to estimate the mapping function. Slower: A lot slower to train as they often have far more parameters to train. Overfitting: More of a risk to overfit the training data and it is harder to explain why specific predictions are made. Locally weighted linear regression is a non-parametric learning algorithm. The term non-parametric means the model structure is not specified a priori but is instead determined from data. It is not meant to imply that such models completely lack parameters but that the number and nature of the parameters are flexible and not fixed in advance. If we use unweighted linear regression, Once θ\thetaθ is determined, we no longer need to keep the training data around to make future predictions. In contrast, if the weighted linear regression is applied, the entire training set should be kept around.]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
        <tag>Linear Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（1）常识]]></title>
    <url>%2Fblog%2F2018%2F02%2F28%2Fmachine-learning-1%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. Elementary concepts What is machine learning? Machine Learning is a scientific discipline that explores the construction and study of algorithms that can learn from data. Machine learning enables analysis of massive quantities of data. We all know “Machine”, i.e. computers or computer programs. Do you know what is “Learning”? Learning denotes “Changes in the system that are adaptive in the sense that they enable the system to do same task or tasks drawn from the same population more efficiently and more effectively the next time” [1]. As a pioneer in the field of artificial intelligence, Herbert A. Simon created with Allen Newell the Logic Theory Machine (1956), and the General Problem Solver (GPS) (1957) programs, which was thought as the first method developed for separating problem solving strategy from information about particular problems. Arthur Samuel created the world’s first successful self-learning programs, the Samuel Checkers-playing. He coined the term “Machine Learning” in 1959, which was defined as “the field of study that gives computers the alility to learn without being explicitly programmed”. Tom Mitchell (former Chair of the Machine Learning Department at CMU) gave a modern definition: “A computer program is said to learn from experience EEE with respect to some class of tasks T and performance measure PPP, if its performance at tasks in TTT, as measured by PPP, improves with experience EEE.” An example: playing checkers. EEE = the experience of playing many games of checkers. TTT = the task of playing checkers. PPP = the probability that the program will win the next game. What can machine learning do？ ML can be used to deal with Big Data deluge and predicitve analytics, e.g. Document Classification Spam Filtering Weather Prediction Stock Market Prediction Collaborative Filtering Clustering Images Human Genetics Decoding thoughts from brain scans Medical data analysis Finance Robotics Natural language processing, speech recognition Computer vision Web forensics Computational biology Sensor networks (new multi-modal sensing devices) Social networks Turbulence problem Three components of a machine learning algorithm Pedro Domingos, a CS professor at the University of Washington, decomposed machine learning into three components: Representation, Evaluation, and Optimization [2]. Table 1. The three components of learning algorithms Represnetation Evaluation Optimization Instances Accuracy/Error rate Combinatorial optimization ^K-nearest neighbor Precision and recall ^Greedy search ^Support vector machines Squared error ^Beam search Hyperplanes Likelihood ^Branch-and-bound ^Naive Bayes Posterior probability Continuous optimization ^Logistic regression Information gain ^Unconstrained (Convex) Decision trees K-L divergence ^^Gradient descent Sets of rules Cost/Utility ^^Conjugate gradient ^Propositional rules Margin ^^Quasi-Newton methods ^Logic programs ^Constrained Neural networks ^^Linear programming Graphical models ^^Quadratic programming ^Bayesian networks ^Conditional random fields Representation A classifier must be represented in some formal language that the computer can handle. A learner takes observations as inputs. The observation language is the language used to describe these observations. The hypotheses that a learner may produce, will be formulated in a language that is called the hypothesis language. The hypothesis space is the set of hypotheses that can be described using this hypothesis language [3]. The problem is how to represent the input, i.e. what features to use. For example, 3-layer feedforward neural networks (or computational graphs) form one type of representation, while support vector machines with RBF kernels form another. Evaluation Evaluation is essentially how you judge or prefer candidate programs (hypotheses). An evaluation function (also called objective function, utility function, loss function, fitness function or scoring function) is needed. Mean squared error (of a model’s output vs. the data output) or likelihood (the estimated probability of a model given the observed data) are examples of different evaluation functions. Optimization Finally, a method is needed to search among the candidate programs (in the space of represented models) for the highest-scoring one as the optimum. Stochastic gradient descent and genetic algorithms are two different ways of optimizing a model class. Machine Learning Tasks Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning “signal” or “feedback” available to a learning system: superivsed learning and unsupervised learning. Other tasks also include semi-supervised learning, active learning, reinforcement learning, etc. Supervised learning Supervised learning (or inductive learning) is the most mature, the most studied and the type of learning used by most machine learning algorithms. Learning with supervision is much easier than learning without supervision. Training data includes desired outputs. The goal is to learn a general rule that maps inputs to outputs. Typically, the inputs are transformed into a feature vector, which contains a number of features that are descriptive of the object. Classification Regression Reinforcement learning is concerned with how to take actions in an environment to maximize some notion of long-term reward, such as game-theory. It differs from standard supervised learning in that correct input/output pairs are not provided, nor sub-optimal actions explicitly corrected. Unsupervised learning Training data does not include desired outputs. It is hard to tell what is good learning and what is not. The program own will find the feature in its inputs as “unlabeled” data. Clustering Dimensionality reduction Anomaly dection Neural Networks Main purpose of studying ML To make money.]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日本海军舰艇命名]]></title>
    <url>%2Fblog%2F2018%2F02%2F14%2Fship-name-japan%2F</url>
    <content type="text"><![CDATA[出云 日本领土陆地面积约37.79万平方公里，由北海道、本州、四国、九州等其他6,852个岛屿组成。看似国土狭小的日本，却拥有可怕的领海面积，约447万平方公里，超过其陆地面积十一倍。由于岛国的局限性，日本自古以来极其重视发展海军，并于第二次世界大战时期达到了巅峰。二战以后至今，日本作为战败国，曾经优秀的海军也随之衰落。截至目前日本海上自卫队的最大军舰是22DDH和24DDH。其中，DDH代表helicopter destroyer，即直升机驱逐舰。22和24分别指的是平成22年（2010年）和平成24年（2012年）完成防卫预算下拨。不过不要被它驱逐舰的名字迷惑了，DDH的体型相当于普通驱逐舰的3～5倍。所以日本其实是在玩文字游戏，其实它就是一艘直升机航空母舰。由于海上自卫队的性质，国际上是绝不允许日本建造巡洋舰、航空母舰等大型进攻型舰艇的，只好打打擦边球。强行命名为驱逐舰也可以看出日本海上自卫队的脸皮比航空母舰的甲板钢还要厚。有趣的是， 按照自卫队的传统，新舰的命名应该在下水日（8月6日）才公布，却因海上幕僚监部公文作业疏忽的缘故7月17日就提前曝光了。她就是出云，日文名いずも，Izumo。 首先看一下出云的数据。全长248米，宽度38米，吃水7.5米，满载排水量26000吨，配备4台通用动力LM2500 IEC型燃气轮机，最高航速30节。搭载七架Sikorsky SH-60K海鹰直升机，其中SH代表Seahwak，主要任务是反潜作战（Anti-submarine warfare）。由Ishikawajima-Harima Heavy Industries海洋联合横滨工厂负责建造，单舰造价1200亿日元。首舰出云于2013年8月6日在神奈川县横滨市下水，2015年3月正式服役。 不过“出云”不是破云而出的意思，出云是日本弥生时代一个令制国（相当于古代中国的州）名字。二号舰被命名为加贺（かが，Kaga），同样是一个古国名。另外，这两艘战舰的命名是沿袭二战期间的出云重巡洋舰和加贺航空母舰的历史。不过二战期间出云和加贺都参与了侵华战争，罪行累累，网上说该舰被命名为出云是“恶魔舰复活”也不过分。相比现代的日本海上自卫队，二战时期日本海军（IJN）的命名有一整套独特的规则，很多名字听起来很和风，很美，比如瑞鹤、夕云、南风等等。 航空母舰命名规则 航空母舰是一种搭载飞机为主要武器的军舰，是目前海军最大的作战舰艇平台。日本与航空母舰有千丝万缕的羁绊。例如，一战二战期间中国没有航空母舰，中文里航空母舰这个词怎么来的呢？并不是直接翻译于Aircraft Carrier，而是来自于日语：航空母艦（こうくうぼかん），简称空母（くうぼ），可见日本航空母舰的影响之深远。所以提起航空母舰有时侯会引起误会，并不是会飞的船，而是省略了两个字，即“航空（器的）母舰”。世界上第一艘标准的航空母舰是日本最先建造完工的。二战爆发前夕，IJN是太平洋上实力最强，拥有十艘正规空母，相比之下英国有八艘，美国只有七艘。成名之战是1941年的日本的六艘航空母舰袭击珍珠港。后来美日之间的珊瑚海海战，中途岛海战，莱特湾海战，更是宣告了航空母舰时代的到来。 日本航空母舰一般按照龙（りゅう）、凤（ほう）、鹤（かく）等神话生物命名。中途岛海战前，日本的十艘航空母舰分别是：赤城，加贺、苍龙、飞龙、翔鹤、瑞鹤、凤翔、龙骧、祥凤和瑞凤。民用船舶改造而成的日本航母以各种鹰（よう）命名，例如大鹰、云鹰、飞鹰、神鹰、海鹰、隼鹰、冲鹰。千岁、千代田是两个特例，她们原是水上飞机母舰，改装成为航空母舰之后保留了原名。 日语中有两种龙，龍（りゅう）是来自中国东方传说中的龙，是掌管风雨的小神，而另一种，竜（たつ，是龍的简化字）一般指西方神话中的dragon，是一种邪恶贪婪的生物（通常是宝藏的看守）。两种龙的共同点都是拥有强大的力量。一个冷知识是日本的龙通常是三爪，与中国唐代的龙的风格一致，中国的龙经历了从三爪到五爪的演变，是皇帝的象征，而韩国的龙是四爪。在日本，凤凰（鳳凰）是一种比龙更高阶的存在。日本人信奉天照大神，有记载称垂仁天皇26年，天照大神降临神风伊势国，于28年化为白凤。龙有邪恶的龙，比如传说里有勇者斗恶龙，但凤凰是绝对的神圣、吉祥、繁荣的象征和征兆，通常出现在皇室的建筑、服饰、纪念币等，例如天皇座驾丰田世纪的车标就是一只凤凰。西方文化里也有类似的生物，即phoenix，但是与凤凰还是有根本的区别。Phoenix一般翻译为不死鸟、火鸟，全身赤红色，长得类似鹰的一种猛禽，特点是会浴火重生。而东方神话里凤凰是五彩的，栖息在梧桐上，象征爱情，类似孔雀的神鸟。山海经里南山经记载，“丹穴之山，有鸟焉，其状如鸡，五采而文，名曰凤皇。首文曰德，翼文曰义，背文曰礼，膺文曰仁，腹文曰信，是鸟也，饮食自然，自歌自舞，见则天下安宁。”与西方的不死鸟最大差别就是东方的凤凰不会浴火重生。至于凤凰涅磐，这个词是郭沫若生造的，并非古代传说。日本的国鸟是绿雉，印在了一万元日元上，因此可以说日本的凤一般指的就是中国的凤凰，祥瑞的象征。讽刺的是，二战中第一艘沉没的日本航母就是祥凤。东方文化里，鹤代表着健康长寿、吉祥高贵，也可以是一种为夫妻带来孩子的仙鸟（送子鹤）。宋徽宗画过一幅瑞鹤图。日本天皇的声音又叫做鹤音、玉音。一千元日元上也有丹顶鹤的图案。北海道的阿伊努人把生活在钏路湿地的丹顶鹤称为湿地之神。总之取名叫鹤显得仙气十足。 翔鹤与瑞鹤两姐妹是第五航空战队的主力，是当时日本最强，飞行员最优秀，战绩最好的航空母舰。作为最著名的两艘日本航母，赤城和加贺却并没有吉祥生物加持。赤城是巡洋舰改装过来的航母，赤城来自关东北部的赤城山來命名。第一第二航空战队的旗舰赤城，是日本当时最大的航母，一度被视为日本海军机动部队的象征，航空兵的摇篮，山本五十六曾担任舰长。加贺是战列舰改装过来的航母，所以是按古代令制国名来命名。最大的特点是跑得慢，只有28节，以至于舰队里其他航母为了等她，还要额外携带大量油筒。赤城、加贺、苍龙、飞龙四舰葬身于中途岛，其中加贺全舰共811人死亡，伤亡惨重。在加贺短暂的一生中，曾与国民党空军交过手，文末有彩蛋。 二战后期日本主要建造了三艘云龙级航母：云龙，天城和葛城。后面两艘取名自天城山、大和葛城山。天城山盛产衫树，是当时海军最常用的甲板木料。有趣的是，云龙1944年8月下水的时候，地主家也没有余粮了，日本已经没有可用的舰载机，云龙只好呆在吴港思考舰生，最后一直当作运输船使用。天城建成的时候物资匮乏，采用的是重巡洋舰的主机。天城也没有舰载机，一直无所事事，1945年7月8日在美军的轰炸中沉没，成为了至今为止全世界最后一艘战损的航空母舰。葛城建成的时候穷的甚至巡洋舰主机都没有了，只好用的是驱逐舰主机，主机功率从15万马力被迫下降为10万马力。天城和葛城这一对难兄难弟，为了躲避空袭，航空母舰的甲板上装饰了植物、田地、小房子之类，企图cosplay小岛，然而被炸弹爆炸掀起的巨浪吹掉。天城运气太差，被炸沉，葛城倒没有受到致命的损伤，幸存到了1946年11月。 1945年2月以后，所有日本的航空母舰因为没有飞行员不得不逗留在港口内，无法四处活动。 战列舰命名规则 战列舰有时候也直接称战舰。特点是厚重的装甲加上越来越巨大的火炮。在飞机导弹出现以前，舰船以火炮作为主要作战武器，战时排列成一条线互相射击，因此称作战列舰。随着1906年英国的无畏号（HMS Dreadnought）服役，全世界海军开始进入全重型火炮（All-Big-Gun）和高航速主导的时代，按此思想建造的战舰统称为无畏舰。日德兰海战之后，随着主炮口径增加到13英寸以上，并且主炮射程已经超出了视力极限以外，统称为超无畏舰。日语中，根据外来语无畏舰的读音ドレッドノート取首字ド，并用同音字“弩”替代，故称为弩级战舰。超弩级战舰也即超无畏舰。尽管在一战二战时期战列舰是主力（Capital ship），越造越大，但现代已经全部淘汰，最后的露面是在1990年的海湾战争。不过未来随着电磁炮的出现和发展，倒可能使战列舰重新登上历史舞台。 1905年颁布的《日本海军舰艇命名办法》规定：战列舰以古国名命名。二战期间比较著名的日本战列舰有：大和，武藏，长门，陆奥，扶桑，山城，伊势，日向等。 从奈良时代开始，日本古代令制国分为五畿七道，后加上北海道。五畿包括：山城国（现京都府），大和国（现奈良县），河内国（现大阪府），和泉国（现大阪府南部），攝津国（现兵库县和大阪府）。七道分别是：东海道，东山道，北陆道，山阴道，山阳道，南海道，西海道。道是仿唐朝制度的，比国高一级的行政单位。每一个道下分数个国，例如东海道里有伊贺国，东山道里有信浓国，西海道还有一个萨摩国。 人类有史以来建成的最大的战列舰是日本海军的大和，名字来自古代畿内五国之一的大和国。还有一艘同型舰武藏。可以说她们体现了日本造船技术的巅峰。大和的满载排水量达到了惊人的72,809吨，吃水达到了10.4米，最高行速27节。赫赫有名的三座三联装94式45倍口径460毫米主炮（18.1英寸），是人类有史以来最大的舰炮，一枚炮弹重1.5吨，射程超过26英里。主炮齐射时，甲板上的防空炮手必须躲回舰体内，防止被冲击波杀伤。实际造价为1.37802亿日元，一艘相当于1937年日本GDP的0.29％。 戏剧性的是，作为日本帝国和民族的象征，她是决不允许被击沉的。因此实力最强的大和却始终无缘前往一线作战，一生里大多数时间都停在港口内消磨时光。1945年4月6日最后的时刻，帝国已是穷途末路，大和默默地驶出港口，而即将迎接她的是15艘美国航空母舰的攻击。 巡洋舰命名规则 巡洋舰最初指的是可以独立行动的战舰，续航力强，被用于巡逻海外殖民地，排水量、装甲及火力一般仅次于战列舰，但航速较高，机动性强，射速快，并且造价远低于战列舰。根据1921年《华盛顿海军条约》，主炮口径超过8英寸（203毫米）为重巡洋舰。1930年的《伦敦海军条约》为了削减海军装备，重新制定了主炮口径6.1英寸及以上为重巡洋舰，其他的划分为轻巡洋舰。重巡洋舰一般用于袭击敌军和保护主力舰，轻巡洋舰承担护航或防空等任务。为了保证火力和防护的平衡，一般要求军舰自身的装甲要能够抵挡自己的火炮攻击。但也有一种奇葩的船型，她拥有比肩战列舰的强大火力，但为了提升机动性，牺牲了自身的装甲防护，航速可以与巡洋舰相当，专门用于追击作战。这就是战列巡洋舰，典型的有英国的胡德，号称英国皇家海军的骄傲（The mighty Hood）。胡德的火力不亚于德国的俾斯麦号战列舰，但胡德的防护水平却相差很远，丹麦海峡一战中被命中弹药库造成剧烈爆炸沉没。为了给胡德报仇，英国甚至出动了海军能动的所有战舰追杀俾斯麦。现代海军已经很少有国家继续建造巡洋舰，随着导弹和雷达的广泛应用，巡洋舰的角色逐渐被大中型驱逐舰所替代。 日本海军的巡洋舰可以说是“山、川”舰队，轻巡洋舰以河流命名，战列巡洋舰和重巡洋舰以山命名。 日本的轻巡洋舰比较重视鱼雷作战。比较著名的有球磨级轻巡洋舰，球磨来自日本熊本县南部水系的球磨川，是日本三大急流之一。球磨级造了五艘，一般作为水雷战队旗舰进行掩护作战，也执行运输船队的护航任务，航速达到了36节。大井和北上于1941年进行了魔改，拆除了三门主炮，在两侧各加装了五座四联装九二式610毫米鱼雷发射管，称重装雷舰。1942年北上拆除了鱼雷发射管改为高速运输舰，搭载了8艘大型登陆艇。1944年北上继续进行魔改，用于特攻作战，搭载了四艘“回天”人操鱼雷。所谓人操鱼雷，就是由人直接操舵，自杀式攻击的特殊鱼雷（潜艇）。回天反映了当时日本对战局逆转的渴望（天を回らし、戰局を逆転させる）。虽然威力很大，直径达到1000毫米，由于操作困难，战果不大。 著名的战列巡洋舰有金刚级四姐妹：金刚，比睿，榛名，雾岛。金刚山在大阪府与奈良县的边境上，景色优美，吸引了众多的登山爱好者。比睿山位于京都府，自古被视作镇护京都的圣山，山上有延曆（历）寺，故有日本佛教之母山的美称。榛名山是火山，位于群马县，上毛三山之一（赤城、榛名、妙义），也是头文字D中秋名山的原型（日语里榛与春同音，对应秋名）。雾岛山是火山群，位于九州宫崎县附近。传说是日本天照大神的孙子降临的地方。首舰金刚号由英国维克斯公司建造，其他三艘在日本建造，1915年完工。所以金刚级具有英国战舰的特点，比如高大的三角主桅杆。有趣的是，一战期间英日属于同盟国，英国曾请求租借日本的金刚级以对抗德国，但被日本回绝。金刚级建成时属于战列巡洋舰，后来日本海军废除了战列巡洋舰这一舰种，1923～1933年金刚型四舰进行了大改，成为（高速）战列舰，但还是保留了之前的命名方式。不过装甲设计没有得到大规模改进，防御能力不如战列舰。其中，金刚曾担任第五代（1931～1933）日本联合舰队旗舰，1944年沉没于台湾海峡附近，是日本唯一一艘被潜艇击沉的战列舰。在舰队Collection中的人设为英国海归，爱喝红茶，外号大傻。比睿由横须贺海军工厂建造，因《伦敦海军条约》被裁成训练舰，在1930年代曾多次担任日本天皇检阅海军的御召舰，深受民众喜爱，其照片曾出现在纪念邮票上。榛名之前所有主力舰军在海外订购或海军工厂建造，而榛名是第一艘由民企，即川崎造船所（现川崎重工）承建，主机测试前发生了故障导致延期，川崎造船所造机工作部长篠田恒太郎因自责剖腹自尽，可见压力之大。另一家民企，三菱合资会社长崎造船所（现三菱重工）则承建了四号舰雾岛，与姐姐比睿在1942年所罗门海战中一同沉没。 驱逐舰命名规则 驱逐舰最初用来对付鱼雷艇，便宜、易造又好用，后来也承担起防空、反潜、攻击等多用途任务，是各国海军建造最多的一种军舰。英语里的Destroyer翻译成驱逐舰着实有点委屈它了。游戏里驱逐舰由于吨位小，航速快，经常被称为小学生（大误）。 日本驱逐舰分两种，一等驱逐舰以天气、潮汐、水流、月亮、季节、自然现象等命名，一个不是很恰当的概括就是“风花雪月”。二等驱逐舰以植物等命名。 很多驱逐舰名字的禅意不输各种艺术品。例如，吹雪型驱逐舰有：吹雪，白雪，初雪，深雪，从云，东云，薄云，白云，矶波，浦波；绫波型驱逐舰有绫波，敷波，朝雾，夕雾，天雾，狭雾，胧，曙，涟，潮；晓型驱逐舰有晓，响，雷，电，著名的第六小学生驱逐队。首舰吹雪于1928年8月10日完工，排水量1980吨，全长118.5米，宽10.4米，吃水3.2米，最高航速38节。以植物命名的驱逐舰有：松，梨，若竹，楢（即橡树）等。这里介绍一艘著名的日本驱逐舰：岛风。该型驱逐舰最初计划建造32艘，可由于她的动力系统过于复杂、生产跟不上等缘故，最后只建造了一艘，岛风没有姊妹舰，也没被编入任何驱逐舰队，从出生开始就很孤独。岛风有两个特点，第一个特点是跑得特别快，装有试验涡轮机，最高航速达到了惊人的40.7节（约每小时72公里），创下日本驱逐舰的最快记录，人称海上飚车老司机。另一个特点是鱼雷特别强，装备了3座五连装九三式重型酸素鱼雷（日语中酸素意为氧气，水素意为氢气，窒素意为氮气），一举成为水雷战最强驱逐舰（日语中的水雷是鱼雷、水雷、深水炸弹的统称）。这种鱼雷是当时全世界最先进的鱼雷，使用压缩氧气代替压缩空气作为推进，最大射程达到了40km，弹头重达1080磅，并且航迹难以被发现。同时期美军的MK15鱼雷最远射程仅14km，弹头也只有827磅。岛风单次可以齐射15枚九三式，几乎是普通驱逐舰的2倍。服役后，太平洋上的局势已经不可逆转，而且鱼雷决战的思想也已经落后，因此岛风并没有什么骄人的战绩。1944年11月8日奥尔莫克湾战役中，虽然岛风以惊人的高速机动能力躲过了多次轰炸，最终还是不敌美军舰载机，中弹太多而后因锅炉过热引起爆炸，沉没在菲律宾附近海域。此后，太平洋再也没有岛风。不过在海底，岛风应该不再那么孤独了吧。 潜艇命名规则 潜水艇就是在水下航行的舰艇，小的有单人操作的水下航行器，大的有苏联建造的941台风弹道导弹核潜艇，水下排水量可达4.8万吨。二战期间大西洋战场上的狼群，指的就是德国海军邓尼茲领导的U型潜艇部队，对盟军的海上交通和补给线造成了巨大的破坏。在日本，潜水艇被称作潜水艦。 根据《日本海军舰艇命名办法》，一等潜艦按汉字“伊”后加上数字命名，二等潜艦按汉字“吕”后加上数字命，三等潜艦按汉字“波”后加上数字命。名称按顺序取自《伊呂波歌》名。这是一首日本平安时代的和歌，每句5音和7音相错。由于全歌以47个不重复的假名组成，后来常被用于书法的范文和假名的学习范文。有一个中译版本：花虽芬芳终须落，此世岂谁可常留。有为山深今日越，不恋醉梦免蹉跎。 二战中的日本潜艇虽然没有德国狼群那么臭名昭著，不过也有那么几个比较出名的。第一个例子是二战中最大的潜艇：伊四〇〇型。它的满载排水量达到了6560吨，达到了一般巡洋舰的水平。之所以造这么大，其特色是可以搭载三架小型轰炸机，计划是潜航到接近海岸线，然后释放晴岚水上飞机携带细菌武器攻击美国本土，因此伊400实际上算是潜水空母，不过尚未投入实战日本就已投降。第二个例子是伊19，1942年9月15日在所罗门群岛伊19使用六发鱼雷击沉了美军的胡蜂号航空母舰（CV-7)。第三个例子是伊58，以击沉美军印第安纳波利斯号重巡洋舰而声名大噪，因为这艘巡洋舰刚刚完成了运输“小男孩”原子弹到提尼安岛的秘密任务。1945年8月6日B-29轰炸机在广岛上空投掷该原子弹，伤亡10万余人。2016年有一部尼古拉斯·凯奇主演的电影：《印第安纳波利斯号：勇者无惧》即是根据此事件改编（USS Indianapolis: Men of Courage）。 本文资料主要来自于维基百科和萌娘百科。在此一并致谢。 彩蛋 舰队Collection 一般译作舰队收藏，是由角川游戏开发、DMM.com提供及运营的网页游戏，以二战时期的日本军舰为题材，玩家需要收集称为舰娘（艦娘（かんむす））的军舰萌拟人化角色卡片，可以对舰娘进行强化及改造，并编制不同的舰队与敌人战斗务求获得胜利。作为PC浏览器运行的网页游戏，在运营半年用户即突破100万。立绘精美，画师在创作时也加入了很多历史元素。例如，大和的形象是一个打伞的大姐姐，里面的伞来源于大和上的一式三型电探天线。所谓的电探又是啥？日本的雷达在以前被翻译成电波探信仪。 舰娘本娘 吾妻 旧日本海军有一艘装甲巡洋舰名叫“吾妻”，订购自法国。1898年由法国Ateliers et Chantiers de la Loire造船厂建造。1900年回到日本服役，曾参与日俄战争中的旅顺口海战，蔚山海战和对马海战。到1945年已是服役四十多年的老舰，被美军击沉于港口内，第二年被解体。 鳳翔 凤翔（ほうしょう，Hōshō）就是上文提到的世界上第一艘服役的全通式甲板航空母舰，采用岛装上层建筑，技术上是从英国窃取而来。本来世界第一属于英国的竞技神号航空母舰，不过由于英国的进度拖拉，导致世界第一被日本海军抢走。凤翔于1920年12月16日开工，1922年12月27日服役，被尊称为航母之母。她是唯一一艘活到日本投降后还没有受损的航空母舰，战后作为复员运输舰到各地进行接运任务。后在大阪日立造船樱岛工厂解体，度过了幸运的一生。注意，凤翔跟祥凤是两码事。 武勋三舰客 旧日本海军二战期间有三艘公认的武勋舰（战绩好，活的久）：瑞鹤被称为“幸運の空母”。榛名一直活跃在一线战场，受到过不同损伤而始终没有被击沉。雪风被称为“奇跡の駆逐艦”，著名的扫把星（祥瑞），日常坑队友，通常在海战中自己毫发无损，克死友军无数。民间又称抗日奇侠雪风号。，感兴趣的我们将在后文《雪风的故事》里再见。 相思不断笕桥东 如果您浏览了上面那么多枯燥的数据，还能坚持看到这里没有关掉页面，那真的十分感谢了。文章结尾，再讲一个故事。 加贺号航空母舰，侵华战争中多次参与对中国人民的屠杀，参加过对苏州、杭州、上海的轰炸。虽然耀武扬威无数，不过也有扑街的时候。1937年8月14日，正值台风过境，日军决定空袭杭州笕桥——中国的空军学校，就是来自加贺。那一天，21架国军空军霍克-3双翼机与45架日本舰载机于空中战斗，加贺损失了8架八九舰攻和2架九四舰轰，而年轻的中国空军竟无一伤亡，吊打日军，胜利凯旋（一说击落日军飞机三架击伤一架）。史称八一四空战，又称笕桥空战。这样一支英勇的空军，更是打到木更津航空队联队长石井义大佐剖腹自杀，被日军视为耻辱。 《冲天》是台湾2015年拍的一部纪录片，豆瓣评分9.2。没有慷慨激昂和洗脑宣传，只平平淡淡地讲述了抗日战争年代一群年轻飞行员的爱情与事业，令人泪流满面。如果有兴趣的话，可以去看一看高志航的故事。笕桥中央航空学校校门口石碑上立着：“我们的身体、飞机和炸弹，当与敌人兵舰阵地同归于尽！”与那个年代大多数参军的农民子弟不同，中央航空学校的毕业生们大多数家庭出身优渥，教育程度高，如果没有战争，他们也许将是各行各业的精英。可命运让他们生在了那个年代，成为了飞行员。《无问西东》中，王力宏演的沈光耀的人物原型，叫做沈崇诲。他就读著名的天津南开中学，18岁考入清华大学土木工程系，1932年毕业后不久放弃了在绥远舒适的工作，投考中央航校轰炸科，毕业后留校担任飞行教官。淞沪会战期间，1937年8月19日，沈崇诲驾机为地面日军炮火击中，难以返回，他决定与敌人同归于尽，瞄准了日军一艘战舰急速俯冲而下，撞舰牺牲，年仅26岁。而这艘被撞的日本军舰，就是本文开头所提到的历史同名战舰：出云。 因为笕桥空战，每年8月14日被定为空军节。“相思不断笕桥东”是《西子姑娘》里的歌词，是中华民国空军军歌之一。抗日战争中，中华民国飞行员的平均牺牲年龄是23岁。有关我们飞行员的故事很多，那些故事里，最多的一句话是：“某天，他一去就再也没有回来。” 希望人类不要再有战争，希望所有人能和自己的爱人永远在一起。]]></content>
      <categories>
        <category>木雁之间</category>
      </categories>
      <tags>
        <tag>Navy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH login without password]]></title>
    <url>%2Fblog%2F2018%2F02%2F07%2Fssh-login%2F</url>
    <content type="text"><![CDATA[本文介绍如何使用SSH进行远程登录以及设置SSH免密码登录。 SSH简介 SSH(Secure Shell)是一个用于计算机间的远程登录会话和其他网络服务的加密网络安全协议，由互联网工程任务组 IETF（Internet Engineering Task Force）的网络工作小組（Network Working Group）所制定。相对于传统的网络服务，如FTP、 POP等， SSH协议避免了使用明文传输数据，帐号和口令，可以有效防止信息泄漏和中间人攻击，也能够防止DNS欺骗和IP欺骗。其具体工作原理本文不作详述。 使用SSH登录远程主机 首先判断是否安装SSH服务。打开Terminal试试： 1$ ssh localhost 若显示connection refused, 则需要手动安装。 安装SSH服务： 通常使用的是openssh, 以Ubuntu为例。 本机： 1$ sudo apt-get install openssh-client 服务器： 1$ sudo apt-get install openssh-server 安装完成后确认开启SSH服务： 1$ ps -e|grep ssh 启动服务器端的SSH服务： 1$ sudo /etc/init.d/ssh start 更改默认端口号： 1$ sudo vim /etc/ssh/sshd_config 使用SSH登录： 本机输入以下命令登录远程服务器(例如ip地址为10.20.120.10的远程服务器)： 1$ ssh user@10.20.120.10 开启图形界面： 1$ ssh -Y user@10.20.120.10 即可使用远程服务器上的图形界面应用。 一般情况下，每次登录必须输入密码。在涉及大量文件传输操作时，频繁输入密码会十分不便。以下介绍如何免密码登录。 设置SSH免密码登录 创建公钥 1$ ssh-keygen -t rsa 默认设置即可，创建id_rsa（私钥）和id_rsa.pub（公钥）文件。以后再次操作记得先行备份。 上传公钥至服务器 1$ rsync -avPz ~/.ssh/id_rsa.pub user@server:~/.ssh/ Note: 如果服务器上无.ssh目录则先创建（mkdir ~/.ssh） 复制公钥至authorized_keys 1$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 更改文件及目录权限 12$ chmod 600 ~/.ssh/authorized_keys$ chmod 700 ~/.ssh 完成后即可不输入密码直接登录。Enjoy it!]]></content>
      <categories>
        <category>吴带当风</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2F2018%2F02%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new "My New Post" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>曹衣出水</category>
      </categories>
  </entry>
</search>
