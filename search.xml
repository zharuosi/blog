<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习笔记（5）理解支持向量机]]></title>
    <url>%2Fblog%2F2018%2F06%2F09%2Fmachine-learning-5%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. Support Vector Machine]]></content>
      <categories>
        <category>笔记篇之夙兴夜寐</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
        <tag>Support Vector Machine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Adobe Premiere视频剪辑入门]]></title>
    <url>%2Fblog%2F2018%2F06%2F07%2Fintroduction-to-premiere%2F</url>
    <content type="text"><![CDATA[本文介绍如何使用Adobe公司旗下的Premiere来剪辑视频。本文只介绍一些基本操作，仅供入门。其他多种多样的玩法以后有时间再慢慢摸索。 Premiere简介 首先简单介绍一下Premiere这个软件。以下摘自中文维基百科：Adobe Premiere Pro是由Adobe公司开发的非线性编辑的视频编辑软件。Premiere是Creative Suite套装的一部分，可用于图像设计、视频编辑与网页开发。Premiere Pro支持许多不同插件以加强其功能、增加额外的视频/声音效果及支持更多的文件格式。Premiere Pro能支持非常高的清晰度，可最高支持10,240x8,192的显示屏清晰度，以及高至32位的色深，可使用RGB和YUV颜色模型。在声音方面，能支持VST声音插件，以及5.1声道环绕立体声。Premiere Pro的插件能够导入与导出至QuickTime或DirectShow的格式。Premiere Pro也支持很多视频与声音格式，在导出和导入视频时，也提供很多编码解码器。通过使用Cineform Neo line的插件，就可以支持3D编辑功能，也可以在2D的显示屏上看见3D物料。Photoshop文件（psd档）可直接导入至Premiere Pro中。 注意， 当前版本仅支持64位操作系统。使用Premiere的另一个原因是其对wmv格式的视频文件支持很友好。如果出现无法打开的视频素材，请使用“格式工厂”进行转换。PS. 格式工厂是目前windows上我用过的最好的格式转换软件。 非线性编辑指的是对视频或音频的素材通过电脑设备或其他数字随机存取的方式剪辑，意味着能够对视频片段中的任意一帧进行操作。在最初电影剪接的过程中，电影底片必须被剪断。数字视频技术出现后，产生了非线性剪接手段。与过去需要两台以上的录像机，从不同的磁带合成到一盘磁带的线性编辑方式相比，能立即重新排列、替换、增加、删除、修改映像数据，以达到快速编辑的目标，并且理论上素材质量不会损失。 视频加速 我在工作中一个需求是对视频加速。原始视频文件是经过慢放处理的，市场一共接近三分钟，而我希望在演讲PPT时在10秒内放完，那我就需要加速至1800%。那么打开Premiere，进行如下操作。 新建项目。 导入视频文件。双击左下区域内的位置，选择所有要导入的素材即可。 新建轨道。右击图中1箭头所示的按钮即可新建轨道等。或者直接将左下区域内出现的视频文件拖至此按钮上。然后右下区域就会出现对应的文件和轨道，按行排列。上部为视频轨道，下部为音频轨道。左上区域可以对单个轨道进行编辑，右上区域可以预览叠加后的效果。拖动右下区域的红色竖线可预览不同时刻。 调整速度。在轨道上视频素材对应位置处右击可以看到很多操作，编辑“剪辑速度/持续时间”，改变数值可使视频加速播放或减速播放。此时可以看到对应轨道上的长度变化。 视频剪辑 将视频导入轨道之后，便可以对时间线进行删减。使用快捷键C，此时时间线会从你cut的地方断开，视频会变成两段，重新拖动便可改变顺序，或提取出你需要的部分重新进行拼接。使用快捷键D，可以删掉当前所在的片段。不使用波纹编辑的话，视频删掉的那部分会变成空白。使用波纹编辑，删掉的部分后续片段会自动衔接至上一片段。 合二为一 我的另一个需求是对两个视频一左一右进行对比，同时播放。于是不仅要调整播放速度使之每个时刻保持一致，还要调整每个视频的位置和比例。 效果控件。左上区域顶部有一排菜单，调整视频位置和比例需要用到其中的效果控件。 单击“运动”。然后转到右上区域，点击要调整的视频，可以看到视频四周出现了调整位置大小的控制点，拖动即可编辑。预览一下，看看是不是实现了同步播放。 调整背景。默认合成的视频背景色是黑色，我们可以调整为白色。有很多方法，这里介绍其中一个。在之前新建轨道的左下角那个按钮上点击，新建彩色蒙版。颜色改为白色。将该片段拖至所有视频轨道下方即可。 输出 输出编辑好的视频时，可以调整输出格式、位置等等。也可以添加滤镜、更改视频解码器等等。这里也可以选取输出的时间片段。拖动橘色的时间线，便可更改视频的起始位置。最后点击导出即可。 以后有其他需求再更新本文。]]></content>
      <categories>
        <category>工技篇之吴带当风</category>
      </categories>
      <tags>
        <tag>Post Processing</tag>
        <tag>Video Editing</tag>
        <tag>Premiere</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[企业的故事]]></title>
    <url>%2Fblog%2F2018%2F05%2F12%2FCV-Enterprise%2F</url>
    <content type="text"><![CDATA[引言 企业（Enterprise）可能是历史上最传奇的一个名字了，除了英法，仅美国共有多达9艘军舰和1架航天飞机以“Enterprise”命名，甚至星际迷航里的联邦星舰NCC-1701A也是。初次印象可能有些奇怪，为什么拿这么一个毫无特色的名词来给这么多舰船命名呢？其实，这是翻译的问题。本文就先从Enterprise的含义说起。 来看Webster’s Dictionary韦氏词典上Enterprise的解释： a project or undertaking that is especially difficult, complicated, or risky a unit of economic organization or activity, especially: a business organization readiness to engage in daring or difficult action 可以看到Enterprise有三层意思：第一，可以表示艰难、复杂、冒险的计划或项目。第二是企业的意思。第三，表示对艰巨的行动做好准备、充满信心、雄心勃勃，意译也就是进取、事业心、探险精神的意思。所以命名为Enterprise的本意是“进取”。例如，开头提到的航天飞机Space Shuttle OV-101也叫Enterprise，一般翻译成进取号。不过因为约定俗成,大家都统一叫企业了。 限于篇幅，本文主要介绍美国的第七艘和第八艘企业：分别是二战期间的航空母舰CV-6和现代航空母舰CVN-65。其中，CV-6在二战中表现极为突出，被美国人亲切地称为&quot;大E&quot;（The Big E）和&quot;幸运E&quot;（Lucky E）。由于日本曾多次单方面宣称企业被击沉，后来却又总是在战斗中遇见，所以也被日本人称为“灰色幽灵”（The Grey Ghost）。CVN-65则是全世界第一艘核动力航空母舰，上世纪六十年代的产物，至今仍远远领先于绝大多数国家。 企业CV-6 1933年，美国总统富兰克林·罗斯福发布了罗斯福新政，以工代赈。国会向海军提供高达两亿三千八百万美元的拨款，用于更新舰艇装备，本来只计划建造小型航空母舰的美国海军开始兴建新的大型航空母舰。编号为CV-6的航空母舰，作为约克城级的第二艘，于1936年10月3日下水，地点是在著名的航母摇篮：纽波特纽斯造船厂。1938年5月12日这艘当时还默默无闻的大船正式服役，直到二战胜利后的1947年2月17日正式退役。在她的一生之中，企业一共航行了442,475公里，击沉敌舰71艘，击破敌舰192艘，击毁敌机911架。整个太平洋战争中，企业一共获得20枚战斗之星，为美国海军舰艇之最。 时间回到1941年12月7日，日本帝国海军旗下六艘大型航空母舰组成的舰队，成功偷袭了美国珍珠港海军基地。在持续1个小时50分钟的轰炸后，美国海军共损失了8艘战列舰、3艘巡洋舰、3艘驱逐舰，188架战机，2402人死亡，1282人受伤。然而匪夷所思的是，美国在太平洋战区的三艘主力航空母舰（列克星敦、萨拉托加和企业）都不在家，幸免于难。企业号原定在前一日回珍珠港，但因风浪而有所延误。珍珠港袭击发生时，美国海军一共只有7艘航空母舰，除了珍珠港的3艘以外，2艘在大西洋（胡蜂和突击者），2艘在本土的诺福克（约克城和大黄蜂）。而彼时日本海军已建成服役10艘航空母舰：赤城，加贺、苍龙、飞龙、翔鹤、瑞鹤、凤翔、龙骧、祥凤和瑞凤。珍珠港事件后，抗日小分队约克城、胡蜂和大黄蜂开赴太平洋支援。值得一提的是，珍珠港事件使美国高层充分意识到，拥有舰载飞机的航空母舰在现代海战中具有极其可怕的破坏力，从此海军的建设重心开始向航空母舰倾斜。而日本则沉浸在珍珠港的胜利中不思进取，其高层仍旧认为战列舰才是大洋上决战的核心力量，航母只是用来偷袭、护航等等，无意中忽视了航空母舰的地位，直到1944年才幡然悔悟，然而为时已晚。有一个小插曲，那天企业的一架SBD轰炸机正飞向珍珠港福特岛海军基地，被友军当成了前来偷袭的日本飞机而击落，两名飞行员牺牲。 1942年2月，企业与约克城分别空袭马绍尔群岛及吉尔伯特群岛。企业炸沉了一艘运输船，并炸伤了几艘轻巡洋舰。2月24日企业空袭威克岛，战果甚微。 1942年4月18日，杜立特空袭东京。陆军的B-25轰炸机如果从珍珠港起飞前往东京，那航程肯定不足，怎么办呢？一位空军上将杜立特提出了一个疯狂的想法，那就是用航空母舰把这些大飞机运到日本附近海域，然后再从航母上起飞。可是航母甲板相对于陆地机场来说要短的多，于是B-25经过了魔改，起飞距离从3000英尺缩短到了800英尺，终于能从大黄蜂上起飞。此时大黄蜂的舰载机都被B-25所代替，所以毫无自卫能力，企业承担起舰队的护航任务。空袭行动意外地被日本渔船发现，为了避免全军覆没，B-25轰炸机只好提前起飞，大黄蜂和企业也迅速撤离战场。那起飞的轰炸机回不来了怎么办呢？当时他们被要求飞往中国迫降，其中很多飞行员被中国农民救助生还。杜立特空袭东京的成功可以说是奇迹。1993年杜立特去世，葬礼上，所有尚可飞行的B-25全部升空以示悼念。4月25日，企业返回珍珠港修整。 1942年5月8日，珊瑚海海战爆发，战况十分惨烈，双方各有损失。美国方面，“列太太”：列克星敦(Lady Lex)被击沉，约克城受损严重。日本方面，轻型航母祥凤被击沉，主力航母翔鹤也受重创，而另一艘主力航母瑞鹤的舰载机损耗严重。这对后续爆发的中途岛海战产生了消极影响。珍珠港之后太平洋战场所有战役中，企业只缺席了这一次。 1942年6月4日，中途岛海战爆发。日本海军由于翔鹤瑞鹤的缺席，只有四艘主力航母与美军进行对决。而由于日本的密码被美军破译，使得日本偷袭中途岛的计划被美军掌握的一清二楚。结果就变成了偷袭不成反被歼，美国以损失一艘大型航母的代价（约克城被击沉），全灭日本的四艘主力航母，其中，企业功不可没。那天上午，本应平均分配各个攻击编队的人数，但因企业指挥失误，导致超过27架轰炸机去猛揍了加贺，并很快送它到了太平洋海底，而只有3架轰炸机及时转向前往轰炸赤城，只有一枚炸弹击中。神奇的是该炸弹引爆了赤城机库的弹药，引发后续的连环爆炸。下午，约克城的侦察机找到了飞龙，企业及时起飞轰炸机编队，将飞龙击沉。 1942年5月，日军占领瓜达尔卡纳尔岛。8月7日，美军陆战队登陆图拉吉以及瓜岛部分地区，包括亨德森机场，开启瓜岛争夺战的序章。美军的三艘航空母舰胡蜂、萨拉托加与企业均参加了战斗，为陆战队提供空中支援。由于登陆顺利，美军第61特遣舰队司令弗莱彻在9日决定将三航母后撤以补充燃油。这项争议性的命令使美军陆战队失去了制空权，并间接导致萨沃岛海战中美军遭遇惨败。 1942年8月24日，东所罗门海战爆发，双方各有损失。美国海军萨拉托加击沉日本海军龙骧。企业号遭到数十架日本飞机集火，飞行甲板被贯穿，引发大火。企业严重受损，只好回到珍珠港大修。8月31日，萨拉托加也受到鱼雷攻击受损严重，同样回到珍珠港修理。 1942年9月15日，胡蜂号航母被日本海军潜艇伊19击沉。10月18日，尼米兹任命哈尔西为南太平洋战区总司令。10月23日，企业返回南太平洋战区。 1942年10月26日，圣克鲁斯群岛海战爆发。大黄蜂的轰炸机重创了翔鹤，而自己也被重创，不得已自沉。企业也受损严重，回到努美阿紧急维修。10月26日的早晨，远方的大黄蜂缓缓下沉，只剩一艘受损的企业孤军奋战。于是他们在企业的飞行甲板上拼出了那个著名的标语：企业vs日本（Enterprise vs Japan），在这样绝望的处境里鼓舞了所有美国人。日本方面，虽然没有航母沉沒，但是致命的是大量有经验的飞行员被消耗。至此，参加了珍珠港事件的765名日本精英飞行员中，至少有409人死亡。翔鹤被迫撤出一线战斗一直维修到了1943年。瑞鹤也因缺乏机组人员被迫返回日本本土。由于时间紧迫，11月11日，企业离开努美阿，再次前往瓜岛，此时抢修人员则继续在舰内赶工。11月14日，企业击沉了日本海军重巡洋舰衣笠和七艘运输船。瓜岛战役结束后，1943年5月8日，企业返抵珍珠港。5月27日企业因其优秀的表现，荣获美国总统集体嘉奖。此时，美国海军的新式航空母舰埃塞克斯级如下饺子般陆续服役。 时间回到1944年6月15日，美军在塞班岛登陆。随后6月19日爆发的菲律宾海海战，是历史上最大的航空母舰对决。美国海军第五舰队此时已拥有15艘航空母舰，而日本海军实力虽早已不如当年，已是困兽之斗，但东拼西凑也能找出9艘航空母舰。企业隶属于第五舰队第三支队，仅该支队就拥有企业、新列克星敦、普林斯顿、圣贾辛托四艘大型航空母舰。由于实力悬殊，日本大败，损失了3艘航空母舰和378架飞机。有趣的是，由于美军的飞机和弹药技术上均大幅领先日军飞机，导致空战变成了单方面的大屠杀，这一波128架来袭的日军飞机一共损毁97架。著名的马里亚纳射火鸡大赛（The Great Marianas Turkey Shoot）由此而来。神奇的是，夜间返航时美军飞机燃油耗尽争先恐后降落导致混乱，仅降落作业就损失了80架，而与日军作战而损失的飞机只有20架。日本的主力航空母舰翔鹤、大凤、飞鹰被击沉，仅存的瑞鹤被企业和约克城联合攻击而严重受损，一度下令弃船，后瑞鹤侥幸被拖回并修复。日本首相东条英机在此期间倒台。日本彻底丧失了西太平洋制海权，“绝对国防圈”遭突破，美国陆军的大型轰炸机B-29进驻马里亚纳，并使用凝固汽油弹对日本本土进行空袭。 1944年10月20日，麦克阿瑟宣布重返菲律宾，期间企业提供了有效的空中支援。此时，莱特湾海战已悄然爆发。共计21艘航空母舰、21艘战列舰、170艘驱逐舰与近2,000架军机参与了此次战斗，为人类历史上最大规模的海战。日本海军孤注一掷，然而被彻底摧毁。无望之下，这也是日本第一次发动神风特攻队自杀攻击。在恩加尼奥角附近，日本海军四艘航空母舰组成了诱饵编队，成功地钓到了大鱼：包括企业在内的美国海军主力航空母舰舰队，而日本最后的主力航空母舰瑞鹤被迫牺牲，被击沉。另一边，萨马岛附近，美国海军用于对地支援的第七舰队只有几艘护航航母和驱逐舰，遭遇了日本海军的主力战列舰编队。于是金凯德不断地给哈尔西发电报求援。连坐镇珍珠港的总司令尼米兹也给哈尔西发了一份简短的电报：“第34特混舰队在哪里？”，但负责电报加密的军官，随意添加了一句“全世界都想知道”，于是电文就变成了“全世界都想知道，第34特混舰队在哪里？”。太平洋上的译码人员误以为是正文未加删减，导致哈尔西十分生气，又耽误了很长时间。那边日本的主力舰队正开心地炮轰美军护航航母，尽管美军驱逐舰拼死骚扰以争取时间，但美军还是损失惨重。正在美国舰队苦苦挣扎的时候，日本司令栗田认为美军主力很快就要赶来，为了保存实力，于是日本主力开始向北撤退。金刚号在返航日本途中，在台湾海峡附近海域被美军潜艇击沉。至此，日本海军绝大部分已经消耗殆尽，仅剩一艘受伤的战列舰大和。 1945年2月19日，美国登陆硫磺岛，航空母舰萨拉托加前往掩护登陆部队。萨拉托加运气总不好，21日遭日本神风特攻队重创，被迫撤出。企业接替她的位置，加入护航航空母舰的编队。从2月23日到3月2日，企业上的飞机昼夜不间断地起降、飞行、执行任务，创下美国航母持续最长的飞行作业纪录。 时间回到1945年3月18日，冲绳战役期间，企业遭到日本轰炸机袭击，炸弹因投弹高度过低而没有引爆，后却被友军的高射炮击中而受损。由于维修而错过了最后围歼战舰大和的坊之岬海战。5月14日，企业遭遇了大批次神风自杀式攻击，舰艏飞行甲板严重扭曲，升降台彻底损毁，飞机无法启起飞，虽不至于沉没，但企业因此失去了作战能力，不得不返回美国普吉湾海军基地大修，直到战争结束。 1945年9月2日，东京湾，在美国最新的密苏里号战列舰上，日本向同盟国签字投降，中国代表徐永昌参加了签字仪式。密苏里州是美国时任总统杜鲁门的家乡。本来企业更有资格代表美国，但此时她仍在船坞中维修，遗憾地错过了东京湾的受降仪式。“在这庄严的仪式之后，我们将告别充满血腥屠杀的旧世界，迎来一个十分美好的世界，一个维护人类尊严的世界，一个致力于追求自由、宽容和正义的世界，这是我最热忱地希望，也是全人类的希望！”道格拉斯·麦克阿瑟说到。 战争结束了，输掉的自然已经灭亡，胜利者也是伤痕累累。太平洋上风云千樯，最后都永远地与这个古老的星球融为一体。 企业CVN-65 第七艘企业光荣退役以后，舰名由下一代航空母舰继承，也就是著名的CVN-65企业号核动力航空母舰。她的头衔可不少： 世界上第一艘核动力航空母舰 美国海军第一代核动力航空母舰 唯一一艘装备8座核反应堆的航空母舰 唯一一艘装备4片方向舵的航空母舰 当时世界上最大、最长的航空母舰 企业级核动力航母仅建造了一艘CVN-65 美国海军服役最长的航空母舰，一共服役了51年 同样诞生于美国航母的摇篮：纽波特纽斯造船厂。1958年2月4日开工，1960年9月24日下水。1961年11月25日企业CVN-65正式服役，12月编入美国大西洋舰队。1964年8月至10月，核动力航母企业、核动力巡洋舰长滩、核动力驱逐舰班布里奇组成“全核舰队”，，65天全程无补给航行了3万海里，绕地球一周，在那个年代堪称外星科技。当然，现在也没几个国家可以做到。 作为世界上第一艘核动力航空母舰，造价自然不菲，高达4.5亿美元（1958年）。满载排水量几乎是CV-6排水量的4倍。下表列出了两艘企业的数据对比。以下称CV-6为老企业，CVN-65为新企业。 技术数据 老企业CV-6 新企业CVN-65 标准排水量 19,900吨 75,700吨 满载排水量 25,600吨 94,000吨 船长 246.7米 342.3米 船宽 33.2米 40.5米 吃水 8.5米 11.9米 舰载机 97架 99架 升降台 3座 4座 弹射器 3座 4座 动力 9座Babcock&amp;Wilco锅炉 8座A2W核反应堆 主机 4座1000kw蒸汽轮机+2座200kw柴油轮机 16座2500kw发电轮机+4座1000kw柴油轮机 功率 120,000轴马力 280,000轴马力 极限航速 32.5节 33节 续航距离 12,000海里（15节） 无限 除了变大变强了以外，新企业又有哪些黑科技呢？首先，最重要的就是飞机弹射装置。以前，需要航母跑到极限航速，转向逆风，飞机从甲板末尾开始加速，才有可能起飞。有了弹射器的话，在飞机起飞的时候，航空母舰不需要再加速到极限，也不需要迎合风向，加速距离也大大减小，所以甲板上能摆更多的飞机。航空母舰的效率大大提升。进入喷气式飞机时代，弹射器更是大型航母发射重型舰载机的必备装备。 这张企业的老照片里，左边两个长条下面就是弹射器的轨道。因为弹射器的技术是绝密，即使是美国盟友也得不到技术转让，比如法国只能进口整个弹射器设备，英国干脆放弃了弹射器改为滑跃起飞。现代弹射器的技术非常复杂，即使是当年的苏联也没有研制成功。只有美国有这种逆天的黑科技，为了防止泄密，安装时都必须使用大棚覆盖。以前，活塞式飞机对弹射器要求很低，即使没有弹射装置也可以起飞。后来英国人发明了液压弹射器，但是技术并不成熟。但是喷气式飞机重量大大增加，对起飞速度要求很高。从越南战争起，美军航母上开始广泛使用蒸气弹射器。新企业上还装有内燃弹射器，不过性能并不令人满意，蒸汽弹射器还是主流。蒸汽弹射器的原理其实很简单，与小时候玩的弹弓类似。但为什么世界上绝大多数国家都造不出来呢？蒸汽弹射器的核心难点是气缸的制造和安装精度，还要频繁承受高压高温蒸汽压力，对缸体材料、制造工艺和维护保障的要求极高。其他国家不使用蒸汽弹射器的另一个原因是其能量转换效率极低，仅有6%，所以需要强大的动力才能保证蒸汽弹射器的运转。美国著名的C13型蒸汽弹射器，每次弹射就要消耗625公斤的蒸汽和1吨左右的缓冲淡水。如果航母每分钟弹射1架，那么连续弹射8架飞机之后，动力系统中蒸汽就会损失20%，动力输出随之损失32%，航速就要下降8节[2]。这就体现了核动力航空母舰的优势所在。美国最新的航空母舰上，蒸汽弹射器也已经被更为先进的电磁弹射器所取代，其便于维护，能量易于调节便于发射不同的飞机，功率损耗低寿命高，作战效率又会有一次飞跃。 第二个新技术是斜角甲板。对比老企业的甲板和新企业的甲板可以看到，老企业的甲板是直挺挺的，这种叫做全通甲板，易于建造。那它缺点也显而易见，所有飞机都大大咧咧摆在上面，起飞和降落不能同时进行，影响效率。有个英国人叫Dennis Campbell发明了斜角甲板，指一条和中线甲板呈角度的甲板跑道，其后部甲板更宽。这个角度一般是9度，在船长一定的情况下，采用斜角甲板能够拥有更长的跑道，并与船首的起飞跑道互不干扰。该设计可以保证同时进行飞机的起飞和降落，降落失败的飞机也可以再次加速复飞而不影响其他区域。斜角甲板技术表面上看让航空母舰变得更胖更丑了，但是却极大地提高了舰载机运转的效率，可以说是天才的发明。 原计划美国要建造6艘新企业级航空母舰，但因为当时核动力技术不成熟，动力系统成本及其昂贵导致它的造价远远超过预期。美国也没钱大手大脚，被迫改为建造常规动力的小鹰级，直到后来的尼米兹级核动力航空母舰的出现，动力系统日趋成熟使得成本下降，才得以大规模投入建造。由于单个反应堆的功率不高，只有35000轴马力，所以新企业上的A2W核反应堆多达8座，才能达到30节以上的高航速。后来技术成熟后，尼米兹号只需要2座A4W核反应堆。A代表航空母舰（Aircraft carrier），2和4分别代表第2代和第4代，W代表制造商西屋公司（Westinghouse）。核动力推进带来的是几乎无限的续航能力，秒天秒地。另一个冷知识是，新企业上没有烟囱，飞机起降不再会被浓烟迷住双眼。也不需要那些庞大的锅炉，节省下来的空间使航空燃油和航空炸弹的装载量大大增加，船员居住条件也更宽敞舒适。 说完了新企业的船体，再说说新企业的核心战斗力，舰载机。新企业常规搭载有20架F-14战斗机、60架F/A-18大战斗攻击机、4架EA-6B电子战飞机、4架E-2C预警机、8架S-3A/B反潜机。每一种飞机都可以单独写一篇介绍了。其中，F-14雄猫与企业一起参与了著名的电影“Top Gun” (中文译名壮志凌云）的拍摄，该电影由汤姆克鲁斯主演，当时是美国海军的征兵宣传片，俘获了一众猫迷的心。F-14最著名的是其AN/AWG-9长程雷达系统，和专为此雷达设计的AIM-54不死鸟空空导弹，能够同时追踪24个90km内的目标，并同时对其中6个目标进行攻击或栏截。这是宙斯盾系统出现之前，当时美军海军所拥有唯一的多目标同时作战系统。不死鸟导弹是全世界第一种主动雷达制导的空空导弹，有效射程高达184km。由于不死鸟导弹重量大、对雷达系统要求高，其他飞机无法使用。直到十七年之后的1991年，AIM-120先进中程空对空导弹服役后，美军才有其他战机有类似的多目标同时攻击能力。F-14第二著名的是其可变的机翼，机翼后掠角度可以由20°至68°之间变动，最大变动速度为每秒7°，由计算机自动控制。这项设计使机翼在任何高度与速度下都能达到最佳的升阻比，使得F-14有惊人的高速及转向性能。翼梁由钛合金制成，强度极高。在美国海军服役32年后，由于成本高昂以及维护复杂，F-14于2006年9月22日正式退役，由F/A-18E/F超级大黄蜂所取代。F/A-18也经常出现在电影中，比如变形金刚等大片里都有露脸。它是专门针对航空母舰起降而开发的对空、对地全天候多功能舰载机，虽然性能上不一定比得过传奇的F-14，但其胜任多用途、维护简单、成本低等特点，成为了航空母舰的最优选择。下图也是企业CVN-65最著名的系列照片之一，甲板上摆出了爱因斯坦著名的质能方程，彰显出其作为核动力航空母舰的骄傲。E既代表能量Energy，也代表企业Enterprise。甲板上密密麻麻的飞机中，大块头是著名的F-14D，机翼向上折叠的是/A-18系列。 至于新企业经历了各种大大小小的事件与冲突，比如著名的古巴导弹危机等等，由于牵扯到太多政治的恶臭，本文不再详述。 终章 如果只能选一个词来形容生命的伟大，那一定是enterprise。终有一天，地球上浩瀚的大洋也不再无垠，我们的征途将会是遥远的星河，地球只是故乡。正如《星际迷航》里联邦星舰NCC-1701A企业号所言：To boldly go where no man has gone before. 彩蛋 二战早期海战的主力还是战列舰，航空母舰只是个无足轻重的小角色，其肥大粗笨的船体和甲板，与型线流畅优美的战列舰比起来，可以说毫无颜值。因此航空母舰时常受到其他海军的嘲笑和不屑，被笑作“顶着舱盖的浴缸”。航母上的军官和水兵被嘲讽是“平顶水手”和“棕靴海军”。西奥多·梅森过去曾是战列舰BB-44加利福尼亚的船员，参加过护送企业CV-6驶往西海岸的行动。他退役后在回忆录中这样写道：“看着（企业）在汹涌波涛中上下起伏，我不禁觉得它就像是一个身形庞大、弯腰驼背的老头，被一大群保镖围着，那时候要是有人告诉我企业将会成为战争中最功勋卓著的战舰，我会毫不犹豫地嘲笑他是一个大傻瓜，依我之见，要把这艘船开到西海岸恐怕都会有麻烦。” 参考资料 [1] Enterprise vs Japan [2] 花费28年投32亿美元，让弹射器做到45秒弹射一架飞机]]></content>
      <categories>
        <category>考据篇之紫电青霜</category>
      </categories>
      <tags>
        <tag>Navy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（4）理解贝耶斯]]></title>
    <url>%2Fblog%2F2018%2F05%2F02%2Fmachine-learning-4%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. Bayesian estimation If you flip a coin, what’s the probability it will fall with the head up? To solve this problem, you may flip the coin many times. You will get the data: X={Xi}i=1n,Xi∈H,TX=\{X_i\}^n_{i=1}, X_i \in {H,T} X={X​i​​}​i=1​n​​,X​i​​∈H,T where HHH means Head and TTT means Tail. The probabilityshould be: P(Head)=θ,P(Tail)=1−θP(Head) = \theta, P(Tail) = 1 - \theta P(Head)=θ,P(Tail)=1−θ If you flip the coin 100 times, with 57 heads and 43 tails, the probability of P(Head)P (Head)P(Head) should be 57/100. Because the flips are i.i.d., which means: Independent events Identically distributed according to Bernoulli distribution (n=1n=1n=1 means BErnoulli distribution) Maximum likelihood estimation The aim of the maximum likelihood estimation is to choose θ\thetaθ that maximizes the probability of observed data. MLE of probability of head can be calculated based on i.i.d. as below: θ^MLE=argmaxθP(X∣θ)\hat{\theta}_{MLE} = \arg \max \limits_{\theta} P(X \mid \theta) ​θ​^​​​MLE​​=arg​θ​max​​P(X∣θ) =argmaxθ∏i=1nP(Xi∣θ)= \arg \max \limits_{\theta} \prod_{i=1}^n P(X_i \mid \theta) =arg​θ​max​​​i=1​∏​n​​P(X​i​​∣θ) =argmaxθ∏i:Xi=HαHθ∏i:Xi=TαT(1−θ)= \arg \max \limits_{\theta} \prod_{i:X_i=H}^{\alpha_H} \theta \prod_{i:X_i=T}^{\alpha_T} (1-\theta) =arg​θ​max​​​i:X​i​​=H​∏​α​H​​​​θ​i:X​i​​=T​∏​α​T​​​​(1−θ) =argmaxθθαH(1−θ)αT= \arg \max \limits_{\theta} \theta^{\alpha_H} (1-\theta)^{\alpha_T} =arg​θ​max​​θ​α​H​​​​(1−θ)​α​T​​​​ Let J(θ)=θαH(1−θ)αTJ(\theta)=\theta^{\alpha_H}(1-\theta)^{\alpha_T}J(θ)=θ​α​H​​​​(1−θ)​α​T​​​​. It is easy to find the θ\thetaθ that maximizes the probability of observed data: ∂J(θ)∂θ∣θ=θ^MLE=0\frac{\partial J(\theta)}{\partial \theta} \Big\vert_{\theta=\hat{\theta}_{MLE}} = 0 ​∂θ​​∂J(θ)​​​∣​∣​∣​​​θ=​θ​^​​​MLE​​​​=0 Thus the MLE of probability of head is: θ^MLE=αHαH+αT\hat{\theta}_{MLE} = \frac{\alpha_H}{\alpha_H+\alpha_T} ​θ​^​​​MLE​​=​α​H​​+α​T​​​​α​H​​​​ Prior and posterior If you flip a coin 5 times, it is possible that 5 heads are obtained. In this case, the probability of head will be 1.0 if you use the maximum likelihood estimation. However, it is crazy to consider that the result of fliping a coin is always head. Thus, we need to use prior probability to adjust the probability calculation. A prior probability distribution of an uncertain quantity is the probability distribution that would express one’s beliefs about this quantity before some evidence is taken into account. A prior can be determined from past information such as previous experiments, or according to some principle such as Jeffreys prior. A posterior probability distribution of an unknow quantity is the probability distribution treated as the conditional probability that is assigned after the relevant evidence or background is taken into account. In contrasts with the likelihood function P(X∣θ)P(X \mid \theta)P(X∣θ), the posterior probability is given as: P(θ∣X)=P(X∣θ)P(θ)P(X)P(\theta \mid X) = \frac{P(X \mid \theta)P(\theta)}{P(X)} P(θ∣X)=​P(X)​​P(X∣θ)P(θ)​​ where P(θ)P(\theta)P(θ) is probability density function (pdf) of the prior probability. Conjugate prior In Bayesian probability theory, if the posterior distributions P(θ∣X)P(\theta \mid X)P(θ∣X) are in the same probability distribution family as the prior probability distribution P(θ)P(\theta)P(θ), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function. When a family of conjugate priors exists, choosing a prior from that family simplifies posterior form. An example of the conjugate prior: In the problem of coin flipping, Likelihood is ~ binomial distribution: P(X∣θ)=(nαH)θαH(1−θ)αTP(X \mid \theta) = \left( \begin{matrix} n \\ \alpha_H \end{matrix} \right) \theta^{\alpha_H} (1-\theta)^{\alpha_T} P(X∣θ)=(​n​α​H​​​​)θ​α​H​​​​(1−θ)​α​T​​​​ =n!αH!(n−αH)!θαH(1−θ)αT= \frac{n!}{\alpha_H! (n-\alpha_H)!} \theta^{\alpha_H} (1-\theta)^{\alpha_T} =​α​H​​!(n−α​H​​)!​​n!​​θ​α​H​​​​(1−θ)​α​T​​​​ It means in nnn independent flipping trial, the probability of occurring αH\alpha_Hα​H​​ heads belongs to binomial distribution, with the probability of head is θ\thetaθ for every flipping. Prior is assumed as Beta distribution: P(θ)=θβH−1(1−θ)βT−1B(βH,βT)∼Beta(βH,βT)P(\theta) = \frac{\theta^{\beta_H-1} (1-\theta)^{\beta_T-1}}{B(\beta_H,\beta_T)} \sim Beta(\beta_H,\beta_T) P(θ)=​B(β​H​​,β​T​​)​​θ​β​H​​−1​​(1−θ)​β​T​​−1​​​​∼Beta(β​H​​,β​T​​) where B(βH,βT)=Γ(βH)Γ(βT)Γ(βH+βT)B(\beta_H,\beta_T)=\frac{\Gamma(\beta_H) \Gamma(\beta_T)}{\Gamma(\beta_H+\beta_T)}B(β​H​​,β​T​​)=​Γ(β​H​​+β​T​​)​​Γ(β​H​​)Γ(β​T​​)​​, Γ(z)\Gamma(z)Γ(z) is the gamma function as an extension of the factorial function, Γ(z)=∫0∞xz−1e−xdx\Gamma(z) = \int_0^{\infty} x^{z-1}e^{-x} dxΓ(z)=∫​0​∞​​x​z−1​​e​−x​​dx, and Γ(n)=(n−1)!\Gamma(n) = (n-1)!Γ(n)=(n−1)!, if nnn is a positive integer. Posterior is also Beta distribution: P(θ∣X)∼Beta(βH+αH,βT+αT)P(\theta \mid X) \sim Beta(\beta_H + \alpha_H,\beta_T + \alpha_T) P(θ∣X)∼Beta(β​H​​+α​H​​,β​T​​+α​T​​) For binomial distribution, the conjugate prior is Beta distribution. The pdf of Beta distribution got more concentrated as values of βH\beta_Hβ​H​​ and βT\beta_Tβ​T​​ increased. As more samples are obtained, βH+αH&gt;&gt;βH\beta_H + \alpha_H &gt;&gt; \beta_Hβ​H​​+α​H​​&gt;&gt;β​H​​, the effect of prior will be “washed out”. Another example of the conjugate prior: In the problem of rolling a dice, there are k=6k=6k=6 outcomes. Likelihood is ~ multinomial (θ=θ1,θ2,...,θk\theta={\theta_1,\theta_2,...,\theta_k}θ=θ​1​​,θ​2​​,...,θ​k​​) P(X∣θ)=n!α1!α2!...αk!∏i=1kθiαiP(X \mid \theta) = \frac{n!}{\alpha_1!\alpha_2!...\alpha_k!} \prod_{i=1}^k \theta_i^{\alpha_i} P(X∣θ)=​α​1​​!α​2​​!...α​k​​!​​n!​​​i=1​∏​k​​θ​i​α​i​​​​ The multinomial distribution is a generalization of the binomial distribution. In nnn independent rolling test, the probability of occurring α=α1,α2,...,αk\alpha={\alpha_1,\alpha_2,...,\alpha_k}α=α​1​​,α​2​​,...,α​k​​ times of the dice number α=X1,X2,...,Xk\alpha={X_1,X_2,...,X_k}α=X​1​​,X​2​​,...,X​k​​ belongs to multinomial distribution, with the probability of XkX_kX​k​​ is θk\theta_kθ​k​​ for every rolling. The sum of θk\theta_kθ​k​​ is 1 and the sum of αk\alpha_kα​k​​ is nnn. Prior is assumed as Dirichlet distribution: P(θ)=∏i=1kθiβi−1B(β1,β2,...,βk)∼Dir(β1,β2,...,βk)P(\theta) = \frac{\prod_{i=1}^k \theta_i^{\beta_i -1}}{B(\beta_1,\beta_2,...,\beta_k)} \sim Dir(\beta_1,\beta_2,...,\beta_k) P(θ)=​B(β​1​​,β​2​​,...,β​k​​)​​∏​i=1​k​​θ​i​β​i​​−1​​​​∼Dir(β​1​​,β​2​​,...,β​k​​) Posterior is also Dirichlet distribution: P(θ∣X)∼Dir(β1+α1,β2+α2,...,βk+αk)P(\theta \mid X) \sim Dir(\beta_1+\alpha_1,\beta_2+\alpha_2,...,\beta_k+\alpha_k) P(θ∣X)∼Dir(β​1​​+α​1​​,β​2​​+α​2​​,...,β​k​​+α​k​​) For multinomial, the conjugate prior is Dirichlet distribution. MLE compared with MAP: maximum a posterior estimation MLE: to choose θ\thetaθ that maximizes the likelihood probability (the probability of observed data) P(X \mid \theta). MAP: to choose θ\thetaθ that maximizes the posterior probability P(\theta \mid X). θ^MAP=argmaxθP(θ∣X)\hat{\theta}_{MAP} = \arg \max \limits_{\theta} P(\theta \mid X) ​θ​^​​​MAP​​=arg​θ​max​​P(θ∣X) =argmaxθP(X∣θ)P(θ)= \arg \max \limits_{\theta} P(X \mid \theta)P(\theta) =arg​θ​max​​P(X∣θ)P(θ) In the problem of flipping trial, the maximum a posterior estimation of probability of head is Beta distribution: P(θ∣X)∼Beta(βH+αH,βT+αT)P(\theta \mid X) \sim Beta(\beta_H+\alpha_H,\beta_T+\alpha_T) P(θ∣X)∼Beta(β​H​​+α​H​​,β​T​​+α​T​​) MLE vs MAP: θ^MLE=αHαH+αT\hat{\theta}_{MLE} = \frac{\alpha_H}{\alpha_H+\alpha_T} ​θ​^​​​MLE​​=​α​H​​+α​T​​​​α​H​​​​ θ^MAP=αH+βH−1βH+αH+βT+αT−2\hat{\theta}_{MAP} = \frac{\alpha_H + \beta_H - 1}{\beta_H+\alpha_H+\beta_T+\alpha_T-2} ​θ​^​​​MAP​​=​β​H​​+α​H​​+β​T​​+α​T​​−2​​α​H​​+β​H​​−1​​ For small sample size, the prior is important. For infinite nnn, prior will be useless. Therefore, when βH=βT=1\beta_H=\beta_T=1β​H​​=β​T​​=1, MAP will be same as MLE. We use Gaussian distribution for a continuous variable xxx which is i.i.d. The central limit theorem (CLT) establishes that, in most situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution The pdf is N(μ,σ2)N(\mu,\sigma^2)N(μ,σ​2​​): P(x∣μ,σ)=1σ2πe−(x−μ)22σ2P(x \mid \mu,\sigma) = \frac{1}{\sigma \sqrt{2\pi}} e^{\frac{-(x-\mu)^2}{2\sigma^2}} P(x∣μ,σ)=​σ√​2π​​​​​1​​e​​2σ​2​​​​−(x−μ)​2​​​​​​ where μ\muμ is the mean and σ2\sigma^2σ​2​​ is the variance. MLE probability for Gaussian distribution: θ^MLE=argmaxθP(X∣θ)\hat{\theta}_{MLE} = \arg \max \limits_{\theta} P(X \mid \theta) ​θ​^​​​MLE​​=arg​θ​max​​P(X∣θ) =argmaxθ∏i=1nP(Xi∣θ)= \arg \max \limits_{\theta} \prod_{i=1}^n P(X_i \mid \theta) =arg​θ​max​​​i=1​∏​n​​P(X​i​​∣θ) MLE for Gaussian mean and variance: μMLE=1n∑i=1nxi\mu_{MLE} = \frac{1}{n} \sum_{i=1}^n x_i μ​MLE​​=​n​​1​​​i=1​∑​n​​x​i​​ σMLE2=1n∑i=1n(xi−μ^)2\sigma_{MLE}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \hat{\mu})^2 σ​MLE​2​​=​n​​1​​​i=1​∑​n​​(x​i​​−​μ​^​​)​2​​ Here we use average value to estimate μ\muμ, which is not a true parameter. The estimation of σ\sigmaσ is under-estimated thus MLE for the variance of a Gaussian is biased. Because the average of x1,x2,...,xn{x_1,x_2,...,x_n}x​1​​,x​2​​,...,x​n​​ is used as μ^\hat{\mu}​μ​^​​, the true mean is not equal to the average of samples. If we use the average of samples to estimate the average of total, there will be a bias. For a unbiased variance estimator: σunbiased2=1n−1∑i=1n(xi−μ^)2\sigma_{unbiased}^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \hat{\mu})^2 σ​unbiased​2​​=​n−1​​1​​​i=1​∑​n​​(x​i​​−​μ​^​​)​2​​ Because only n−1n-1n−1 xix_ix​i​​ is independent with the average of samples. The nthn^{th}n​th​​ xix_ix​i​​ can be obtained by the average and other n−1n-1n−1 values, which includes no additional information. If we use different samples, the mean is also different. The prior for mean is also Gaussian distribution: P(μ∣η,λ)=1λ2πe−(μ−η)22λ2P(\mu \mid \eta,\lambda) = \frac{1}{\lambda\sqrt{2\pi}}e^{\frac{-(\mu-\eta)^2}{2\lambda^2}} P(μ∣η,λ)=​λ√​2π​​​​​1​​e​​2λ​2​​​​−(μ−η)​2​​​​​​ The conjugate prior for mean is Gaussian distribution and for variance is Wishart distribution. Bayes classifier Bayes Optimal Classifier Bayes optimal classifier is a kind of ensemble learning. In Bayesian learning, the primary question is: What is the most probable hypothesis give data? If we have: X: Features Y: Target classes How to decide which class yyy is for a new sample with feature xxx? (x∈Xx\in Xx∈X and y∈Yy\in Yy∈Y) The Bayes rule is: P(Y=y∣X=x)=P(X=x∣Y=y)P(Y=y)P(X=x)P(Y=y \mid X=x) = \frac{P(X=x \mid Y=y)P(Y=y)}{P(X=x)} P(Y=y∣X=x)=​P(X=x)​​P(X=x∣Y=y)P(Y=y)​​ According to the Bayes decision rule, we need to minimize the conditional risk of making a wrong classification of a sample. Becasue R(y∣x)=1−P(y∣x)R(y \mid x)=1-P(y \mid x)R(y∣x)=1−P(y∣x), the Bayes optimal classifier can be written as: h∗(x)=argmaxY=yP(Y=y∣X=x)h^*(x) = \arg \max \limits_{Y=y} P(Y=y \mid X=x) h​∗​​(x)=arg​Y=y​max​​P(Y=y∣X=x) As the denominator P(X=x)P(X=x)P(X=x) is fixed for call cases, the maximum can be derived as: h∗(x)=argmaxY=yP(X=x∣Y=y)P(Y=y)h^*(x) = \arg \max \limits_{Y=y} P(X=x \mid Y=y)P(Y=y) h​∗​​(x)=arg​Y=y​max​​P(X=x∣Y=y)P(Y=y) P(X=x∣Y=y)P(X=x \mid Y=y)P(X=x∣Y=y) is the class conditional density (likelihood) P(Y=y)P(Y=y)P(Y=y) is the class prior A binary classification and the optimal Bayes decision boundary is shown as follows. Conditional independence Given Z, X is conditionally independent of Y means: P(X,Y∣Z)=P(X∣Z)P(Y∣Z)P(X,Y \mid Z) = P(X \mid Z)P(Y \mid Z) P(X,Y∣Z)=P(X∣Z)P(Y∣Z) e.g. If the alarm in your house is active (Z), your neighbor Mary will call the police (X) or another neighbor John will call the police (Y). It is assumed that they do not communicate for each other. In this case, Mary’s calling and John’s calling are conditionally independent. It also means whether Mary’s calling the police is irrelated to John’s calling: P(X∣Y,Z)=P(X∣Z)P(X \mid Y,Z) = P(X \mid Z) P(X∣Y,Z)=P(X∣Z) Note: Independence ⇎\nLeftrightarrow⇎ Conditionally Independence X is conditionally independent of Y with a given Z, does not mean that, X is independent of Y. e.g. It is unknown whether Mary’s calling is independent of John’s calling. They can call the police for other cases other than the alarm in your house. On the contrary, X is independent of Y, does not result to, X is conditionally independent of Y with a given Z. e.g. X: rolling a dice, the number is 3. Y: rolling a dice, the number is 2. Z: rolling a dice two times, the sum of the two numbers is 5. In this case, X is not conditionally independent of Y with the given Z. Naive Bayes (NB) Naive Bayes Optimal Classifier assumes that the data is conditionally independent on the given class, which makes the computation more feasible. P(X1,...,Xd∣Y)=∏i=1dP(Xi∣Y)P(X_1,...,X_d \mid Y) = \prod_{i=1}^d P(X_i \mid Y) P(X​1​​,...,X​d​​∣Y)=​i=1​∏​d​​P(X​i​​∣Y) where ddd is the number of features. For each discrete feature, the likelihood is P(Xi∣Y)P(X_i \mid Y)P(X​i​​∣Y). If the class prior is $P(Y), the decision rule of Naive Bayes can be: hNB∗(x)=argmaxyP(x1,x2,...,xd∣y)P(y)h^*_{NB}(x) = \arg \max \limits_{y} P(x_1,x_2,...,x_d \mid y)P(y) h​NB​∗​​(x)=arg​y​max​​P(x​1​​,x​2​​,...,x​d​​∣y)P(y) =argmaxy∏i=1dP(xi∣y)P(y)= \arg \max \limits_{y} \prod_{i=1}^d P(x_i \mid y)P(y) =arg​y​max​​​i=1​∏​d​​P(x​i​​∣y)P(y) Actually, the features are not conditionally independent, then P(X1,...,Xd∣Y)≠∏i=1dP(Xi∣Y)P(X_1,...,X_d \mid Y) \neq \prod_{i=1}^d P(X_i \mid Y) P(X​1​​,...,X​d​​∣Y)≠​i=1​∏​d​​P(X​i​​∣Y) Nonetheless, Naive Bayes sometimes still performs well even when the assumption of conditionally independence is violated. If the training data is insufficient, e.g., there is no sample for a feature X1X_1X​1​​ in the class YYY, the likelihood will be zero. No matter what the values X2,...,XdX_2,...,X_dX​2​​,...,X​d​​ take, the probability keeps zero as: P(X1,...,Xd∣Y)=0P(X_1,...,X_d \mid Y) = 0 P(X​1​​,...,X​d​​∣Y)=0 If the features are continuous, we use Gaussian Naive Bayes (GNB) instead. Let Xi=xX_i = xX​i​​=x and Y=ykY=y_kY=y​k​​. The probability is: P(x∣yk)=1σik2πe−(x−μik)22σik2P(x \mid y_k) = \frac{1}{\sigma_{ik}\sqrt{2\pi}} e^{\frac{-(x-\mu_{ik})^2}{2\sigma_{ik}^2}} P(x∣y​k​​)=​σ​ik​​√​2π​​​​​1​​e​​2σ​ik​2​​​​−(x−μ​ik​​)​2​​​​​​ The subscript indicates the mean and variance for each class kkk and each feature iii are different. Reference [1] Some common probability distributions [2] Why use n−1n-1n−1 for unbiased variance estimation [3] Example of NB: Sex classification based on features including height, weight, and foot size]]></content>
      <categories>
        <category>笔记篇之夙兴夜寐</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
        <tag>Bayesian Learning</tag>
        <tag>Maximum Likelihood Estimation</tag>
        <tag>Bayes Optimal Classifier</tag>
        <tag>Naive Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉菲的故事]]></title>
    <url>%2Fblog%2F2018%2F04%2F18%2FDD-Laffey%2F</url>
    <content type="text"><![CDATA[引言 如果你喜欢葡萄酒，一定对拉菲很熟悉。这个法语中的拉菲Lafite，因为赌神的那句“来一瓶82年的拉菲”的著名台词梗火起来。回到1982年夏季的波尔多，气候炎热却不干燥，葡萄成熟较早，等到收成时均是干燥的晴天。在这种气候下收获的葡萄，品质十分完美。Robert M. Parker给出1982年的拉菲古堡干红100分的满分评价，果香浓郁，单宁柔和。2014年纽约春季拍卖会上，一箱82年的拉菲拍出了41,650美元的天价。 不过今天不讲葡萄酒，本文讲的是二战期间美国海军的故事。 拉菲，英文名Laffey。美国海军历史上有两艘船都叫拉菲。 初代拉菲 第一艘拉菲，是美国海军Benson级驱逐舰七番舰，舷号DD-459。以一名美国水手巴特利特·拉菲（1841–1901）的名字命名，文末有他的故事简介。 1940年6月14日，法国巴黎沦陷。 1940年7月19日，美国众议院以316：0全票通过通过了两洋舰队法案，以应对德国海军的威胁。 1941年1月13日，拉菲号驱逐舰开工建造，地点是旧金山的伯利恒钢铁厂。 1941年10月31日，举行下水仪式。 伯利恒钢铁Bethlehem Steel Corporation的子公司，伯利恒船舶工业公司，曾是美国最大的造船企业，总部位于宾夕法尼亚州的Bethlehem。这家成立于1857年的老牌企业，曾是美国第二大钢铁生产公司，于2003年倒闭，被出售给美国国际钢铁集团。后者被印度的米塔尔钢铁公司并购。下图是拉菲下水时的旧照，屁股上的459和舰名Laffey清晰可见，注意此时烟囱和火炮都尚未安装。密密麻麻的钢铁森林与薄雾海色，可一窥当年的工业时代。 1941年12月7日，日本偷袭珍珠港，太平洋战争正式爆发。 1942年3月31日，拉菲服役，舰长为威廉·E·汉克。海试后启程奔赴太平洋战场。 1942年6月4日，中途岛海战爆发，美国海军大获全胜。 1942年8月7日，瓜达尔卡纳尔岛战役爆发。 1942年8月28日，拉菲抵达瓦努阿图的埃法特岛，执行反潜作战任务。 1942年9月15日，第18特遣舰队航行至马基拉岛东南，胡蜂号航空母舰被日本伊19潜艇击沉，拉菲参与救援并安全撤离。 1942年10月11日，埃斯佩兰斯海角海战爆发。拉菲击破日军旗舰青叶，并配合美军旗舰旧金山击沉日军驱逐舰初雪。 1942年11月13日，瓜达尔卡纳尔岛海战第五次战役（日方称第三次所罗门海战）中，拉菲重创比叡，后寡不敌众沉没。 拉菲不到9个月的一生就像流星般短暂，共获得三枚战斗之星。在那个疯狂的年代，你我都是前仆后继，朝生暮死。 所罗门的战神 1942年11月11日，拉菲号加入第67特遣舰队水面打击群，执行护送运输船队的任务。13日，美军前往萨沃岛海域，拉菲担任舰队前卫。当天夜里，一轮新月无力地挂在海空，能见度极低，随后爆发了激烈而混乱的海战，双方船只难分敌我，纠缠在一起，事后蒙森号上的一名军官称之为“在酒吧里把灯打掉后的斗殴”。 拉菲率先攻击了彼时正拿着探照灯乱晃的日军晓号驱逐舰，导致其动力丧失，迅速被拉菲、奥班农、亚特兰大、旧金山、波特兰和海伦娜乱炮击沉。美军亚特兰大号巡洋舰被探照灯照亮，遭到了日军的集中火力攻击而大破，失去了动力随波逐流。 混乱中，拉菲突然发现左舷有个开着探照灯的巨大黑影向自己高速冲过来，正是日军旗舰，金刚级战列舰二番舰比叡，满载排水量32,000吨的怪物，而拉菲只有区区1,800吨。由于日军指挥不得其法，舰队阵型大乱，本应被护航的旗舰比叡反而冲到了最前面。拉菲立即规避并发射五枚鱼雷，其中两枚击中，不过因为引信过近未引爆。拉菲与比叡最近时仅相距10～20英尺，也就是差不多3到6米的距离，以至于比叡号的舰炮由于俯角不足竟然无法瞄准拉菲。拉菲抓住骑脸的机会，对准比叡的舰桥倾泻了所有的炮弹，连机枪也用上来进行炮击，摧毁了比叡的舰桥和指挥所，日军首席参谋铃木正金中佐获得成就：当场阵亡。比叡舰长西田正雄大佐和第11舰队司令阿部弘毅中将均身受重伤。比叡号既然无法射击驱逐舰，转而向仅仅2,300米远处的旧金山号射击。卡拉汉海军少将阵亡。 血洗比叡舰桥后，拉菲准备撤离。不过此时已经来不及，日军后续舰船已经跟上。拉菲右边是比叡的小妹雾岛：金刚级战列舰四番舰，左前方是日本驱逐舰小分队：村雨、五月雨、朝云，远处有巡洋舰长良虎视眈眈。混战中，拉菲首先摧毁了村雨的锅炉，逼村雨退出了战斗。可惜寡不敌众，拉菲受到集火，2～4号主炮炮塔相继被摧毁，但仍坚持用仅剩的1号炮塔英勇还击。距离渐渐拉开，比叡的主炮一轮齐射，一枚14英寸炮弹命中拉菲舰桥，拉菲的各仓室也纷纷中弹被毁。1时52分许，一枚（可能来自驱逐舰照月的）九三式鱼雷击中拉菲号左舷舰尾，剧烈的爆炸使其断成两截，随后弹药库发生殉爆，汉克舰长丧生，拉菲在烈火熊熊中沉没海底。拉菲号全舰247人中59人阵亡、116人受伤，幸存者乘救生挺登陆瓜岛获救。 这场混乱的夜战中，美军损失惨重。旗舰亚特兰大号轻巡洋舰被击沉，诺曼·斯科特少将战死，一共损失了两艘巡洋舰和四艘驱逐舰。日方损失了一艘重巡洋舰和两艘驱逐舰。不过美军的战略意图得以实现，挫败了日军当晚炮轰亨德森机场的企图，为瓜岛登陆部队争取了宝贵的一天时间。此后，由于拉菲无畏的进攻使得比叡号丧失了射击能力，以及一众日军高级参谋被消灭，日军只好放弃作战任务，撤出战区。13日上午4时20分，驱逐舰雪风到达战场…如果不清楚这意味这什么，请看上文雪风的故事。后面战事本文不再详述。日本舰队的结局是：13日天亮以后，在美军不断空袭中，比叡这艘天皇御昭舰被迫自沉。15日凌晨，雾岛号在与华盛顿号和南达科他号的交火中严重受损而沉没。可以说，拉菲在此战役中功不可没。 拉菲的英雄事迹极大地振奋了美国海军和民众的士气。各大报刊争相报道以鼓舞民心，南太平洋战区总司令哈尔西上将也致信嘉奖。威廉·E·汉克少校因其非凡勇气，被追授海军十字勋章加佩金星章。瓜达尔卡纳尔海战胜利后，罗斯福将美国总统集体嘉奖授予拉菲号的全体船员。一起欣赏一下嘉奖令全文： 美利坚合众国总统荣幸地将总统集体嘉奖授予 美国军舰拉菲号（DD-459） 以表彰下文所述事迹：兹嘉奖拉菲号驱逐舰在1942年9月 15日至11月13日于南太平洋战场中对抗日军的杰出表现。 她不顾附近仍有敌潜艇出没，毅然对落水船员伸出援手； 在埃斯佩兰斯海角海战中崭露头角后，她又成功击退敌机 空袭；在萨沃岛的决战中，尽管已遍体鳞伤且燃起烈火， 拉菲号仍对进犯之敌予以迎头痛击。在敌人仓皇撤退后， 不幸伤重沉没。她为我们树立了一个英勇不屈的光辉榜样。 为了纪念拉菲号和她的汉克舰长在瓜达尔卡纳尔海战中的无畏表现，1943年新建的两艘艾伦·萨姆纳级驱逐舰分别获名拉菲号 (DD-724)和汉克号 (DD-702)。DD-724就是下文要讲的二代拉菲的故事。 二代拉菲 二代拉菲终于有了一张合格的彩色照片。她坚持到了二战的胜利。现在还在美国南卡罗来纳州Mount Pleasant的爱国者地海军海事博物馆Patriots Point Naval &amp; Maritime Museum。背后的大屁股是USS Yorktown，约克城号航空母舰（CV-10）。这是第二艘名为约克城的航空母舰。为纪念在中途岛海战中沉没的约克城号航空母舰（CV-5），中途岛海战四个月后，美国把一艘正在建造的埃塞克斯级航母更名为约克城，这艘新约克城见证了二战的胜利。初代约克城和初代拉菲均在二战中英勇牺牲，在未来漫长的岁月里，二代拉菲将继续为二代约克城护航。 1943年6月28日，二代拉菲号驱逐舰开工建造，地点是缅因州的巴斯钢铁厂。 1943年11月21日，举行下水仪式。建造时间不到五个月。 1944年2月8日，正式服役。 巴斯钢铁厂简称BIW，于1844年由托马斯·W·海德建立，1995年被通用动力公司收购。巴斯钢铁被誉为美国海面战舰的摇篮，包括二战中著名的著名的弗莱彻级驱逐舰，治亚号战列舰。二战后，美军现役主力巡洋舰提康德罗加级，主力驱逐舰阿利·伯克级，以及下一代的主力：相当科幻的朱姆沃尔特级驱逐舰DDG-1000，都出生于此。 1944年6月6日，二代拉菲来到了英吉利海峡参加了著名的诺曼底登陆，炮击了德军岸防部队。行动中她的运气不错，仅挨了一发炮弹还是哑弹。 1945年4月12日，罗斯福总统去世。 1945年4月16日，二代拉菲遭神风特攻队自杀式飞机袭击。 二代拉菲是一艘幸运舰，有个小名叫The Ship That Would Not Die。当年一共有22架自杀飞机围攻，炸毁了舰上的火炮，并引起了弹药库的爆炸，舵机也遭损坏，舰尾进水下沉，死亡32人，71人负伤。二代拉菲的反击干掉了9架。据说当时的通讯官法兰克曼森少尉询问贝克顿舰长是否弃船, 贝克顿骂回去，“去你妈的，只要还有一门炮能开火老子就绝不会弃船（No! I’ll never abandon ship as long as a single gun will fire）”。但他几乎没有听见旁边有人轻声抗议，“除非我还能找到一个人来开火（And if I can find one man to fire it）”。最终由于美军出色的损管，二代拉菲奇迹般地没有沉没，经过修复，1945年10月继续服役。 1952年1月中旬，二代拉菲起航前往韩国，支援朝鲜战争，参与了长达861的元山封锁（Blockade of Wonsan）。 1975年3月9日，二代拉菲终于可以休息了，正式退役除籍。 二代拉菲在二战中获得了五枚战斗之星。不可思议的是，二代拉菲同样获得了总统集体嘉奖，创造了历史。 彩蛋 洛阳和汉阳 上文提到的初代拉菲属于美国海军Benson级驱逐舰，在1940-1943年期间美国一共建造了32艘Benson级驱逐舰。首舰Benson号，本森or班森，舷号DD-421，战斗到了二战的胜利。1946年退役并转为后备舰艇。五番舰Hilary P. Jones号，即希拉里·P·琼斯号，舷号DD-427。在一八五号公共法案下作为共同防卫援助方案的一部分，1954年这两艘驱逐舰被移交（借）给中华民国海军，并分别改名为洛阳号和汉阳号驱逐舰。1958年8月洛阳舰担任蒋介石座舰，巡视过金门及马祖。汉阳舰在服役过程表现优异，参与各项演训屡获第一，也经常担任总统座舰与高阶长官专送任务。 巴特利特·拉菲的故事 Bartlett Laffey参加了1861-1865年间的南北战争。那个时候，内河的船还是以船尾明轮当作推进，拉菲正是在一艘这样的炮艇USS Marmora上服役。1864年3月5日，南军对密西西比州的亚祖城发动猛烈袭击，马莫拉号炮艇在亚祖河口进行防御。战役当中，水手拉菲带了一队士兵登上河岸支援战斗，包括一门M1841式榴弹炮。敌军猛烈的火力摧毁了运输用的炮架，也破坏了它的装填装置。此时拉菲守岗位毫不退缩，守护了这门榴弹炮，并坚持反击，直到击退了南方联军的进攻。拉菲及其他两名水手因其勇敢的表现而被授予荣誉勋章（Medal of Honor）。荣誉勋章是由美国政府颁发的最高军事荣衔，授予那些“在战斗中冒生命危险，在义务之外表现出英勇无畏”的军人。历史也已证明，两艘拉菲都继承了他英勇无畏的精神，不辱其名。 所罗门的疯狗 瓜岛战役期间，日本守军资源奇缺，而运输船速度又慢又脆弱，所以日本海军常在夜间趁美军无法有效使用轰炸机的时候，以高速驱逐舰向瓜岛输送兵力及物资。日军自嘲这些专跑夜路的行动是老鼠输送，而美方称之为东京快车。日军总共派出350船次的驱逐舰，总共运输了约2万人以上。夕立是日本海军白露级驱逐舰四番舰，成功执行了多次惊心动魄的东京快车行动。夕立以大胆出名，曾经在1942年9月4日，夕立仅带领两艘驱逐舰突袭了美军舰队，击沉两艘美军运输舰，并炮击了亨德森机场。联合舰队参谋长宇垣缠在著作中将吉川舰长形容为“伟勋之士”，村雨的船员在回忆录中更是将夕立形容为“非常识”的“无法模仿”的船。 时间来到1942年11月12日的第三次所罗门海战，按照日本的说法，开战时夕立和春雨疯狂突进，导致整个美军舰队大乱。春雨发射鱼雷脱离战场之后，夕立独自冲进美军舰队，和美军船只混在一起。两舷火力全开，造成美军巨大伤亡，多艘船只燃起大火。由于寡不敌众，夕立的火力逐渐被打掉。疯狂的吉川舰长甚至命令船员“张帆”，用床单拼凑成的风帆向美军撞去。事后的夕立被日本人赞誉为阿修罗，所罗门的噩梦等，其战功被特记在第四水雷战队的战功详报功绩栏中。然而美军并没有记录到如此神勇的夕立，只记录到没头没脑的夕立冲过来，与日本舰队分隔开来，立即被美军炮火攻击。夕立升起表示投降的白旗，但随后却又再次向美军方开火，愤怒的波特兰立刻将其击沉。所以获得外号所罗门的疯狗。历史学家更倾向于美军的说法，认为是日本的造神运动以掩盖此次海战的失利和激励军队的士气。至于真相，只有永恒沉默的铁底湾才知道吧。 所罗门的鬼神 不像夕立的战果有水分，另一艘日本驱逐舰绫波号，则是实打实的表现神勇，战功卓著。绫波，熟悉的Ayanami，是吹雪型特型驱逐舰的第11艘，1930年4月30日服役。同样在第三次所罗门海战期间，在队友脱离编队前往支援其他舰队之后，绫波独自面对了美军2艘战列舰、4艘驱逐舰。结果是绫波击沉2艘驱逐舰，大破1艘驱逐舰，并导致一艘战列舰（南达科他）的电力中断。因此，在日军中绫波得到了黑豹、鬼神的称呼。随后绫波也被美军舰队集火攻击，发生爆炸。因为弹药库随时会殉爆，舰长下达全员退舰的命令，船员全部跳进海中，在漂流期间合唱军歌。大部分生还者被赶来的浦波救起。后因弹药库鱼雷爆炸后绫波逐渐沉入海底。 拉菲以及所罗门海战的故事就讲到这里了。古今多少事，都付笑谈中。 参考资料 [1] Wikipedia - USS Marmora [2] Wikipedia - Bartlett Laffey [3] Wikipedia - 拉菲号驱逐舰 [4] 碧蓝航线:拉菲 [5] 战舰少女:拉菲(DD-724) [6] 初代拉菲的美国总统集体嘉奖全文 [7] 二代拉菲的美国总统集体嘉奖全文 [8] 二代拉菲的推特主页]]></content>
      <categories>
        <category>考据篇之紫电青霜</category>
      </categories>
      <tags>
        <tag>Navy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解螺旋桨]]></title>
    <url>%2Fblog%2F2018%2F04%2F06%2Fpropeller-concepts%2F</url>
    <content type="text"><![CDATA[Introduction 螺旋桨是一种常见的推进装置，一般由桨毂和桨叶组成。在空气中旋转的螺旋桨可以为飞机提供动力，在水里旋转的螺旋桨可以推动船舶前进。本文介绍螺旋桨设计中的基本概念和流体力学原理。 Geometry Hub: The solid center disk that mates with the propeller shaft and to which the blades are attached. A smaller hub can lead to a larger thrust, however there is a tradeoff between size and strength. Blade: Twisted fins or foils that protrude from the propeller hub. The shape and the speed at which they are driven dictates the torque a given propeller can deliver. Higher diameter equates to higher efficiency for low speed vehicles (&lt;35kn). To obtain higher torque, the rpm (revolution per minute) should be reduced and the diameter should be increased. In high speed vessels, larger diameters lead to high drag. ZZZ is used to represent the number of blades. Here are basic nomenclatures to describe a blade section: Leading edge: the point at the front of the airfoil that has maximum curvature. Trailing edge: the point of maximum curvature at the rear of the airfoil. Meanline (camber): half distance along a section between the upper and lower surfaces. Quite often the meanline distribution is tabulated forms such as a NACA a=0.8 meanline, where a=0.8 means the meanline can create constant lift of 80% of the chord, then the lift drops linearly to zero at 100%. Chord ©: the nose-tail line, connecting the leading edge and trailing edge. Camber height (f): the distance between nose-tail line and meanline normal to chord. Thickness (t): the section thickness along a line normal to the meanline based on American convention. Thickness measured normal to the chord line is based on British convention. Angle of attack (AOA): the angle between a reference line on a body (often the chord line of an airfoil) and the vector representing the relative motion between the body and the fluid through which it is moving. The chord line of the root or the zero lift axis is often chosen as the reference line. Note that top pressure on the blade section is lower than the bottom pressure. The difference of the pressure leads to the lift force on the blade, thus thrust is generated. Face: the pressure face, high-pressure side, faces backwards and pushes the water. Back: the suction face, low-pressure side, faces upstream and towards the front of the vessel. Leading edge: the side cuts through the fluid. Trailing edge: the edge of the downstreams. Pitch: the axial distance advanced during one complete rotation of screw is called nominal pitch. The distance the ship is propelled forward in one propeller rotation is actually less than the pitch. The trace of the tip points on the blade is a helix. Pitch ratio is the ratio of pitch to diameter. Slip: The difference between the nominal pitch and the actual distance in one retation (s=pnt−VAts = pnt - V_Ats=pnt−V​A​​t). Propeller section: A circular arc section cut through the blade at some radius. We can expand it to 2-D foil section. Midchord line: the line produced from the midpoint of section nose tail line of each section along a blade. Rake: Axial distance from the midchord point at the hub section and the section of interest. skew angle: the angle between a radial line going through the hub section midchord point and a radial line through the midchord point of the section of interest AND projected. Performance Speed of advance: the propeller advances through the water at a speed of advance VAV_AV​A​​, which delivers a thrust TTT. When the speed of advance is zero, the efficiency is also zero, but the propeller still delivers thrust and absorbs power. Thrust power: PT=TVAP_T = T V_A P​T​​=TV​A​​ Effective power: PE=RVP_E = R V P​E​​=RV Wake: In general the water around the stern has acquired a forward motion in the same direction as the ship. This forward-moving water is called wake. The difference between the ship speed V and the speed of advance is the wake speed. The wake fraction is defined as: w=V−VAVw = \frac{V-V_A}{V} w=​V​​V−V​A​​​​ Wake is due to three principal causes: The frictional drag of the hull causes a following current towards the stern. The streamline flow past the hull causes an increased pressure around the stern. In this region the relative velocity of the water past the hull will be less than the ship’s speed. The ship forms a wave pattern on the surface of the water, and the water particles in the crests have a forward velocity due to the orbital motion, while in the troughs the orbital velocity is sternward. This wake will be positive or negative according to whether there is a crest or a trough of the wave in the vicinity of the propeller. Thrust deduction: The propeller close to the hull can induce a low pressure on the hull which increases its drag. The trust must be higher to overcome the additional drag. The thrust deduction coefficient is defined as: t=T−RRt = \frac{T-R}{R} t=​R​​T−R​​ where R is the tptal ship resistance and T is the propeller thrust. Here are the non-dimensional characterization of the propeller performance. Advance coefficient: J=VAnDJ = \frac{V_A}{nD} J=​nD​​V​A​​​​ Thrust coefficient: KT=T/(ρn2D4)K_T = T/(\rho n^2D^4) K​T​​=T/(ρn​2​​D​4​​) Torque coefficient: KQ=Q/(ρn2D5)K_Q = Q/(\rho n^2D^5) K​Q​​=Q/(ρn​2​​D​5​​) Propeller efficiency: η0=TVA2πnQ\eta_0 = \frac{T V_A}{2\pi nQ} η​0​​=​2πnQ​​TV​A​​​​ Propulsive efficiency: ηt=RV2πnQ≈1−t1−wη0\eta_t = \frac{R V}{2\pi nQ} \approx \frac{1-t}{1-w} \eta_0 η​t​​=​2πnQ​​RV​​≈​1−w​​1−t​​η​0​​ Because of the wake, the propulsive efficiency for a propeller can be greater than 1.0. It means the propeller reduced the ship resistance by taking advantage of its wake. When the local absolute pressure is less than local vapor pressure, cavitation occurs, generally on the suction side. The water can boil in a lower pressure (than atomstpheric pressure) condition even though the temperature is lower than 100∘C100^{\circ} C100​∘​​C. Cavitation number based on inflow velocity: σv=P−Pvap0.5ρAVA2\sigma_v = \frac{P - P_{vap}}{0.5\rho A V_A^2} σ​v​​=​0.5ρAV​A​2​​​​P−P​vap​​​​ Cavitation number based on propeller tip velocity: σND=P−PvapρN2D2\sigma_{ND} = \frac{P - P_{vap}}{\rho N^2 D^2} σ​ND​​=​ρN​2​​D​2​​​​P−P​vap​​​​ Cavitation causes a great deal of noise, damage to components, vibrations, and a loss of efficiency. The pitting caused by the collapse of cavities produces great wear on components. Highly localized collapses of cavities can erode metals over time. Reference [1] Wikipedia - Propeller [2] Wikipedia - Cavitation [3] MIT 2.016 Hydrodynamics [4] An introduction to propeller cavitation [5] Principles of Naval Architecture Volume II: Resistance, Propulsion and Vibration]]></content>
      <categories>
        <category>工技篇之曹衣出水</category>
      </categories>
      <tags>
        <tag>Principle of Naval Architecture</tag>
        <tag>Marine Hydrodynamics</tag>
        <tag>Propulsion</tag>
        <tag>Propeller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[雪风的故事]]></title>
    <url>%2Fblog%2F2018%2F03%2F25%2FDD-Yukikaze%2F</url>
    <content type="text"><![CDATA[引言 雪风是旧日本帝国在二战期间建造的一艘驱逐舰。她的一生充满传奇，作为战败国的一艘普通的驱逐舰，历经太平洋上大大小小超过十六次的海战而没有沉没，几无受损，被称作“奇跡の駆逐艦”、“呉の雪風 佐世保の時雨”、“不沈の航迹”等。关于雪风的文献很多：《激動の昭和 : 世界奇跡の駆逐艦 : 雪風》、《强运駆逐舰栄光の生涯》、《駆逐艦雪風―誇り高き不沈艦の生涯》等。 本文要讲的雪风的故事，从1938年开始，到1971年结束。 雪风的诅咒 雪风的诞生 1938年8月2日，第8艘阳炎型驱逐舰开工建造，地点是佐世保海军工厂。 1939年3月24日，举行下水及命名仪式。 1940年1月20日，雪风竣工并服役，母港为吴港。 佐世保海军工厂是日本重要的海军造船厂，赤城加贺两艘王牌航母在这里接受改装。明治22年（1889）佐世保镇守府设立，同时设有佐世保造船部。明治36年（1903）改为佐世保海军工厂。昭和36年（1961）更名为佐世保重工业株式会社佐世保造船所。佐世保港位于长崎县，其母港位于广岛县。是不是很耳熟？这两个县都是当时日本的重工业基地，战争末期遭原子弹轰炸。 雪风的诅咒有二，一是自己永不沉没，二是队友非沉即破。海军间流传雪风会将周围舰船的运气都吸光。也就是从那个遥远的昭和15年1月开始，雪风开始了它精彩而又祥瑞的一生。 雪风和它的朋友们 1941年12月12日，雪风初登场，作为支援参加了菲律宾的登陆作战。 1941年12月24日，雪风前往拉蒙湾，支援登陆作战。此战雪风轻微受损，接受维修舰明石的维修。 1942年1月4日，旗舰妙高号重巡洋舰带着第五战队的小学生们在达沃休息，昼ご饭を食べる，遭到美军B-17轰炸机轰炸。32枚600磅的航空炸弹从25000英尺的高空落下，本不指望能打中什么，谁能想到雪风当时正在旁边补充油料…这些瞎投的炸弹当中有一枚刚好钻进了妙高身体爆炸，妙高严重受损，三座前炮塔损毁，35名水兵和军官死亡。雪风自己由于指挥得当躲避所有的炸弹。 妙高：MMP... 雪风：？ 1942年3月3日，泗水北方海域，雪风击沉了美军白桦号潜艇。 白桦：出名了出名了！ 雪风：First Blood! 1942年6月4日，中途岛海战爆发。雪风作为运输船队的护卫并预备从事防空战。中途岛海战详情参考此链接。日本海军四艘主力航母、248架飞机、1艘重巡洋舰沉没，3000人以上战死。雪风及时撤退完好无损。 赤城：露落露消我太阁， 加贺：浪花之梦梦还多。 雪风：？ 1942年8月8日，第一次所罗门海战（萨沃岛海战）爆发，也是瓜达尔卡纳尔岛战役的开端。雪风执行运输与护送任务。 1942年10月26日，平静的海色被大破。圣克鲁斯群岛战役爆发，雪风执行护卫任务，舰队旗舰翔鹤大破，舰体严重受损，退出一线战场长达一年。 翔鹤：MMP... 雪风：？ 1942年11月12日夜晚，第三次所罗门海战（瓜达尔卡纳尔岛海战）爆发，海况恶劣，能见度极低，雪风遭到友军误射。舰队旗舰比叡（金刚级战列舰）开着九座探照灯就冲进美军舰队并被围殴。雪风的九妹驱逐舰天津风大破，比叡严重受损并自沉，司令官阿部弘毅转乘雪风。14日同型舰雾岛被击沉。自此四艘金刚舰同时沉了俩。12月28日，天皇批准日军从瓜岛撤退。 比叡：四十九年一睡梦， 雾岛：荣花一期酒一盅。 天津风：MMP... 雪风：？ 1943年2月1日开始执行ケ号作战（瓜达尔卡纳尔岛撤退）。每次海战均有战舰沉没或严重受损，例如，驱逐舰卷云沉没，舞风沉没，卷风大破，矶风大破。雪风毫发无损。 卷云：时之有限花吹散， 舞风：此心归于春山风。 卷风：MMP... 矶风：MMP... 雪风：？ 1943年3月2日，俾斯麦海海战爆发，日本海军被澳大利亚空军和美国空军袭击并重创，运输船队包括旭盛丸、建武丸、爱洋丸、神爱丸、太明丸、帝洋丸、大井川丸、野岛全军覆没。驱逐舰白雪、荒潮、朝潮、时津风被击沉，3000至5000名人员战死，损失物资2500吨。其中，被击沉的时津风也是雪风的姐妹舰，雪风依然毫发无损，顺便捞起了时津风的船员。 各种丸：人生五十年， 时津风：如梦亦如幻。 白雪： 有生斯有死， 二潮： 壮士何所憾。 雪风：？ 1943年7月6日至9日，改造后的雪风做了第八舰队（外南洋部队）的旗舰，驱逐舰做舰队旗舰可能是前无古人后无来者。 1943年7月12日，科隆班加拉岛海战爆发。夜战中雪风作战神勇，在克死旗舰神通（以及战队司令伊崎俊二）后击破了美军3艘巡洋舰。雪风也不幸被击中，然而炮弹竟然未爆炸。此战役以美军作战失败告终，但日军同样损失惨重，所谓的最精锐战队華の二水戦也如昙花一现。 神通：仅与金刚寺菩萨种的青松作一别。 雪风：？ 1943年7月20日，参与向科隆班加拉岛的输送作战。满月的夜间，船队受到美军轰炸，驱逐舰夕暮、清波被击沉，重巡洋舰熊野损毁严重。有流言认为夕暮成为了雪风的替死鬼。 夕暮：生于天地之清澈， 清波：归于本愿之清澄。 熊野：MMP... 雪风：？ 1944年1月10日起，雪风和姐妹舰天津风以及轻型航母千岁执行护卫运输船队任务。1月16日天津风被美军潜艇击中断成两截，司令战死，船头包括舰桥部分沉没，船尾在海上漂了一周被拖回越南，安装新船头后得以苟活。 1944年2月，由于时津风被击沉，初风被妙高撞沉，天津风也在大修，第十六驱逐队只剩雪风一艘了。雪风此时被编入第十七驱逐队，然而并不受大家欢迎，因为有流言称“雪風が十六駆で僚艦を全部食い尽くした”。 1944年5月14日，驱逐舰“电”被美军潜艇击中，雪风前往救援，电此时已完全沉没。5月18日，雪风触礁导致螺旋桨损坏。6月9日，谷风被美军潜艇击沉。第十七驱逐队从突袭珍珠港开始至此都没有战损，然而雪风加入之后，谷风成为第一艘被击沉的队员。此后雪风倍遭排挤。 谷风：吾心如那吹散云雾见明月的秋之晚风。 雪风：？ 1944年6月19日，史上最大的航空母舰对决，马里亚纳海战（菲律宾海海战）爆发。由于力量对比悬殊，当天的空战被美军戏称为“马里亚纳射火鸡大赛”。雪风没有受损，反而使用探照灯击落3架美军飞机。五航战的骄傲，航母翔鹤被击沉，傍晚时分大凤也追随翔鹤而去。 翔鹤：梅雨如露亦如泪， 大凤：杜鹃载吾名至云。 美军飞机：瞎了MMP... 雪风：令人窒息的操作 1944年10月20日，史上最大的海战，莱特湾海战爆发，双方舰船总吨位达到了206万吨。宇宙第一战列舰大和的姐妹舰武藏被击沉。雪风曾护航过的扶桑和山城先后被击沉。第二舰队司令栗田健男的旗舰爱宕号重巡洋舰被击沉。重巡洋舰摩耶被击沉。妙高、高雄、长门、金刚和榛名受重创，其中金刚与浦风在返航日本途中被美军潜艇击沉。战列舰大和接替旗舰位置。10月25日，“幸运の空母”瑞鹤也走到了终点，被美军飞机击沉。除了这些大船，还有12艘驱逐舰被击沉。雪风本计划救援重巡洋舰筑摩，结果大和发错了命令让雪风归队，让野分去救援。驱逐舰野分执行完救援任务后被美军击沉。 瑞鹤：迩来忧患集一身， 武藏：铁胄身躯今始破。 野分：我比窦娥还冤... 雪风：？ 1944年11月24日，雪风护卫长门返回横须贺港，然后执行新航母信浓的护航任务。信浓原本是大和级战列舰的三号舰，后被改为航空母舰。11月29日，信浓海试，刚出港口就被美军潜艇Archerfish发现并跟踪。护航的雪风误判其为渔船，导致信浓被美军潜艇击沉。雪风继续执行打捞工作，甲板上坐满信浓的船员安全返航。 信浓：吾身如筑摩江芦间点点灯火随之消逝。 雪风：？ 1945年4月6日，日本海军已经消耗殆尽，宇宙第一战列舰大和领衔的冲绳水上特攻作战开始，雪风和时雨为旗舰大和护航，结局可想而知。由于资源奇缺，大和出航的燃料来自于其他船舰油罐底部的剩油。4月7日，大和被400架飞机围殴后沉没。雪风的姐妹舰滨风被击沉。雪风也被击中，神奇的是炸弹没爆炸，鱼雷穿底而过。几乎无损的雪风又顺便捞起了大和的幸存者，再顺手送沉了严重损毁已不能行动的驱逐舰矶风。第十七驱逐队也被雪风吃掉只剩雪风一艘了。 大和：春樱秋枫留不住， 滨风：人去关卡亦成空。 雪风：？ 1945年8月15日，日本宣布投降，全体联合舰队的82艘驱逐舰只剩雪风。在整个太平洋战争中，雪风只有不到10名船员死亡，2 人失踪，四任舰长均得以善终。8月18日，雪风护送潜水母舰长鲸回到舞鹤港，途中意外触发水雷。屈服于雪风殿下的淫威，这颗水雷在雪风完全通过后才爆炸。8月26日雪风改为第一预备舰，解除武装后，9月15日改为特别输送舰并引渡给美军。 雪风的尾声 1945年10月5日，不沉战舰雪风退役。阳炎型姐妹舰一共18艘，只有雪风活到了战后。然而雪风的传奇还没结束。 1946年12月30日，雪风成为赔偿舰引渡至联合国。雪风的乘组员们直到最后时刻仍细心地整备。联合国方面感叹曰“战败国的军舰仍不理后事，细心地整备及保养舰只，真是令人惊叹”。 1947年7月3日雪风到达上海高昌庙码头，7月6日移交至中华民国。 1948年5月1日，雪风正式改名为“丹阳”舰，舷号12，成为中华民国海军旗舰。次日，常凯申就听到了刘邓大军渡过黄河开始千里跃进大别山的消息…此后内战开始攻守逆转，国民党方面美援断绝，丹阳被拖到台湾基隆。 1954年6月23日，苏联陶普斯号油轮在台湾海峡被丹阳舰拦截并押送返回高雄，获得大量航空汽油。1955年10月20日陶普斯被改名为会稽号运油舰，编入中华民国海军。主要任务为每个月往返基隆、高雄，给空军机场运送飞机燃油。1953年10月4日，中国和波兰合资成立的中波轮船股份公司所属的的布拉卡号油轮被丹阳舰拦截，羁押于高雄。1954年3月18日，中波轮船股份公司所属的哥德瓦尔特号远洋货轮在台湾东南海域遭丹阳舰炮击，被俘获后羁押于基隆。 1965年12月16日，由于船体机件老化等问题，丹阳舰降旗停役，1966年11月16日正式退役，1971年12月31日完成拆解。1971年10月25日联合国大会通过2758号决议，中华人民共和国政府依据此决议取得原由中华民国政府在联合国拥有的中国席位与代表权，而中华民国政府被驱逐出联合国… 有文章称雪风拆解的当天，委座遇意外车祸一病不起。经考据，委座这场意外车祸发生在1969年9月16日下午，距离雪风拆解还早。不过自从雪风拆解以后，委座身体健康日渐恶化…于1975年4月5号去世。 终章 丹阳舰除役后，日本人成立了雪风永久保存期成会，致力于促成“最後之日本海軍艦艇”归还日本。但在最后关头由于台风的关系而浸水严重受损，导致归还失败。 1971年12月8日，中华民国政府将雪风的舵轮与锚送还日本。舵轮收藏于江田岛的旧海军兵学校，而锚则在庭园中展示。1972年，得到了雪风的日本金融危机爆发，田中角荣内阁因被指金权政治而倒台…而在送走雪风以后，台湾的经济开始起飞… 结束了这传奇的一生后，雪风的两只螺旋桨仍留在台湾，一只被被台湾海军官校收藏，另一只现存于台湾成功岭军史公园。舰钟则安放在台湾左营军史馆。 蒋公：我有一句MMP一定要讲... TG：雪风如此多娇，引无数英雄竞折腰。 日本：祥瑞御免，家宅平安。 雪风：我们的战士，神圣的信仰，永远都不会磨灭，她照耀着我们每一个人，阳炎型8番舰の雪风向您报道。 冷静分析 雪风由于其不沉的奇迹，深受海军高层的重视。然而其他舰船上的水兵们却不这么认为。人人都把雪风看作是扫把星，会把友军的好运全部都吸干。雪风所在的第十六驱逐队消耗殆尽，自己毫发无损。所以最痛恨雪风的应该是她的亲姐妹。从开战至末期都完好无缺的第十七驱逐队，在雪风加入后又逐一沉没，战后只剩孤独的雪风。不过奇迹也好，诅咒也罢，很多时候都被过分解读了。除了雪风的运气好之外，其乘员素质高超，作战英勇，比一般驱逐舰都要优秀，这才是使雪风不沉奇迹的保障。战争总有伤亡和牺牲，雪风并非猥琐自保坑队友，而且出勤率极高，别的舰船被击沉了，又关雪风什么事呢？后期日美国力已不可同日而语，联合舰队的毁灭也是必然，雪风要为此背锅那也是很冤了。 很多时候人们会忽略了雪风的历任舰长和船员的高素质。1941年8月联合舰队最后一次划船赛中，雪风的轮机部门夺得了第一名。而且由于伤亡较少，出勤率高，船员很少更新，所谓好的越好，死的大都是新兵和菜鸟。经历的战役越多，老兵们的经验值也越来越高，就停靠码头这一基本功，技术差的舰长半天靠不上去，雪风每次都是一次成功。所以别的军舰很难躲避的攻击，雪风能够很简单地规避。另外，日本研发的新式海军装备，都先给雪风当试验品，雪风也就成为了日本海军第一艘装备雷达和声纳的驱逐舰。雪风的传奇舰长寺内正道曾是驱逐舰“电”的舰长，在各种伤亡惨重的战役中往往能全身而退。寺内正道曾自信的说，只要我在的船就不会沉，他的信条就是雪风上不能死人。寺内正道准确控制航速，油耗和燃油分配能够处于最佳状态。莱特湾海战中，回航期间驱逐舰的油箱都跑干了，需要冒着巨大危险停下来靠其他巡洋舰加油，只有雪风没有发生断油的状况。冲绳海战中，寺内正道直接站在椅子上，头伸出天井目视观测美军密集的空袭。此次作战大和特攻舰队全军覆没，而寺内正道“左脚踢航海长表示左舵，右脚踢航海长表示右舵”，带领雪风在有着“鉄の暴風”称号的冲绳海战中杀出了一条生路。丹阳舰时代，寺内正道也是雪风永久保存期成会的成员之一，并且参与了迎回雪风船锚的仪式。 无论如何，雪风是一艘优秀的驱逐舰，也遇到了优秀的舰长和船员，可惜投错了胎，认了日本军国主义作爹。最后来一张雪风的彩照，前主炮已经被拆除。 彩蛋 修正 很多文章包括中文维基百科均提到雪风号“于12月24日参与拉莫湾的登陆支援”，一查发现拉莫湾居然在南极，还在想雪风这是圣诞节开了时空传送到了南极？再一想不对啊，日本跑到南极登陆做什么，又没有发现第一使徒。这个拉莫湾一定还是在菲律宾海域。终于查到是“拉蒙湾”，即Lamon Bay，位于菲律宾吕宋岛东部。 名字含义 雪风是第八艘阳炎级驱逐舰，她的名字意思是混杂雪花的强风。阳炎（陽炎）也是一种气象现象。英语里叫Heat haze or heat shimmer。汉语里叫热霾，也就是天气炎热时空气中产生的雾，热浪滚滚，有点像（但不是）海市蜃楼的感觉。其他姐妹舰名字的含义： 天津风指的是吹过高天原的风（天つ风）。 时津风指的是顺应季节刮的风。 矶风指的是吹向海滩的风。 浦风即是海风。 滨风为沙滩上的风。 谷风是白天从山谷向山顶吹拂的风。 山风是夜晚从山顶向山谷吹拂的风。 海战精英——神通 神通号轻巡洋舰长期担任最精锐的第二水雷战队的旗舰。1943年7月的科隆班加拉岛海战中，面对绝对优势的盟军，神通开着探照灯就冲啊冲，冲入敌阵掩护旗下驱逐舰作战。神通以一敌三，承受了超过2000发炮弹，迅速被还原为零件状态。后人云：如融熔的铁水在燃烧。最终，烈火熊熊的神通在受到鱼雷攻击之后断为两截，后半截迅速沉没，在海面挣扎的前半截居然还在坚持炮击直到最后一刻。神通一战成名，被美国军史学家萨缪埃尔·莫里森评价为整个战争中作战最勇猛的日本军舰。 著名海战 瓜达尔卡纳尔群岛位于南太平洋，热带岛屿的风光如画，人际罕至，植物茂盛，昆虫肥大，疟疾横行，空气中充满腐烂的恶臭。发生在这里的瓜岛战役，是日本从战略优势走向劣势的转折点，也是美军在太平洋战略反攻的开始。1942年8月7日至1943年2月9日期间共进行了六次较大规模的海战，31000名日本士兵和7100名盟军士兵战死，38艘日本舰船和29艘盟军舰船长眠海底。瓜岛海战期间萨沃湾沉没的日美双方舰船如上图，由于太多飞机和船只沉没于此，后来人们也称此海域为铁底湾：Iron bottom sound。海底的钢铁经常会干扰途径船只罗盘。《细细的红线》这部詹姆斯·琼斯1961写的小说描述了这些血腥的战斗，并被改编成电影，1999年获德国国际电影节金熊奖，以及第71届奥斯卡最佳影片、最佳导演奖提名。这六次海战双方叫法不同，总结如下： 日军称呼 日文原文 盟军称呼 英语原文 第一次所罗门海战 第一次ソロモン海戦 萨沃岛海战 Battle of Savo Island 第二次所罗门海战 第二次ソロモン海戦 东所罗门海战 Battle of the Eastern Solomons 萨沃岛近海海战 サボ島沖海戦 埃斯佩兰斯海角海战 Battle of Cape Esperance 南太平洋海战 南太平洋海戦 圣克鲁斯群岛战役 Battle of the Santa Cruz Islands 第三次所罗门海战 第三次ソロモン海戦 瓜达尔卡纳尔海战 Naval Battle of Guadalcanal 伦加海夜战 ルンガ沖夜戦 塔萨法隆格海战 Battle of Tassafaronga 莱特湾海战是人类历史上最大的海战，也是世界上最后一次航空母舰+战列舰的对决，美日双方合计42艘主力舰加175艘驱逐舰参与了战斗，也是神风特攻队自杀式攻击的首秀，从1944年10月20日持续至10月26日。此时日军已到穷途末路，这175艘驱逐舰中，美军141艘，日本只有34艘。正规航空母舰日军只剩1艘，而美军有9艘，外加26艘轻型航母和护航航母。作战飞机美国有约1500架，日本只有200来架。图为1944年10月20日的道格拉斯麦克阿瑟登上莱特岛。 进击的跳弹 有趣的是，1943年3月俾斯麦海战中盟军空军B-25轰炸机使用了跳弹攻击。跳弹（？）指的是轰炸机低空高速投掷航空炸弹，打水漂般地击中对方舰船。这种战术最先由英国人发明，使用一种叫跳弹的炸弹成功地轰炸了德国鲁尔水坝。一般的航空炸弹为了减小阻力均被设计成流线型，试验发现流线型根本跳不起来啊，于是跳弹就被设计成了圆柱体，高速投下后可蹦蹦跳跳的漂出去几千米距离。轰炸机就可以提前逃走避免进入日军防空炮射程，挽救了无数飞行员的生命。太平洋上最活跃的B-24远程轰炸机，二战中产量高达18482架。作为参考，今天美国空军所有类型的飞机总量大约是5000架，天朝空军大约1700架。 本文资料主要来自于维基百科、萌娘百科和日本战国时代名人辞世诗。在此一并致谢。]]></content>
      <categories>
        <category>考据篇之紫电青霜</category>
      </categories>
      <tags>
        <tag>Navy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（3）理解分类]]></title>
    <url>%2Fblog%2F2018%2F03%2F15%2Fmachine-learning-3%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. Supervised Learning In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal), like SVM, regression, decision trees, naive Bayes, etc. Classification means the output takes discrete values. Three common classification methods are Logistic Regression, Fisher Linear Discriminant Analysis and Nearest Neighbor Classification. Logistic Regression In logistic regression, the dependent variable is categorical. Therefore, logistic regression is a classification model (with discrete labels). Logistic regression can be binomial, ordinal or multinomial. The binary logistic model is used to estimate the probability of a binary response based on one or more predictor (or independent) variables (features). The output for a dependent variable can have only two values, e.g., “0” and “1”. The conditional distribution y∣xy \mid xy∣x is a Bernoulli distribution rather than a Gaussian distribution. Fig. 1. The standard logistic function The model function of LR model is called the logistic function: g(z)=11+e−zg(z) = \frac{1}{1+e^{-z}} g(z)=​1+e​−z​​​​1​​ where zzz is defined as: z=θTx=θ0+θ1x1+...+θnxnz = \theta^T x = \theta_0 + \theta_1 x_1 + ... + \theta_n x_n z=θ​T​​x=θ​0​​+θ​1​​x​1​​+...+θ​n​​x​n​​ We can obtain the generalized linear model function parameterized by θ\thetaθ: hθ(x)=11+e−θTxh_{\theta}(x) = \frac{1}{1+e^{-\theta^T x}} h​θ​​(x)=​1+e​−θ​T​​x​​​​1​​ The probability of the dependent varible is: p(y=1∣x;θ)=hθ(x)p(y=1 \mid x;\theta) = h_{\theta}(x) p(y=1∣x;θ)=h​θ​​(x) p(y=0∣x;θ)=1−hθ(x)p(y=0 \mid x;\theta) = 1 - h_{\theta}(x) p(y=0∣x;θ)=1−h​θ​​(x) p(y∣x;θ)=(hθ(x))y(1−hθ(x))1−yp(y \mid x;\theta) = (h_{\theta}(x))^y (1 - h_{\theta}(x))^{1-y} p(y∣x;θ)=(h​θ​​(x))​y​​(1−h​θ​​(x))​1−y​​ Maximum likehood estimation(MLE) is used to find the parameter values that maximize the likelihood function with the given observations. Define the likehood: L(θ)=p(y⃗∣X;θ)=(hθ(x))y(1−hθ(x))1−yL(\theta) = p(\vec{y} \mid X;\theta) = (h_{\theta}(x))^y (1 - h_{\theta}(x))^{1-y} L(θ)=p(​y​⃗​​∣X;θ)=(h​θ​​(x))​y​​(1−h​θ​​(x))​1−y​​ =∏i=1m((hθ(x(i)))y(i)(1−hθ(x(i)))1−y(i)= \prod_{i=1}^m ((h_{\theta}(x^{(i)}))^{y^{(i)}} (1 - h_{\theta}(x^{(i)}))^{1-y^{(i)}} =​i=1​∏​m​​((h​θ​​(x​(i)​​))​y​(i)​​​​(1−h​θ​​(x​(i)​​))​1−y​(i)​​​​ In practice, it is often convenient to use the log-likelihood (natural logarithm): logL(θ)=∑i=1my(i)loghθ(x(i))+(1−y(i))log(1−hθ(x(i)))\log L(\theta) = \sum_{i=1}^m y^{(i)}\log h_{\theta}(x^{(i)}) + (1-y^{(i)})\log(1 - h_{\theta}(x^{(i)})) logL(θ)=​i=1​∑​m​​y​(i)​​logh​θ​​(x​(i)​​)+(1−y​(i)​​)log(1−h​θ​​(x​(i)​​)) A log-likelihood can be interpreted as a measure of “encoding length” - the number of bits you expect to spend to encode this information, which is called cross-entropy. The problem is to find the independent variable at which the function values are maximized: argmaxθ∑i=1my(i)loghθ(x(i))+(1−y(i))log(1−hθ(x(i))){argmax}_{\theta} \sum_{i=1}^m y^{(i)}\log h_{\theta}(x^{(i)}) + (1-y^{(i)})\log(1 - h_{\theta}(x^{(i)})) argmax​θ​​​i=1​∑​m​​y​(i)​​logh​θ​​(x​(i)​​)+(1−y​(i)​​)log(1−h​θ​​(x​(i)​​)) By derivation, we can find the θ\thetaθ. there are some optimization methods: Gradient ascent (to find a local maximum) Newton method Conjugate gradient etc. Gradient ascent Apply a gradient ascent algorithm for every jjj: θj:=θj+α∂∂θjlogL(θ)\theta_j:= \theta_j + \alpha \frac{\partial}{\partial \theta_j} \log L(\theta) θ​j​​:=θ​j​​+α​∂θ​j​​​​∂​​logL(θ) where +++ means the direction is the same as the gradient. From the definition of g(z)g(z)g(z), we have: ∂g(z)∂z=g(z)(1−g(z))\frac{\partial g(z)}{\partial z} = g(z) (1 - g(z)) ​∂z​​∂g(z)​​=g(z)(1−g(z)) ∂hθ(x(i))∂θj=hθ(x(i))(1−hθ(x(i)))xj\frac{\partial h_{\theta}(x^{(i)})}{\partial \theta_j} = h_{\theta}(x^{(i)}) (1 - h_{\theta}(x^{(i)})) x_j ​∂θ​j​​​​∂h​θ​​(x​(i)​​)​​=h​θ​​(x​(i)​​)(1−h​θ​​(x​(i)​​))x​j​​ ∂logL(θ)∂θj=∑i=1m(y(i)−hθ(x(i)))xj\frac{\partial \log L(\theta)}{\partial \theta_j} = \sum_{i=1}^m (y^{(i)} - h_{\theta}(x^{(i)}))x_j ​∂θ​j​​​​∂logL(θ)​​=​i=1​∑​m​​(y​(i)​​−h​θ​​(x​(i)​​))x​j​​ Thus, batch gradient ascent method: θj:=θj+α∑i=1m(y(i)−hθ(x(i)))xj\theta_j:= \theta_j + \alpha \sum_{i=1}^m (y^{(i)} - h_{\theta}(x^{(i)})) x_j θ​j​​:=θ​j​​+α​i=1​∑​m​​(y​(i)​​−h​θ​​(x​(i)​​))x​j​​ stochastic gradient ascent method: θj:=θj+α(y(i)−hθ(x(i)))xj\theta_j:= \theta_j + \alpha (y^{(i)} - h_{\theta}(x^{(i)})) x_j θ​j​​:=θ​j​​+α(y​(i)​​−h​θ​​(x​(i)​​))x​j​​ where iii is a randomly chosen sample. Then we can repeat choosing the sample and update the gradient and weights. Newton’s method (Newton-Raphson method) In numerical analysis, Newton’s method is a iterative method for finding successively better approximations to the solution xxx of a real-valued function in the form of f(x)=0f(x) = 0f(x)=0 based on Taylor expansion. The process is repeated as: θk+1:=θk−f(θk)f′(θk)\theta^{k+1}:=\theta^k - \frac{f(\theta^k)}{f&#x27;(\theta^k)} θ​k+1​​:=θ​k​​−​f​′​​(θ​k​​)​​f(θ​k​​)​​ Now we need to solve ∇logL(θ)=0\nabla \log L(\theta) = 0∇logL(θ)=0, the process can be written as: θk+1:=θk−H−1∇θlogL(θ)\theta^{k+1}:=\theta^k - H^{-1} \nabla_{\theta} \log L(\theta) θ​k+1​​:=θ​k​​−H​−1​​∇​θ​​logL(θ) where HHH is the Hessian matrix with n+1n+1n+1 by n+1n+1n+1, whose entries are given by: Hij=∂2logL(θ)∂θi∂θjH_{ij} = \frac{\partial^2 \log L(\theta)}{\partial \theta_i \partial \theta_j} H​ij​​=​∂θ​i​​∂θ​j​​​​∂​2​​logL(θ)​​ The problem is: (why 1/m?) argminθJ(θ)=1m∑i=1m−y(i)loghθ(x(i))−(1−y(i))log(1−hθ(x(i)))argmin_{\theta} J(\theta) = \frac{1}{m} \sum_{i=1}^m -y^{(i)}\log h_{\theta}(x^{(i)}) - (1-y^{(i)})\log(1 - h_{\theta}(x^{(i)})) argmin​θ​​J(θ)=​m​​1​​​i=1​∑​m​​−y​(i)​​logh​θ​​(x​(i)​​)−(1−y​(i)​​)log(1−h​θ​​(x​(i)​​)) The Newton’s method is: θ(t+1)=θ(t)−H−1∇J(θ(t))\theta^{(t+1)} = \theta^{(t)} - H^{-1} \nabla J(\theta^{(t)}) θ​(t+1)​​=θ​(t)​​−H​−1​​∇J(θ​(t)​​) where the gradient of the cost function is: ∇J(θ)=1m∑i=1m(hθ(x(i))−y(i))xj\nabla J(\theta) = \frac{1}{m} \sum_{i=1}^m (h_{\theta} (x^{(i)}) - y^{(i)} )x_j ∇J(θ)=​m​​1​​​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)x​j​​ and the Hessian Matrix is: H=1m∑i=1mhθ(x(i))(1−hθ(x(i)))x(i)(x(i))TH = \frac{1}{m} \sum_{i=1}^m h_{\theta}(x^{(i)})( 1 - h_{\theta}(x^{(i)}) )x^{(i)}(x^{(i)})^T H=​m​​1​​​i=1​∑​m​​h​θ​​(x​(i)​​)(1−h​θ​​(x​(i)​​))x​(i)​​(x​(i)​​)​T​​ (H is a matrix, where is jjj?) Logistic regression has a linear decision boundary (hyperplane). Linear discriminant analysis (LDA) Linear discriminant analysis (LDA), which is also known as Fisher’s linear discriminant, is a simple way for dimensionality reduction in linear classification. By LDA, we can find a linear combination of features that classifies objects in two or more groups. Compared with logistic regression, LDA assumes that the independent variables are normally distributed (Gaussian distribution). Logistic regression is more preferable in applications when the assumption is not valid. Compared with analysis of variance (ANOVA), ANOVA uses categorical independent variables and a continuous dependent variable while LDA adopts continuous independent variables and a categorical dependent variable. Compared with principle component analysis (PCA), PCA is an unsupervised learning method, which does not take into account any difference in class, only for dimensionality reduction. LDA attemps to model the difference between the classes of data using labeled data. Compared with factor analysis, factor analysis builds the feature combinations based on differences. LDA deals with similarities. LDA is a dependence technique. A dependence method is one in which a variable of set of variables is identifies as the dependent variable to be predicted or explained by other, independent variables. Dependence techniques include multiple regression analysis, discriminant analysis, and conjoint analysis. An interdependence method is one in which no single variable or group of variables is defined as being independent or dependent. The goal of interdependence methods is data reduction, or grouping things together. Cluster analysis, factor analysis, and multidimensional scaling are the most commonly used interdependence methods. Consider the nnn dimensional input vector xxx and project it down to one dimension: y=wTxy = w^Tx y=w​T​​x where www is the weight vector for linear transformation. Thus we can place a threshold on yyy and then classify yyy as C1/C2C_1/C_2C​1​​/C​2​​. The dimensionality reduction leads to a considerable loss of information, and classes may become strongly overlapping in the low dimensional space. The goal is to find a projection that maximizes the class separation by adjusting www. Therefore, we need to maxmimize the between-class scatter (use difference of mean values) and minimize the within-class scatter (use covariance matrix). Fig. 1. Comparison of the good projection and the bad projection in LDA Algorithm of LDA Two-class cases The purpose of LDA considers maximizing the “Rayleigh quotient”: J(w)=wTSBwwTSWwJ(w) = \frac{w^T S_B w}{w^T S_W w} J(w)=​w​T​​S​W​​w​​w​T​​S​B​​w​​ where SBS_BS​B​​ is the between class scatter matrix and SWS_WS​W​​ is the within class scatter matrix. the means of every class is: μ1=1N1∑x∈c1x\mu_1 = \frac{1}{N_1} \sum_{x\in c_1} \mathbf{x} μ​1​​=​N​1​​​​1​​​x∈c​1​​​∑​​x μ2=1N2∑x∈c2x\mu_2 = \frac{1}{N_2} \sum_{x\in c_2} \mathbf{x} μ​2​​=​N​2​​​​1​​​x∈c​2​​​∑​​x μ=1N1+N2∑x∈allx\mu = \frac{1}{N_1+N_2} \sum_{x\in all} \mathbf{x} μ=​N​1​​+N​2​​​​1​​​x∈all​∑​​x where xk(k=1,..,m)x_k (k=1,..,m)x​k​​(k=1,..,m) is a vector in Xm×nX_{m\times n}X​m×n​​. The scatter matrix for every class is defined as S1S_1S​1​​ and S2S_2S​2​​: S1=∑x∈c1(x−μ1)(x−μi)TS_1 = \sum_{x\in c_1} (x - \mu_1) (x - \mu_i)^T S​1​​=​x∈c​1​​​∑​​(x−μ​1​​)(x−μ​i​​)​T​​ S2=∑x∈c2(x−μ2)(x−μ2)TS_2 = \sum_{x\in c_2} (x - \mu_2) (x - \mu_2)^T S​2​​=​x∈c​2​​​∑​​(x−μ​2​​)(x−μ​2​​)​T​​ SW=S1+S2S_W = S_1 + S_2 S​W​​=S​1​​+S​2​​ SB=N1(μ1−μ)(μ1−μ)T+N2(μ2−μ)(μ2−μ)T=(μ2−μ1)(μ2−μ1)TS_B = N_1(\mu_1 - \mu) (\mu_1 - \mu)^T + N_2(\mu_2 - \mu) (\mu_2 - \mu)^T = (\mu_2 - \mu_1) (\mu_2 - \mu_1)^T S​B​​=N​1​​(μ​1​​−μ)(μ​1​​−μ)​T​​+N​2​​(μ​2​​−μ)(μ​2​​−μ)​T​​=(μ​2​​−μ​1​​)(μ​2​​−μ​1​​)​T​​ The solution to maximize the J(w)J(w)J(w) can be obtained by derivation and Lagrange multiplier. Let ∣∣wTSww∣∣=1||w^T S_w w||=1∣∣w​T​​S​w​​w∣∣=1 to find one solution for www. ∇wc(w)=∇w(wTSBw−λ(wTSww−1))=0\nabla_w c(w) = \nabla_w (w^T S_B w - \lambda (w^T S_w w - 1)) = 0 ∇​w​​c(w)=∇​w​​(w​T​​S​B​​w−λ(w​T​​S​w​​w−1))=0 The Fisher linear discrimination can be obtained as: Sw−1SBw=λwS_w^{-1} S_B w = \lambda w S​w​−1​​S​B​​w=λw Thus www is the eigenvector of the matrix Sw−1SBS_w^{-1} S_BS​w​−1​​S​B​​. Solution is based on solving a generalized eigenvalue problem. In two-class case, www is calculated as: w=Sw−1(μ2−μ1)w = S_w^{-1} (\mu_2 - \mu_1) w=S​w​−1​​(μ​2​​−μ​1​​) Multi-class cases The purpose is to maximize the “Rayleigh quotient”: J(w)=∣ATSBA∣∣ATSWA∣J(w) = \frac{\mid A^T S_B A \mid}{\mid A^T S_W A \mid} J(w)=​∣A​T​​S​W​​A∣​​∣A​T​​S​B​​A∣​​ where AAA is the projection matrix. μj=1Nj∑x∈cjx\mu_j = \frac{1}{N_j} \sum_{x\in c_j} \mathbf{x} μ​j​​=​N​j​​​​1​​​x∈c​j​​​∑​​x μ=1∑j=1CNj∑x∈allx\mu = \frac{1}{\sum_{j=1}^C N_j} \sum_{x\in all} \mathbf{x} μ=​∑​j=1​C​​N​j​​​​1​​​x∈all​∑​​x where cjc_jc​j​​ means the group of class jjj. CCC is the number of classes. NjN_jN​j​​ is the sample number in the class cjc_jc​j​​. The scatter matrix is generalized as: Sj=∑x∈cj(x−μj)(x−μj)TS_j = \sum_{x\in c_j} (x - \mu_j) (x - \mu_j)^T S​j​​=​x∈c​j​​​∑​​(x−μ​j​​)(x−μ​j​​)​T​​ SW=∑j=1CSj=∑j=1C∑xi∈c=j(xi−μc=j)(xi−μc=j)TS_W = \sum_{j=1}^C S_j = \sum_{j=1}^C \sum_{x_i\in c=j} (x_i - \mu_{c=j}) (x_i - \mu_{c=j})^T S​W​​=​j=1​∑​C​​S​j​​=​j=1​∑​C​​​x​i​​∈c=j​∑​​(x​i​​−μ​c=j​​)(x​i​​−μ​c=j​​)​T​​ SB=∑j=1CNj(μj−μ)(μj−μ)TS_B = \sum_{j=1}^C N_j(\mu_j - \mu) (\mu_j - \mu)^T S​B​​=​j=1​∑​C​​N​j​​(μ​j​​−μ)(μ​j​​−μ)​T​​ where NjN_jN​j​​ is used as the weight. Determinants are the product of all eigenvalues of the matrix. We also have: Sw−1SBAk=λAkS_w^{-1} S_B A_k = \lambda A_k S​w​−1​​S​B​​A​k​​=λA​k​​ To solve AAA, we need to use the eigenvalues of Sw−1SBS_w^{-1} S_BS​w​−1​​S​B​​ and obtain the matrix AAA by KKK eigenvectors. KKK is the number of base vectors, namely the dimension of projection matrix AAA as A=[A1∣A2∣...AK]A=[A_1\mid A_2 \mid ... A_K]A=[A​1​​∣A​2​​∣...A​K​​]. The maximum of KKK is C−1C-1C−1. The classification will be better for the eigenvectors with respect to the larger eigenvalues. Multi-class classification can be transferred to binary classification. The techniques can be categorized into One vs Rest and One vs One. One vs Rest OVR (or one-vs.-all, OvA, one-against-all, OAA) involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. The base classifiers are required to produce a real-valued confidence score for its decision, rather than just a class label, e.g., a new label vector zzz where zi=1z_i = 1z​i​​=1 if yi=ky_i = ky​i​​=k and zi=0z_i = 0z​i​​=0 otherwise. We can use C−1C - 1C−1 binary classifiers. Each binary classifier solves the problem of separating samples in a particular class from samples not in that class. This popular strategy suffers from several problems. Firstly, the scale of the confidence values may differ between the binary classifiers. Second, even if the class distribution is balanced in the training set, the binary classification learners see unbalanced distributions because typically the set of negatives they see is much larger than the set of positives (?). Benefits: Small storing cost and short test time. Shortcomings: Long training time and imbalanced Samples. One vs One OVO involves using C(C−1)/2C(C-1)/2C(C−1)/2 binary classifiers for every possible pairs of classes and learning to distinguish these two classes. At prediction time, a voting scheme is applied: all C(C−1)/2C(C-1)/2C(C−1)/2 classifiers are applied to an sample. The class that got the highest number of vote amongst the discriminant functions gets predicted by the combined classifier. Sometimes it suffers from ambiguities in that some regions of its input space may receive the same number of votes. Benefits: Short training time. Shortcomings: Too many classifiers, large storing cost and long test time. Nearest Neighbor Classification The basic idea of the nearest neighbor classification is: a new sample is classified by calculating the distance to the nearest training case; the sign of that point then determines the classification of the sample. The k-NN classifier extends this idea by taking the k nearest neighbors and assigning the sign of the majority. It is common to select k small and odd to break ties (typically 1, 3 or 5). Larger k values help reduce the effects of noisy points within the training data set, and the choice of k is often performed through cross-validation. KNN classifier is non-parametric, which means it does not make any assumptions on the underlying data distribution. The model structure is determined from the data. KNN is a good choice for a classification study when there is little or no prior knowledge about the distribution data. There is no explicit training phase for KNN. KNN makes predictions just-in-time by calculating the similarity between an input sample and each training instance. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors.]]></content>
      <categories>
        <category>笔记篇之夙兴夜寐</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
        <tag>Classification</tag>
        <tag>Logistic Regression</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（2）理解贝耶斯]]></title>
    <url>%2Fblog%2F2018%2F03%2F06%2Fmachine-learning-2%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. The Learning Problem Input variables / features : x(i)x^{(i)}x​(i)​​ Target variable : y(i)y^{(i)}y​(i)​​ A training example: (x(i)x^{(i)}x​(i)​​,y(i)y^{(i)}y​(i)​​) A training set: {(x(i)x^{(i)}x​(i)​​,y(i)y^{(i)}y​(i)​​); i=1,2,...,mi=1,2,...,mi=1,2,...,m} The space of input values: XXX The space of output values: YYY The goal is addressed as: Given a training set, learn a function h:X−&gt;Yh: X-&gt;Yh:X−&gt;Y so that h(x)h(x)h(x) is a good predictor for the corresponding value of yyy. If there are nnn features, like x=[x1,x2,...,xn]Tx=[x_1,x_2,...,x_n]^Tx=[x​1​​,x​2​​,...,x​n​​]​T​​, the training set will be Xm×nX_{m\times n}X​m×n​​ and Ym×1Y_{m\times 1}Y​m×1​​. Note that mmm is the number of samples and nnn is the number of features of each sample. Linear Regression Linear regression is a very simple approach for supervised learning. We use linear regression to predict a quantitative response YYY from the predictor variable XXX. The relationship between XXX and YYY is assumed to be linear. A linear function h(x)h(x)h(x) can be used to map from XXX to YYY: h(x)=∑i=0nθixi=θTϕ(x)h(x) = \sum_{i=0}^n \theta_i x_i = \theta ^T \phi(x) h(x)=​i=0​∑​n​​θ​i​​x​i​​=θ​T​​ϕ(x) The intercept term is: x0=1x_0 = 1x​0​​=1. Note that here iii is not the number of samples. multiple linear regression Multiple linear regression is a generalization of linear regression by considering more than one independent variable (XXX: n&gt;1n&gt;1n&gt;1). multivariate linear regression (General linear model) The multivariate linear regression is a generalization of multiple linear regression model to the case of more than one dependent variable (YYY will be a matrix). Linear basis function models In polynomial curve fitting, the feature extraction is represented as the polynomial basis functions: ϕj(x)=xj\phi_j(x) = x^jϕ​j​​(x)=x​j​​, jjj is the polynomial order (1≤j≤M1\leq j\leq M1≤j≤M). A small change in xxx affects all basis functions. Thus using a polynomial basis function is global. Consider the sigmoidal basis functions: ϕj(x)=σ(x−μjs)\phi_j(x) = \sigma (\frac{x - \mu_j}{s})ϕ​j​​(x)=σ(​s​​x−μ​j​​​​), where σ(x)=ex1+ex\sigma(x) = \frac{e^x}{1+e^x}σ(x)=​1+e​x​​​​e​x​​​​. A small change in xxx only affects nearby basis functions (near μj\mu_jμ​j​​). The Least Mean Square (LMS) method The cost function is defined as: J(θ)=12∑i=1m(hθ(x(i))−y(i))2J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 J(θ)=​2​​1​​​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)​2​​ The problem is how to get a minimum J(θ)J(\theta)J(θ). Gradient descent Apply a gradient descent algorithm for every jjj: θj:=θj−α∂∂θjJ(θ)\theta_j:= \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) θ​j​​:=θ​j​​−α​∂θ​j​​​​∂​​J(θ) where jjj is the number of basis function (1≤j≤M1\leq j\leq M1≤j≤M). Note that descent means the direction is the opposite of the gradient (−∇J- \nabla J−∇J). For a single sample, the LMS update rule (or Widro-Hoff learning rule) can be obtained as: θj:=θj−α(hθ(x(i))−y(i))xj(i)\theta_j:= \theta_j - \alpha (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} θ​j​​:=θ​j​​−α(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ Batch gradient descent For many samples, we use the batch gradient descent as: θj:=θj−α∑i=1m(hθ(x(i))−y(i))xj(i)\theta_j :=\theta_j - \alpha \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)} θ​j​​:=θ​j​​−α​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ in the iteration: Repeat until convergence { ... (for every j) } The iteration can be written as: θjk+1=θjk−α∑i=1m(hθ(x(i))−y(i))xj(i)\theta_j^{k+1}=\theta_j^k - \alpha \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)} θ​j​k+1​​=θ​j​k​​−α​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ where iii is the number of samples (1≤i≤m1\leq i\leq m1≤i≤m), kkk is the iteration step. Note that xj(i)x_j^{(i)}x​j​(i)​​ is the feature extraction, x(i)x^{(i)}x​(i)​​ is the sample. until ∣θjk+1−θjk∣&lt;ϵ|\theta_j^{k+1} - \theta_j^{k}|&lt;\epsilon∣θ​j​k+1​​−θ​j​k​​∣&lt;ϵ, then θj\theta_jθ​j​​ reaches convergence. For every iteration, all samples will be traversed. If a new sample is added, the iteration has to start from the first sample again. Stochastic gradient descent When the training set is large, instead, we use Stochastic gradient descent as: θjk+1=θjk−α(hθ(x(i))−y(i))xj(i)\theta_j^{k+1} = \theta_j^k - \alpha (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} θ​j​k+1​​=θ​j​k​​−α(h​θ​​(x​(i)​​)−y​(i)​​)x​j​(i)​​ in Loop { for i = 1,m { ... (for every j) } } until ∣θjk+1−θjk∣&lt;ϵ|\theta_j^{k+1} - \theta_j^{k}|&lt;\epsilon∣θ​j​k+1​​−θ​j​k​​∣&lt;ϵ, then θj\theta_jθ​j​​ reaches convergence. For each θ\thetaθ, only one sample is used for iteration until the convergence. If a new sample is added, the iteration will continue only using the new sample. Finally, the new θ\thetaθ and hθh_{\theta}h​θ​​ are obtained. Feature scaling is a method used to standardize the range of independent variables or features of data (data normalization). Feature scaling can be used to improve the iteration rate in gradient descent, e.g., rescaling and standardiztion. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. Thus each feature contributes approximately proportionately to the final values. The normal equations To avoid interations, we can use the normal equations. Consider Xm×nX_{m\times n}X​m×n​​ and Ym×1Y_{m\times 1}Y​m×1​​ in matrix, the derivative of J(θ)J(\theta)J(θ) equals zero: ∇θJ(θ)=XTXθ−XTy⃗=0\nabla_\theta J(\theta) = X^T X\theta - X^T \vec{y} = 0 ∇​θ​​J(θ)=X​T​​Xθ−X​T​​​y​⃗​​=0 Finally, θ=(XTX)−1XTy⃗\theta = (X^T X)^{-1}X^T\vec{y} θ=(X​T​​X)​−1​​X​T​​​y​⃗​​ In most situations of practical interest, the number of data points mmm is larger than the dimensionality nnn of the input space, and the matrix XXX is of full column rank. A matrix is full column rank when each of the columns of the matrix are linearly independent. Then XTXX^TXX​T​​X is necessarily invertible and therefore positive definite. The minimum J(θ)J(\theta)J(θ) can be obtained at the critical point when ∇θJ(θ)=0\nabla_\theta J(\theta)=0∇​θ​​J(θ)=0. If m≤nm \leq nm≤n or XXX is not of full column rank, XTXX^TXX​T​​X is not invertible. Regularization is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting. E.g. In Quadratic ridge regression, the cost function is rewritten as: J(θ)=12∑i=1m(hθ(x(i))−y(i))2+λ2∑j=1n∣θj∣2J(\theta) = \frac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2} \sum_{j=1}^n |\theta_j|^2 J(θ)=​2​​1​​​i=1​∑​m​​(h​θ​​(x​(i)​​)−y​(i)​​)​2​​+​2​​λ​​​j=1​∑​n​​∣θ​j​​∣​2​​ According to ∇θJ(θ)=0\nabla_\theta J(\theta)=0∇​θ​​J(θ)=0, θ^\hat{\theta}​θ​^​​ can be calculated as: θ^=(XTX+λI)−1XTy⃗\hat{\theta} = (X^TX+\lambda I)^{-1}X^T\vec{y} ​θ​^​​=(X​T​​X+λI)​−1​​X​T​​​y​⃗​​ Direct methods: Solve the normal equations by Gaussian elimination or QR decomposition Benefit: in a single step or very few steps Shortcoming: not feasible when data are streaming in real time or of very large amount Iterative methods: use the stochastic or gradient descent Benefit: converging fast, more attactive in large practical problems Shortcoming: the learning rate α\alphaα should be carefully chosen. For probalilistic interpretation of Least Mean Square, by the independence assumption, LMS is equivalent to maximum likehood estimation (MLE) of θ\thetaθ. Locally weighted linear regression (LWR) The problem is how to get a minimum J(θ)J(\theta)J(θ) given as: J(θ)=∑i=1mw(i)(hθ(x(i))−y(i))2J(\theta) = \sum_{i=1}^m w^{(i)}(h_{\theta}(x^{(i)}) - y^{(i)})^2 J(θ)=​i=1​∑​m​​w​(i)​​(h​θ​​(x​(i)​​)−y​(i)​​)​2​​ where w(i)=exp(−(x(i)−x)22τ2)w^{(i)} = exp(-\frac{(x^{(i)} - x)^2}{2\tau^2}) w​(i)​​=exp(−​2τ​2​​​​(x​(i)​​−x)​2​​​​) where xxx is the query point for which we’d like to know its yyy. Essentially higher weights will be put on the training examples close to xxx. Parametric and Nonparametric A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at a parametric model, it won’t change its mind about how many parameters it needs. Nonparametric methods are good when you have a lot of data and no prior knowledge, and when you don’t want to worry too much about choosing just the right features. (Artificial Intelligence: A Modern Approach) Benefits of parametric models: Simpler: These methods are easier to understand and interpret results. Speed: Parametric models are very fast to learn from data. Less Data: They do not require as much training data and can work well even if the fit to the data is not perfect. Limitations of parametric models: Constrained: By choosing a functional form these methods are highly constrained to the specified form, because a parametric model has a fixed and finite number of parameters (θ\thetaθ). Limited Complexity: The methods are more suited to simpler problems. Poor Fit: In practice the methods are unlikely to match the underlying mapping function. Benefits of nonparametric models: Flexibility: Capable of fitting a large number of functional forms. Power: No assumptions (or weak assumptions) about the underlying function. Performance: Can result in higher performance models for prediction. Limitations of non-parametric models: More data: Require a lot more training data to estimate the mapping function. Slower: A lot slower to train as they often have far more parameters to train. Overfitting: More of a risk to overfit the training data and it is harder to explain why specific predictions are made. Locally weighted linear regression is a non-parametric learning algorithm. The term non-parametric means the model structure is not specified a priori but is instead determined from data. It is not meant to imply that such models completely lack parameters but that the number and nature of the parameters are flexible and not fixed in advance. If we use unweighted linear regression, Once θ\thetaθ is determined, we no longer need to keep the training data around to make future predictions. In contrast, if the weighted linear regression is applied, the entire training set should be kept around.]]></content>
      <categories>
        <category>笔记篇之夙兴夜寐</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
        <tag>Linear Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（1）常识]]></title>
    <url>%2Fblog%2F2018%2F02%2F28%2Fmachine-learning-1%2F</url>
    <content type="text"><![CDATA[Introduction Here is a not so short note (Rolling Update) for machine learning and artificial intelligience. Main concepts and some references are presented. This note is based on Wikipedia, Andrew Ng’s Lecture Notes CS229 of Standford, Machine Learning course of SJTU IEEE-yangyang, and Machine Learning Crash Course of Google. Elementary concepts What is machine learning? Machine Learning is a scientific discipline that explores the construction and study of algorithms that can learn from data. Machine learning enables analysis of massive quantities of data. We all know “Machine”, i.e. computers or computer programs. Do you know what is “Learning”? Learning denotes “Changes in the system that are adaptive in the sense that they enable the system to do same task or tasks drawn from the same population more efficiently and more effectively the next time” [1]. As a pioneer in the field of artificial intelligence, Herbert A. Simon created with Allen Newell the Logic Theory Machine (1956), and the General Problem Solver (GPS) (1957) programs, which was thought as the first method developed for separating problem solving strategy from information about particular problems. Arthur Samuel created the world’s first successful self-learning programs, the Samuel Checkers-playing. He coined the term “Machine Learning” in 1959, which was defined as “the field of study that gives computers the alility to learn without being explicitly programmed”. Tom Mitchell (former Chair of the Machine Learning Department at CMU) gave a modern definition: “A computer program is said to learn from experience EEE with respect to some class of tasks T and performance measure PPP, if its performance at tasks in TTT, as measured by PPP, improves with experience EEE.” An example: playing checkers. EEE = the experience of playing many games of checkers. TTT = the task of playing checkers. PPP = the probability that the program will win the next game. What can machine learning do？ ML can be used to deal with Big Data deluge and predicitve analytics, e.g. Document Classification Spam Filtering Weather Prediction Stock Market Prediction Collaborative Filtering Clustering Images Human Genetics Decoding thoughts from brain scans Medical data analysis Finance Robotics Natural language processing, speech recognition Computer vision Web forensics Computational biology Sensor networks (new multi-modal sensing devices) Social networks Turbulence problem Three components of a machine learning algorithm Pedro Domingos, a CS professor at the University of Washington, decomposed machine learning into three components: Representation, Evaluation, and Optimization [2]. Table 1. The three components of learning algorithms Represnetation Evaluation Optimization Instances Accuracy/Error rate Combinatorial optimization ^K-nearest neighbor Precision and recall ^Greedy search ^Support vector machines Squared error ^Beam search Hyperplanes Likelihood ^Branch-and-bound ^Naive Bayes Posterior probability Continuous optimization ^Logistic regression Information gain ^Unconstrained (Convex) Decision trees K-L divergence ^^Gradient descent Sets of rules Cost/Utility ^^Conjugate gradient ^Propositional rules Margin ^^Quasi-Newton methods ^Logic programs ^Constrained Neural networks ^^Linear programming Graphical models ^^Quadratic programming ^Bayesian networks ^Conditional random fields Representation A classifier must be represented in some formal language that the computer can handle. A learner takes observations as inputs. The observation language is the language used to describe these observations. The hypotheses that a learner may produce, will be formulated in a language that is called the hypothesis language. The hypothesis space is the set of hypotheses that can be described using this hypothesis language [3]. The problem is how to represent the input, i.e. what features to use. For example, 3-layer feedforward neural networks (or computational graphs) form one type of representation, while support vector machines with RBF kernels form another. Evaluation Evaluation is essentially how you judge or prefer candidate programs (hypotheses). An evaluation function (also called objective function, utility function, loss function, fitness function or scoring function) is needed. Mean squared error (of a model’s output vs. the data output) or likelihood (the estimated probability of a model given the observed data) are examples of different evaluation functions. Optimization Finally, a method is needed to search among the candidate programs (in the space of represented models) for the highest-scoring one as the optimum. Stochastic gradient descent and genetic algorithms are two different ways of optimizing a model class. Machine Learning Tasks Machine learning tasks are typically classified into two broad categories, depending on whether there is a learning “signal” or “feedback” available to a learning system: superivsed learning and unsupervised learning. Other tasks also include semi-supervised learning, active learning, reinforcement learning, etc. Supervised learning Supervised learning (or inductive learning) is the most mature, the most studied and the type of learning used by most machine learning algorithms. Learning with supervision is much easier than learning without supervision. Training data includes desired outputs. The goal is to learn a general rule that maps inputs to outputs. Typically, the inputs are transformed into a feature vector, which contains a number of features that are descriptive of the object. Classification Regression Reinforcement learning is concerned with how to take actions in an environment to maximize some notion of long-term reward, such as game-theory. It differs from standard supervised learning in that correct input/output pairs are not provided, nor sub-optimal actions explicitly corrected. Unsupervised learning Training data does not include desired outputs. It is hard to tell what is good learning and what is not. The program own will find the feature in its inputs as “unlabeled” data. Clustering Dimensionality reduction Anomaly dection Neural Networks Main purpose of studying ML To make money.]]></content>
      <categories>
        <category>笔记篇之夙兴夜寐</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Artificial Intelligence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日本海军舰艇命名]]></title>
    <url>%2Fblog%2F2018%2F02%2F14%2Fship-name-japan%2F</url>
    <content type="text"><![CDATA[出云 日本领土陆地面积约37.79万平方公里，由北海道、本州、四国、九州等其他6,852个岛屿组成。看似国土狭小的日本，却拥有可怕的领海面积，约447万平方公里，超过其陆地面积十一倍。由于岛国的局限性，日本自古以来极其重视发展海军，并于第二次世界大战时期达到了巅峰。二战以后至今，日本作为战败国，曾经优秀的海军也随之衰落。截至目前日本海上自卫队的最大军舰是22DDH和24DDH。其中，DDH代表helicopter destroyer，即直升机驱逐舰。22和24分别指的是平成22年（2010年）和平成24年（2012年）完成防卫预算下拨。不过不要被它驱逐舰的名字迷惑了，DDH的体型相当于普通驱逐舰的3～5倍。所以日本其实是在玩文字游戏，其实它就是一艘直升机航空母舰。由于海上自卫队的性质，国际上是绝不允许日本建造巡洋舰、航空母舰等大型进攻型舰艇的，只好打打擦边球。强行命名为驱逐舰也可以看出日本海上自卫队的脸皮比航空母舰的甲板钢还要厚。有趣的是， 按照自卫队的传统，新舰的命名应该在下水日（8月6日）才公布，却因海上幕僚监部公文作业疏忽的缘故7月17日就提前曝光了。她就是出云，日文名いずも，Izumo。 首先看一下出云的数据。全长248米，宽度38米，吃水7.5米，满载排水量26000吨，配备4台通用动力LM2500 IEC型燃气轮机，最高航速30节。搭载七架Sikorsky SH-60K海鹰直升机，其中SH代表Seahwak，主要任务是反潜作战（Anti-submarine warfare）。由Ishikawajima-Harima Heavy Industries海洋联合横滨工厂负责建造，单舰造价1200亿日元。首舰出云于2013年8月6日在神奈川县横滨市下水，2015年3月正式服役。 不过“出云”不是破云而出的意思，出云是日本弥生时代一个令制国（相当于古代中国的州）名字。二号舰被命名为加贺（かが，Kaga），同样是一个古国名。另外，这两艘战舰的命名是沿袭二战期间的出云重巡洋舰和加贺航空母舰的历史。不过二战期间出云和加贺都参与了侵华战争，罪行累累，网上说该舰被命名为出云是“恶魔舰复活”也不过分。相比现代的日本海上自卫队，二战时期日本海军（IJN）的命名有一整套独特的规则，很多名字听起来很和风，很美，比如瑞鹤、夕云、南风等等。 航空母舰命名规则 航空母舰是一种搭载飞机为主要武器的军舰，是目前海军最大的作战舰艇平台。日本与航空母舰有千丝万缕的羁绊。例如，一战二战期间中国没有航空母舰，中文里航空母舰这个词怎么来的呢？并不是直接翻译于Aircraft Carrier，而是来自于日语：航空母艦（こうくうぼかん），简称空母（くうぼ），可见日本航空母舰的影响之深远。所以提起航空母舰有时侯会引起误会，并不是会飞的船，而是省略了两个字，即“航空（器的）母舰”。世界上第一艘标准的航空母舰是日本最先建造完工的。二战爆发前夕，IJN是太平洋上实力最强，拥有十艘正规空母，相比之下英国有八艘，美国只有七艘。成名之战是1941年的日本的六艘航空母舰袭击珍珠港。后来美日之间的珊瑚海海战，中途岛海战，莱特湾海战，更是宣告了航空母舰时代的到来。 日本航空母舰一般按照龙（りゅう）、凤（ほう）、鹤（かく）等神话生物命名。中途岛海战前，日本的十艘航空母舰分别是：赤城，加贺、苍龙、飞龙、翔鹤、瑞鹤、凤翔、龙骧、祥凤和瑞凤。民用船舶改造而成的日本航母以各种鹰（よう）命名，例如大鹰、云鹰、飞鹰、神鹰、海鹰、隼鹰、冲鹰。千岁、千代田是两个特例，她们原是水上飞机母舰，改装成为航空母舰之后保留了原名。 日语中有两种龙，龍（りゅう）是来自中国东方传说中的龙，是掌管风雨的小神，而另一种，竜（たつ，是龍的简化字）一般指西方神话中的dragon，是一种邪恶贪婪的生物（通常是宝藏的看守）。两种龙的共同点都是拥有强大的力量。一个冷知识是日本的龙通常是三爪，与中国唐代的龙的风格一致，中国的龙经历了从三爪到五爪的演变，是皇帝的象征，而韩国的龙是四爪。在日本，凤凰（鳳凰）是一种比龙更高阶的存在。日本人信奉天照大神，有记载称垂仁天皇26年，天照大神降临神风伊势国，于28年化为白凤。龙有邪恶的龙，比如传说里有勇者斗恶龙，但凤凰是绝对的神圣、吉祥、繁荣的象征和征兆，通常出现在皇室的建筑、服饰、纪念币等，例如天皇座驾丰田世纪的车标就是一只凤凰。西方文化里也有类似的生物，即phoenix，但是与凤凰还是有根本的区别。Phoenix一般翻译为不死鸟、火鸟，全身赤红色，长得类似鹰的一种猛禽，特点是会浴火重生。而东方神话里凤凰是五彩的，栖息在梧桐上，象征爱情，类似孔雀的神鸟。山海经里南山经记载，“丹穴之山，有鸟焉，其状如鸡，五采而文，名曰凤皇。首文曰德，翼文曰义，背文曰礼，膺文曰仁，腹文曰信，是鸟也，饮食自然，自歌自舞，见则天下安宁。”与西方的不死鸟最大差别就是东方的凤凰不会浴火重生。至于凤凰涅磐，这个词是郭沫若生造的，并非古代传说。日本的国鸟是绿雉，印在了一万元日元上，因此可以说日本的凤一般指的就是中国的凤凰，祥瑞的象征。讽刺的是，二战中第一艘沉没的日本航母就是祥凤。东方文化里，鹤代表着健康长寿、吉祥高贵，也可以是一种为夫妻带来孩子的仙鸟（送子鹤）。宋徽宗画过一幅瑞鹤图。日本天皇的声音又叫做鹤音、玉音。一千元日元上也有丹顶鹤的图案。北海道的阿伊努人把生活在钏路湿地的丹顶鹤称为湿地之神。总之取名叫鹤显得仙气十足。 翔鹤与瑞鹤两姐妹是第五航空战队的主力，是当时日本最强，飞行员最优秀，战绩最好的航空母舰。作为最著名的两艘日本航母，赤城和加贺却并没有吉祥生物加持。赤城是巡洋舰改装过来的航母，赤城来自关东北部的赤城山來命名。第一第二航空战队的旗舰赤城，是日本当时最大的航母，一度被视为日本海军机动部队的象征，航空兵的摇篮，山本五十六曾担任舰长。加贺是战列舰改装过来的航母，所以是按古代令制国名来命名。最大的特点是跑得慢，只有28节，以至于舰队里其他航母为了等她，还要额外携带大量油筒。赤城、加贺、苍龙、飞龙四舰葬身于中途岛，其中加贺全舰共811人死亡，伤亡惨重。在加贺短暂的一生中，曾与国民党空军交过手，文末有彩蛋。 二战后期日本主要建造了三艘云龙级航母：云龙，天城和葛城。后面两艘取名自天城山、大和葛城山。天城山盛产衫树，是当时海军最常用的甲板木料。有趣的是，云龙1944年8月下水的时候，地主家也没有余粮了，日本已经没有可用的舰载机，云龙只好呆在吴港思考舰生，最后一直当作运输船使用。天城建成的时候物资匮乏，采用的是重巡洋舰的主机。天城也没有舰载机，一直无所事事，1945年7月8日在美军的轰炸中沉没，成为了至今为止全世界最后一艘战损的航空母舰。葛城建成的时候穷的甚至巡洋舰主机都没有了，只好用的是驱逐舰主机，主机功率从15万马力被迫下降为10万马力。天城和葛城这一对难兄难弟，为了躲避空袭，航空母舰的甲板上装饰了植物、田地、小房子之类，企图cosplay小岛，然而被炸弹爆炸掀起的巨浪吹掉。天城运气太差，被炸沉，葛城倒没有受到致命的损伤，幸存到了1946年11月。 1945年2月以后，所有日本的航空母舰因为没有飞行员不得不逗留在港口内，无法四处活动。 战列舰命名规则 战列舰有时候也直接称战舰。特点是厚重的装甲加上越来越巨大的火炮。在飞机导弹出现以前，舰船以火炮作为主要作战武器，战时排列成一条线互相射击，因此称作战列舰。随着1906年英国的无畏号（HMS Dreadnought）服役，全世界海军开始进入全重型火炮（All-Big-Gun）和高航速主导的时代，按此思想建造的战舰统称为无畏舰。日德兰海战之后，随着主炮口径增加到13英寸以上，并且主炮射程已经超出了视力极限以外，统称为超无畏舰。日语中，根据外来语无畏舰的读音ドレッドノート取首字ド，并用同音字“弩”替代，故称为弩级战舰。超弩级战舰也即超无畏舰。尽管在一战二战时期战列舰是主力（Capital ship），越造越大，但现代已经全部淘汰，最后的露面是在1990年的海湾战争。不过未来随着电磁炮的出现和发展，倒可能使战列舰重新登上历史舞台。 1905年颁布的《日本海军舰艇命名办法》规定：战列舰以古国名命名。二战期间比较著名的日本战列舰有：大和，武藏，长门，陆奥，扶桑，山城，伊势，日向等。 从奈良时代开始，日本古代令制国分为五畿七道，后加上北海道。五畿包括：山城国（现京都府），大和国（现奈良县），河内国（现大阪府），和泉国（现大阪府南部），攝津国（现兵库县和大阪府）。七道分别是：东海道，东山道，北陆道，山阴道，山阳道，南海道，西海道。道是仿唐朝制度的，比国高一级的行政单位。每一个道下分数个国，例如东海道里有伊贺国，东山道里有信浓国，西海道还有一个萨摩国。 人类有史以来建成的最大的战列舰是日本海军的大和，名字来自古代畿内五国之一的大和国。还有一艘同型舰武藏。可以说她们体现了日本造船技术的巅峰。大和的满载排水量达到了惊人的72,809吨，吃水达到了10.4米，最高行速27节。赫赫有名的三座三联装94式45倍口径460毫米主炮（18.1英寸），是人类有史以来最大的舰炮，一枚炮弹重1.5吨，射程超过26英里。主炮齐射时，甲板上的防空炮手必须躲回舰体内，防止被冲击波杀伤。实际造价为1.37802亿日元，一艘相当于1937年日本GDP的0.29％。 戏剧性的是，作为日本帝国和民族的象征，她是决不允许被击沉的。因此实力最强的大和却始终无缘前往一线作战，一生里大多数时间都停在港口内消磨时光。1945年4月6日最后的时刻，帝国已是穷途末路，大和默默地驶出港口，而即将迎接她的是15艘美国航空母舰的攻击。 巡洋舰命名规则 巡洋舰最初指的是可以独立行动的战舰，续航力强，被用于巡逻海外殖民地，排水量、装甲及火力一般仅次于战列舰，但航速较高，机动性强，射速快，并且造价远低于战列舰。根据1921年《华盛顿海军条约》，主炮口径超过8英寸（203毫米）为重巡洋舰。1930年的《伦敦海军条约》为了削减海军装备，重新制定了主炮口径6.1英寸及以上为重巡洋舰，其他的划分为轻巡洋舰。重巡洋舰一般用于袭击敌军和保护主力舰，轻巡洋舰承担护航或防空等任务。为了保证火力和防护的平衡，一般要求军舰自身的装甲要能够抵挡自己的火炮攻击。但也有一种奇葩的船型，她拥有比肩战列舰的强大火力，但为了提升机动性，牺牲了自身的装甲防护，航速可以与巡洋舰相当，专门用于追击作战。这就是战列巡洋舰，典型的有英国的胡德，号称英国皇家海军的骄傲（The mighty Hood）。胡德的火力不亚于德国的俾斯麦号战列舰，但胡德的防护水平却相差很远，丹麦海峡一战中被命中弹药库造成剧烈爆炸沉没。为了给胡德报仇，英国甚至出动了海军能动的所有战舰追杀俾斯麦。现代海军已经很少有国家继续建造巡洋舰，随着导弹和雷达的广泛应用，巡洋舰的角色逐渐被大中型驱逐舰所替代。 日本海军的巡洋舰可以说是“山、川”舰队，轻巡洋舰以河流命名，战列巡洋舰和重巡洋舰以山命名。 日本的轻巡洋舰比较重视鱼雷作战。比较著名的有球磨级轻巡洋舰，球磨来自日本熊本县南部水系的球磨川，是日本三大急流之一。球磨级造了五艘，一般作为水雷战队旗舰进行掩护作战，也执行运输船队的护航任务，航速达到了36节。大井和北上于1941年进行了魔改，拆除了三门主炮，在两侧各加装了五座四联装九二式610毫米鱼雷发射管，称重装雷舰。1942年北上拆除了鱼雷发射管改为高速运输舰，搭载了8艘大型登陆艇。1944年北上继续进行魔改，用于特攻作战，搭载了四艘“回天”人操鱼雷。所谓人操鱼雷，就是由人直接操舵，自杀式攻击的特殊鱼雷（潜艇）。回天反映了当时日本对战局逆转的渴望（天を回らし、戰局を逆転させる）。虽然威力很大，直径达到1000毫米，由于操作困难，战果不大。 著名的战列巡洋舰有金刚级四姐妹：金刚，比睿，榛名，雾岛。金刚山在大阪府与奈良县的边境上，景色优美，吸引了众多的登山爱好者。比睿山位于京都府，自古被视作镇护京都的圣山，山上有延曆（历）寺，故有日本佛教之母山的美称。榛名山是火山，位于群马县，上毛三山之一（赤城、榛名、妙义），也是头文字D中秋名山的原型（日语里榛与春同音，对应秋名）。雾岛山是火山群，位于九州宫崎县附近。传说是日本天照大神的孙子降临的地方。首舰金刚号由英国维克斯公司建造，其他三艘在日本建造，1915年完工。所以金刚级具有英国战舰的特点，比如高大的三角主桅杆。有趣的是，一战期间英日属于同盟国，英国曾请求租借日本的金刚级以对抗德国，但被日本回绝。金刚级建成时属于战列巡洋舰，后来日本海军废除了战列巡洋舰这一舰种，1923～1933年金刚型四舰进行了大改，成为（高速）战列舰，但还是保留了之前的命名方式。不过装甲设计没有得到大规模改进，防御能力不如战列舰。其中，金刚曾担任第五代（1931～1933）日本联合舰队旗舰，1944年沉没于台湾海峡附近，是日本唯一一艘被潜艇击沉的战列舰。在舰队Collection中的人设为英国海归，爱喝红茶，外号大傻。比睿由横须贺海军工厂建造，因《伦敦海军条约》被裁成训练舰，在1930年代曾多次担任日本天皇检阅海军的御召舰，深受民众喜爱，其照片曾出现在纪念邮票上。榛名之前所有主力舰军在海外订购或海军工厂建造，而榛名是第一艘由民企，即川崎造船所（现川崎重工）承建，主机测试前发生了故障导致延期，川崎造船所造机工作部长篠田恒太郎因自责剖腹自尽，可见压力之大。另一家民企，三菱合资会社长崎造船所（现三菱重工）则承建了四号舰雾岛，与姐姐比睿在1942年所罗门海战中一同沉没。 驱逐舰命名规则 驱逐舰最初用来对付鱼雷艇，便宜、易造又好用，后来也承担起防空、反潜、攻击等多用途任务，是各国海军建造最多的一种军舰。英语里的Destroyer翻译成驱逐舰着实有点委屈它了。游戏里驱逐舰由于吨位小，航速快，经常被称为小学生（大误）。 日本驱逐舰分两种，一等驱逐舰以天气、潮汐、水流、月亮、季节、自然现象等命名，一个不是很恰当的概括就是“风花雪月”。二等驱逐舰以植物等命名。 很多驱逐舰名字的禅意不输各种艺术品。例如，吹雪型驱逐舰有：吹雪，白雪，初雪，深雪，从云，东云，薄云，白云，矶波，浦波；绫波型驱逐舰有绫波，敷波，朝雾，夕雾，天雾，狭雾，胧，曙，涟，潮；晓型驱逐舰有晓，响，雷，电，著名的第六小学生驱逐队。首舰吹雪于1928年8月10日完工，排水量1980吨，全长118.5米，宽10.4米，吃水3.2米，最高航速38节。以植物命名的驱逐舰有：松，梨，若竹，楢（即橡树）等。这里介绍一艘著名的日本驱逐舰：岛风。该型驱逐舰最初计划建造32艘，可由于她的动力系统过于复杂、生产跟不上等缘故，最后只建造了一艘，岛风没有姊妹舰，也没被编入任何驱逐舰队，从出生开始就很孤独。岛风有两个特点，第一个特点是跑得特别快，装有试验涡轮机，最高航速达到了惊人的40.7节（约每小时72公里），创下日本驱逐舰的最快记录，人称海上飚车老司机。另一个特点是鱼雷特别强，装备了3座五连装九三式重型酸素鱼雷（日语中酸素意为氧气，水素意为氢气，窒素意为氮气），一举成为水雷战最强驱逐舰（日语中的水雷是鱼雷、水雷、深水炸弹的统称）。这种鱼雷是当时全世界最先进的鱼雷，使用压缩氧气代替压缩空气作为推进，最大射程达到了40km，弹头重达1080磅，并且航迹难以被发现。同时期美军的MK15鱼雷最远射程仅14km，弹头也只有827磅。岛风单次可以齐射15枚九三式，几乎是普通驱逐舰的2倍。服役后，太平洋上的局势已经不可逆转，而且鱼雷决战的思想也已经落后，因此岛风并没有什么骄人的战绩。1944年11月8日奥尔莫克湾战役中，虽然岛风以惊人的高速机动能力躲过了多次轰炸，最终还是不敌美军舰载机，中弹太多而后因锅炉过热引起爆炸，沉没在菲律宾附近海域。此后，太平洋再也没有岛风。不过在海底，岛风应该不再那么孤独了吧。 潜艇命名规则 潜水艇就是在水下航行的舰艇，小的有单人操作的水下航行器，大的有苏联建造的941台风弹道导弹核潜艇，水下排水量可达4.8万吨。二战期间大西洋战场上的狼群，指的就是德国海军邓尼茲领导的U型潜艇部队，对盟军的海上交通和补给线造成了巨大的破坏。在日本，潜水艇被称作潜水艦。 根据《日本海军舰艇命名办法》，一等潜艦按汉字“伊”后加上数字命名，二等潜艦按汉字“吕”后加上数字命，三等潜艦按汉字“波”后加上数字命。名称按顺序取自《伊呂波歌》名。这是一首日本平安时代的和歌，每句5音和7音相错。由于全歌以47个不重复的假名组成，后来常被用于书法的范文和假名的学习范文。有一个中译版本：花虽芬芳终须落，此世岂谁可常留。有为山深今日越，不恋醉梦免蹉跎。 二战中的日本潜艇虽然没有德国狼群那么臭名昭著，不过也有那么几个比较出名的。第一个例子是二战中最大的潜艇：伊四〇〇型。它的满载排水量达到了6560吨，达到了一般巡洋舰的水平。之所以造这么大，其特色是可以搭载三架小型轰炸机，计划是潜航到接近海岸线，然后释放晴岚水上飞机携带细菌武器攻击美国本土，因此伊400实际上算是潜水空母，不过尚未投入实战日本就已投降。第二个例子是伊19，1942年9月15日在所罗门群岛伊19使用六发鱼雷击沉了美军的胡蜂号航空母舰（CV-7)。第三个例子是伊58，以击沉美军印第安纳波利斯号重巡洋舰而声名大噪，因为这艘巡洋舰刚刚完成了运输“小男孩”原子弹到提尼安岛的秘密任务。1945年8月6日B-29轰炸机在广岛上空投掷该原子弹，伤亡10万余人。2016年有一部尼古拉斯·凯奇主演的电影：《印第安纳波利斯号：勇者无惧》即是根据此事件改编（USS Indianapolis: Men of Courage）。 本文资料主要来自于维基百科和萌娘百科。在此一并致谢。 彩蛋 舰队Collection 一般译作舰队收藏，是由角川游戏开发、DMM.com提供及运营的网页游戏，以二战时期的日本军舰为题材，玩家需要收集称为舰娘（艦娘（かんむす））的军舰萌拟人化角色卡片，可以对舰娘进行强化及改造，并编制不同的舰队与敌人战斗务求获得胜利。作为PC浏览器运行的网页游戏，在运营半年用户即突破100万。立绘精美，画师在创作时也加入了很多历史元素。例如，大和的形象是一个打伞的大姐姐，里面的伞来源于大和上的一式三型电探天线。所谓的电探又是啥？日本的雷达在以前被翻译成电波探信仪。 舰娘本娘 吾妻 旧日本海军有一艘装甲巡洋舰名叫“吾妻”，订购自法国。1898年由法国Ateliers et Chantiers de la Loire造船厂建造。1900年回到日本服役，曾参与日俄战争中的旅顺口海战，蔚山海战和对马海战。到1945年已是服役四十多年的老舰，被美军击沉于港口内，第二年被解体。 鳳翔 凤翔（ほうしょう，Hōshō）就是上文提到的世界上第一艘服役的全通式甲板航空母舰，采用岛装上层建筑，技术上是从英国窃取而来。本来世界第一属于英国的竞技神号航空母舰，不过由于英国的进度拖拉，导致世界第一被日本海军抢走。凤翔于1920年12月16日开工，1922年12月27日服役，被尊称为航母之母。她是唯一一艘活到日本投降后还没有受损的航空母舰，战后作为复员运输舰到各地进行接运任务。后在大阪日立造船樱岛工厂解体，度过了幸运的一生。注意，凤翔跟祥凤是两码事。 武勋三舰客 旧日本海军二战期间有三艘公认的武勋舰（战绩好，活的久）：瑞鹤被称为“幸運の空母”。榛名一直活跃在一线战场，受到过不同损伤而始终没有被击沉。雪风被称为“奇跡の駆逐艦”，著名的扫把星（祥瑞），日常坑队友，通常在海战中自己毫发无损，克死友军无数。民间又称抗日奇侠雪风号。，感兴趣的我们将在后文《雪风的故事》里再见。 相思不断笕桥东 如果您浏览了上面那么多枯燥的数据，还能坚持看到这里没有关掉页面，那真的十分感谢了。文章结尾，再讲一个故事。 加贺号航空母舰，侵华战争中多次参与对中国人民的屠杀，参加过对苏州、杭州、上海的轰炸。虽然耀武扬威无数，不过也有扑街的时候。1937年8月14日，正值台风过境，日军决定空袭杭州笕桥——中国的空军学校，就是来自加贺。那一天，21架国军空军霍克-3双翼机与45架日本舰载机于空中战斗，加贺损失了8架八九舰攻和2架九四舰轰，而年轻的中国空军竟无一伤亡，吊打日军，胜利凯旋（一说击落日军飞机三架击伤一架）。史称八一四空战，又称笕桥空战。这样一支英勇的空军，更是打到木更津航空队联队长石井义大佐剖腹自杀，被日军视为耻辱。 《冲天》是台湾2015年拍的一部纪录片，豆瓣评分9.2。没有慷慨激昂和洗脑宣传，只平平淡淡地讲述了抗日战争年代一群年轻飞行员的爱情与事业，令人泪流满面。如果有兴趣的话，可以去看一看高志航的故事。笕桥中央航空学校校门口石碑上立着：“我们的身体、飞机和炸弹，当与敌人兵舰阵地同归于尽！”与那个年代大多数参军的农民子弟不同，中央航空学校的毕业生们大多数家庭出身优渥，教育程度高，如果没有战争，他们也许将是各行各业的精英。可命运让他们生在了那个年代，成为了飞行员。《无问西东》中，王力宏演的沈光耀的人物原型，叫做沈崇诲。他就读著名的天津南开中学，18岁考入清华大学土木工程系，1932年毕业后不久放弃了在绥远舒适的工作，投考中央航校轰炸科，毕业后留校担任飞行教官。淞沪会战期间，1937年8月19日，沈崇诲驾机为地面日军炮火击中，难以返回，他决定与敌人同归于尽，瞄准了日军一艘战舰急速俯冲而下，撞舰牺牲，年仅26岁。而这艘被撞的日本军舰，就是本文开头所提到的历史同名战舰：出云。 因为笕桥空战，每年8月14日被定为空军节。“相思不断笕桥东”是《西子姑娘》里的歌词，是中华民国空军军歌之一。抗日战争中，中华民国飞行员的平均牺牲年龄是23岁。有关我们飞行员的故事很多，那些故事里，最多的一句话是：“某天，他一去就再也没有回来。” 希望人类不要再有战争，希望所有人能和自己的爱人永远在一起。]]></content>
      <categories>
        <category>考据篇之木雁之间</category>
      </categories>
      <tags>
        <tag>Navy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH免密码登录]]></title>
    <url>%2Fblog%2F2018%2F02%2F07%2Fssh-login%2F</url>
    <content type="text"><![CDATA[本文介绍如何使用SSH进行远程登录以及设置SSH免密码登录。 SSH简介 首先简单介绍一下什么是SSH(Secure Shell)。 SSH是一个用于计算机间的远程登录会话和其他网络服务的加密网络安全协议，由互联网工程任务组 IETF(Internet Engineering Task Force)的网络工作小組（Network Working Group）所制定。相对于传统的网络服务，如FTP、 POP等， SSH协议避免了使用明文传输数据，帐号和口令，可以有效防止信息泄漏和中间人攻击，也能够防止DNS欺骗和IP欺骗。其具体工作原理本文不作详述。 使用SSH登录远程主机 首先判断是否安装SSH服务。打开Terminal试试： 1$ ssh localhost 若显示connection refused, 则需要手动安装。 安装SSH服务： 通常使用的是openssh, 以Ubuntu为例。 本机： 1$ sudo apt-get install openssh-client 服务器： 1$ sudo apt-get install openssh-server 安装完成后确认开启SSH服务： 1$ ps -e|grep ssh 启动服务器端的SSH服务： 1$ sudo /etc/init.d/ssh start 更改默认端口号： 1$ sudo vim /etc/ssh/sshd_config 使用SSH登录： 本机输入以下命令登录远程服务器(例如ip地址为10.20.120.10的远程服务器)： 1$ ssh user@10.20.120.10 开启图形界面： 1$ ssh -Y user@10.20.120.10 即可使用远程服务器上的图形界面应用。 一般情况下，每次登录必须输入密码。在涉及大量文件传输操作时，频繁输入密码会十分不便。以下介绍如何免密码登录。 设置SSH免密码登录 创建公钥 1$ ssh-keygen -t rsa 默认设置即可，创建id_rsa（私钥）和id_rsa.pub（公钥）文件。以后再次操作记得先行备份。 上传公钥至服务器 1$ rsync -avPz ~/.ssh/id_rsa.pub user@server:~/.ssh/ Note: 如果服务器上无.ssh目录则先创建（mkdir ~/.ssh） 复制公钥至authorized_keys 1$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 更改文件及目录权限 12$ chmod 600 ~/.ssh/authorized_keys$ chmod 700 ~/.ssh 完成后即可不输入密码直接登录。]]></content>
      <categories>
        <category>工技篇之吴带当风</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2F2018%2F02%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new "My New Post" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>工技篇之曹衣出水</category>
      </categories>
  </entry>
</search>
